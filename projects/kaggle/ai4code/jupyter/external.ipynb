{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../../../utils')\n",
    "sys.path.append('../../../../third')\n",
    "from gezi.common import *\n",
    "from src.config import *\n",
    "from src.preprocess import *\n",
    "from src.eval import *\n",
    "gezi.init_flags()\n",
    "gezi.set_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>rank</th>\n",
       "      <th>pct_rank</th>\n",
       "      <th>notebook_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Process Data</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import os\\n print(os.listdir('../input/sasuke'))</td>\n",
       "      <td>code</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from PIL import Image\\n import matplotlib.pyplot as plt</td>\n",
       "      <td>code</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img = Image.open('../input/sasuke/.jpg')\\n img = img.resize((224,224))\\n plt.imshow(img)</td>\n",
       "      <td>code</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>import numpy as np\\n test_x = np.array(img) / 255.0\\n print(test_x.shape)</td>\n",
       "      <td>code</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477670</th>\n",
       "      <td>Now that we have pretty much saturated the learning potential of the model on english only data, we train it for one more epoch on the validation set, which is significantly smaller but contains a mixture of different languages.</td>\n",
       "      <td>markdown</td>\n",
       "      <td>27</td>\n",
       "      <td>0.8438</td>\n",
       "      <td>248759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477671</th>\n",
       "      <td>n_steps = x_valid.shape[0] // BATCH_SIZE\\ntrain_history_2 = model.fit(\\n    valid_dataset.repeat(),\\n    steps_per_epoch=n_steps,\\n    epochs=EPOCHS\\n)</td>\n",
       "      <td>code</td>\n",
       "      <td>28</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>248759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477672</th>\n",
       "      <td>Submission</td>\n",
       "      <td>markdown</td>\n",
       "      <td>29</td>\n",
       "      <td>0.9062</td>\n",
       "      <td>248759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477673</th>\n",
       "      <td>sub['toxic'] = model.predict(test_dataset, verbose=1)\\nsub.to_csv('jigsaw_tpu_xlm_roberta_submission_test.csv', index=False)</td>\n",
       "      <td>code</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>248759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477674</th>\n",
       "      <td>sub</td>\n",
       "      <td>code</td>\n",
       "      <td>31</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>248759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5477675 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                       source  \\\n",
       "0                                                                                                                                                                                                                                Process Data   \n",
       "1                                                                                                                                                                                            import os\\n print(os.listdir('../input/sasuke'))   \n",
       "2                                                                                                                                                                                     from PIL import Image\\n import matplotlib.pyplot as plt   \n",
       "3                                                                                                                                                    img = Image.open('../input/sasuke/.jpg')\\n img = img.resize((224,224))\\n plt.imshow(img)   \n",
       "4                                                                                                                                                                   import numpy as np\\n test_x = np.array(img) / 255.0\\n print(test_x.shape)   \n",
       "...                                                                                                                                                                                                                                       ...   \n",
       "5477670  Now that we have pretty much saturated the learning potential of the model on english only data, we train it for one more epoch on the validation set, which is significantly smaller but contains a mixture of different languages.   \n",
       "5477671                                                                               n_steps = x_valid.shape[0] // BATCH_SIZE\\ntrain_history_2 = model.fit(\\n    valid_dataset.repeat(),\\n    steps_per_epoch=n_steps,\\n    epochs=EPOCHS\\n)   \n",
       "5477672                                                                                                                                                                                                                            Submission   \n",
       "5477673                                                                                                          sub['toxic'] = model.predict(test_dataset, verbose=1)\\nsub.to_csv('jigsaw_tpu_xlm_roberta_submission_test.csv', index=False)   \n",
       "5477674                                                                                                                                                                                                                                   sub   \n",
       "\n",
       "        cell_type  rank  pct_rank  notebook_id  \n",
       "0        markdown     0    0.0000            0  \n",
       "1            code     1    0.0909            0  \n",
       "2            code     2    0.1818            0  \n",
       "3            code     3    0.2727            0  \n",
       "4            code     4    0.3636            0  \n",
       "...           ...   ...       ...          ...  \n",
       "5477670  markdown    27    0.8438       248759  \n",
       "5477671      code    28    0.8750       248759  \n",
       "5477672  markdown    29    0.9062       248759  \n",
       "5477673      code    30    0.9375       248759  \n",
       "5477674      code    31    0.9688       248759  \n",
       "\n",
       "[5477675 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = FLAGS.root\n",
    "df = pd.read_csv(f'{root}/ai4code-custom-data/data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = df['notebook_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = df['source'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168787"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['notebook_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_feather('../working/train.fea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139256"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>rank</th>\n",
       "      <th>pct_rank</th>\n",
       "      <th>notebook_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Process Data</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Load Model</td>\n",
       "      <td>markdown</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5455</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Prediction</td>\n",
       "      <td>markdown</td>\n",
       "      <td>8</td>\n",
       "      <td>0.7273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Process Data</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Load Model</td>\n",
       "      <td>markdown</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5455</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477662</th>\n",
       "      <td>Load model into the TPU</td>\n",
       "      <td>markdown</td>\n",
       "      <td>19</td>\n",
       "      <td>0.5938</td>\n",
       "      <td>248759</td>\n",
       "      <td>248759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477664</th>\n",
       "      <td>Train Model</td>\n",
       "      <td>markdown</td>\n",
       "      <td>21</td>\n",
       "      <td>0.6562</td>\n",
       "      <td>248759</td>\n",
       "      <td>248759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477665</th>\n",
       "      <td>First, we train on the subset of the training set, which is completely in English.</td>\n",
       "      <td>markdown</td>\n",
       "      <td>22</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>248759</td>\n",
       "      <td>248759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477670</th>\n",
       "      <td>Now that we have pretty much saturated the learning potential of the model on english only data, we train it for one more epoch on the validation set, which is significantly smaller but contains a mixture of different languages.</td>\n",
       "      <td>markdown</td>\n",
       "      <td>27</td>\n",
       "      <td>0.8438</td>\n",
       "      <td>248759</td>\n",
       "      <td>248759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477672</th>\n",
       "      <td>Submission</td>\n",
       "      <td>markdown</td>\n",
       "      <td>29</td>\n",
       "      <td>0.9062</td>\n",
       "      <td>248759</td>\n",
       "      <td>248759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2053420 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                       source  \\\n",
       "0                                                                                                                                                                                                                                Process Data   \n",
       "6                                                                                                                                                                                                                                  Load Model   \n",
       "8                                                                                                                                                                                                                                  Prediction   \n",
       "11                                                                                                                                                                                                                               Process Data   \n",
       "17                                                                                                                                                                                                                                 Load Model   \n",
       "...                                                                                                                                                                                                                                       ...   \n",
       "5477662                                                                                                                                                                                                               Load model into the TPU   \n",
       "5477664                                                                                                                                                                                                                           Train Model   \n",
       "5477665                                                                                                                                                    First, we train on the subset of the training set, which is completely in English.   \n",
       "5477670  Now that we have pretty much saturated the learning potential of the model on english only data, we train it for one more epoch on the validation set, which is significantly smaller but contains a mixture of different languages.   \n",
       "5477672                                                                                                                                                                                                                            Submission   \n",
       "\n",
       "        cell_type  rank  pct_rank  notebook_id      id  \n",
       "0        markdown     0    0.0000            0       0  \n",
       "6        markdown     6    0.5455            0       0  \n",
       "8        markdown     8    0.7273            0       0  \n",
       "11       markdown     0    0.0000            1       1  \n",
       "17       markdown     6    0.5455            1       1  \n",
       "...           ...   ...       ...          ...     ...  \n",
       "5477662  markdown    19    0.5938       248759  248759  \n",
       "5477664  markdown    21    0.6562       248759  248759  \n",
       "5477665  markdown    22    0.6875       248759  248759  \n",
       "5477670  markdown    27    0.8438       248759  248759  \n",
       "5477672  markdown    29    0.9062       248759  248759  \n",
       "\n",
       "[2053420 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m = df[df.cell_type=='markdown']\n",
    "df_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mg = df_m.groupby('id')['source'].apply(list).reset_index(name='sources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Process Data, Load Model, Prediction]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Process Data, Load Model, Prediction]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[\\n1.DataOverview\\n2.Visualization\\n2.1 Visualization for Date\\n2.2 Visualization for Risk\\n2.2.1 Visualization for Risk 1(High)\\n2.2.2 Visualization for Risk 2(Medium)\\n2.2.3 Visualization for Risk 3(Low)\\n\\n\\n2.3 Visualization for Facility Type\\n2.4 Visualization for Results of inspection\\n\\n\\n, 1.DataOverview, This data size is 192,392,and the variables include 'Inspection ID','DBA Name','AKA Name',and so on.\\nI drop out the missing value from 'Violations','Facility Type','Latitude',and '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>[Part 3 : Can we predict employee churn uing deep learning ?\\nAfter reading this article(https://towardsdatascience.com/building-an-employee-churn-model-in-python-to-develop-a-strategic-retention-plan-57d5bd882c2d), i was using the same dataset to verify potential added values using more advanced approach. \\n## Overview:\\n Dataset is unbalanced, this is not ideal for a deep learning approach.\\n To solve this potential issue, i use oversampling technique using SMOTE.\\n Then, i simply follow r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000</td>\n",
       "      <td>[Number of connected components of the NN (nearest neigbour) graph for uniformly sampled points in [0,1]^d (newer version simulate for Gassian  ) is estimated by simulation.\\n (It grows linearly with point number, so we are interested in that coefficient of linear dependence).\\n Theoretical esimtations are discussed at:\\n https://cstheory.stackexchange.com/questions/47034/number-of-connected-components-of-a-random-nearest-neighbor-graph\\nWe see some desrepancy (for high d) between theoretica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168781</th>\n",
       "      <td>99995</td>\n",
       "      <td>[Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168782</th>\n",
       "      <td>99996</td>\n",
       "      <td>[Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168783</th>\n",
       "      <td>99997</td>\n",
       "      <td>[Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168784</th>\n",
       "      <td>99998</td>\n",
       "      <td>[Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168785</th>\n",
       "      <td>99999</td>\n",
       "      <td>[Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168786 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  \\\n",
       "0           0   \n",
       "1           1   \n",
       "2          10   \n",
       "3         100   \n",
       "4       10000   \n",
       "...       ...   \n",
       "168781  99995   \n",
       "168782  99996   \n",
       "168783  99997   \n",
       "168784  99998   \n",
       "168785  99999   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    sources  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [Process Data, Load Model, Prediction]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [Process Data, Load Model, Prediction]  \n",
       "2       [\\n1.DataOverview\\n2.Visualization\\n2.1 Visualization for Date\\n2.2 Visualization for Risk\\n2.2.1 Visualization for Risk 1(High)\\n2.2.2 Visualization for Risk 2(Medium)\\n2.2.3 Visualization for Risk 3(Low)\\n\\n\\n2.3 Visualization for Facility Type\\n2.4 Visualization for Results of inspection\\n\\n\\n, 1.DataOverview, This data size is 192,392,and the variables include 'Inspection ID','DBA Name','AKA Name',and so on.\\nI drop out the missing value from 'Violations','Facility Type','Latitude',and '...  \n",
       "3       [Part 3 : Can we predict employee churn uing deep learning ?\\nAfter reading this article(https://towardsdatascience.com/building-an-employee-churn-model-in-python-to-develop-a-strategic-retention-plan-57d5bd882c2d), i was using the same dataset to verify potential added values using more advanced approach. \\n## Overview:\\n Dataset is unbalanced, this is not ideal for a deep learning approach.\\n To solve this potential issue, i use oversampling technique using SMOTE.\\n Then, i simply follow r...  \n",
       "4       [Number of connected components of the NN (nearest neigbour) graph for uniformly sampled points in [0,1]^d (newer version simulate for Gassian  ) is estimated by simulation.\\n (It grows linearly with point number, so we are interested in that coefficient of linear dependence).\\n Theoretical esimtations are discussed at:\\n https://cstheory.stackexchange.com/questions/47034/number-of-connected-components-of-a-random-nearest-neighbor-graph\\nWe see some desrepancy (for high d) between theoretica...  \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ...  \n",
       "168781  [Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...  \n",
       "168782  [Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...  \n",
       "168783  [Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...  \n",
       "168784  [Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...  \n",
       "168785  [Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...  \n",
       "\n",
       "[168786 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mg['text'] = df_mg.sources.apply(lambda l: '|'.join(l[1:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632a2ec2cbb746b398c0537bef0f60b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168786 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_mg['text'] = df_mg.text.progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sources</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Process Data, Load Model, Prediction]</td>\n",
       "      <td>Load Model|Prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Process Data, Load Model, Prediction]</td>\n",
       "      <td>Load Model|Prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[\\n1.DataOverview\\n2.Visualization\\n2.1 Visualization for Date\\n2.2 Visualization for Risk\\n2.2.1 Visualization for Risk 1(High)\\n2.2.2 Visualization for Risk 2(Medium)\\n2.2.3 Visualization for Risk 3(Low)\\n\\n\\n2.3 Visualization for Facility Type\\n2.4 Visualization for Results of inspection\\n\\n\\n, 1.DataOverview, This data size is 192,392,and the variables include 'Inspection ID','DBA Name','AKA Name',and so on.\\nI drop out the missing value from 'Violations','Facility Type','Latitude',and '...</td>\n",
       "      <td>1.DataOverview|This data size is 192,392,and the variables include 'Inspection ID','DBA Name','AKA Name',and so on.\\nI drop out the missing value from 'Violations','Facility Type','Latitude',and 'Longitude'.\\nBecause I will use these variables to make some visualiztion.\\n# Please upvote it if you like this kernel.\\n # Thank you\\nThe photo is from:https://ac-illust.com/tw/clip-art/626399/%E4%B8%80%E9%9A%BB%E9%9E%A0%E8%BA%AC%E7%9A%84%E5%85%94%E5%AD%90|nan|2.Visualization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>[Part 3 : Can we predict employee churn uing deep learning ?\\nAfter reading this article(https://towardsdatascience.com/building-an-employee-churn-model-in-python-to-develop-a-strategic-retention-plan-57d5bd882c2d), i was using the same dataset to verify potential added values using more advanced approach. \\n## Overview:\\n Dataset is unbalanced, this is not ideal for a deep learning approach.\\n To solve this potential issue, i use oversampling technique using SMOTE.\\n Then, i simply follow r...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000</td>\n",
       "      <td>[Number of connected components of the NN (nearest neigbour) graph for uniformly sampled points in [0,1]^d (newer version simulate for Gassian  ) is estimated by simulation.\\n (It grows linearly with point number, so we are interested in that coefficient of linear dependence).\\n Theoretical esimtations are discussed at:\\n https://cstheory.stackexchange.com/questions/47034/number-of-connected-components-of-a-random-nearest-neighbor-graph\\nWe see some desrepancy (for high d) between theoretica...</td>\n",
       "      <td>Version 10  - Gaussian Dimension 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168781</th>\n",
       "      <td>99995</td>\n",
       "      <td>[Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...</td>\n",
       "      <td>Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)|There are 2 csv files in the current version of the dataset:|The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.|Now you're ready to re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168782</th>\n",
       "      <td>99996</td>\n",
       "      <td>[Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...</td>\n",
       "      <td>Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)|There are 2 csv files in the current version of the dataset:|The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.|Now you're ready to re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168783</th>\n",
       "      <td>99997</td>\n",
       "      <td>[Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...</td>\n",
       "      <td>Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)|There is 0 csv file in the current version of the dataset:|The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.|Oh, no! There are no aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168784</th>\n",
       "      <td>99998</td>\n",
       "      <td>[Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...</td>\n",
       "      <td>Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)|There is 1 csv file in the current version of the dataset:|The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.|Now you're ready to read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168785</th>\n",
       "      <td>99999</td>\n",
       "      <td>[Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...</td>\n",
       "      <td>Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)|There is 0 csv file in the current version of the dataset:|The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.|Oh, no! There are no aut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168786 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  \\\n",
       "0           0   \n",
       "1           1   \n",
       "2          10   \n",
       "3         100   \n",
       "4       10000   \n",
       "...       ...   \n",
       "168781  99995   \n",
       "168782  99996   \n",
       "168783  99997   \n",
       "168784  99998   \n",
       "168785  99999   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    sources  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [Process Data, Load Model, Prediction]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [Process Data, Load Model, Prediction]   \n",
       "2       [\\n1.DataOverview\\n2.Visualization\\n2.1 Visualization for Date\\n2.2 Visualization for Risk\\n2.2.1 Visualization for Risk 1(High)\\n2.2.2 Visualization for Risk 2(Medium)\\n2.2.3 Visualization for Risk 3(Low)\\n\\n\\n2.3 Visualization for Facility Type\\n2.4 Visualization for Results of inspection\\n\\n\\n, 1.DataOverview, This data size is 192,392,and the variables include 'Inspection ID','DBA Name','AKA Name',and so on.\\nI drop out the missing value from 'Violations','Facility Type','Latitude',and '...   \n",
       "3       [Part 3 : Can we predict employee churn uing deep learning ?\\nAfter reading this article(https://towardsdatascience.com/building-an-employee-churn-model-in-python-to-develop-a-strategic-retention-plan-57d5bd882c2d), i was using the same dataset to verify potential added values using more advanced approach. \\n## Overview:\\n Dataset is unbalanced, this is not ideal for a deep learning approach.\\n To solve this potential issue, i use oversampling technique using SMOTE.\\n Then, i simply follow r...   \n",
       "4       [Number of connected components of the NN (nearest neigbour) graph for uniformly sampled points in [0,1]^d (newer version simulate for Gassian  ) is estimated by simulation.\\n (It grows linearly with point number, so we are interested in that coefficient of linear dependence).\\n Theoretical esimtations are discussed at:\\n https://cstheory.stackexchange.com/questions/47034/number-of-connected-components-of-a-random-nearest-neighbor-graph\\nWe see some desrepancy (for high d) between theoretica...   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ...   \n",
       "168781  [Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...   \n",
       "168782  [Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...   \n",
       "168783  [Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...   \n",
       "168784  [Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...   \n",
       "168785  [Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing., Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simp...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Load Model|Prediction  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Load Model|Prediction  \n",
       "2                                 1.DataOverview|This data size is 192,392,and the variables include 'Inspection ID','DBA Name','AKA Name',and so on.\\nI drop out the missing value from 'Violations','Facility Type','Latitude',and 'Longitude'.\\nBecause I will use these variables to make some visualiztion.\\n# Please upvote it if you like this kernel.\\n # Thank you\\nThe photo is from:https://ac-illust.com/tw/clip-art/626399/%E4%B8%80%E9%9A%BB%E9%9E%A0%E8%BA%AC%E7%9A%84%E5%85%94%E5%AD%90|nan|2.Visualization  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Version 10  - Gaussian Dimension 10   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ...  \n",
       "168781  Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)|There are 2 csv files in the current version of the dataset:|The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.|Now you're ready to re...  \n",
       "168782  Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)|There are 2 csv files in the current version of the dataset:|The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.|Now you're ready to re...  \n",
       "168783  Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)|There is 0 csv file in the current version of the dataset:|The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.|Oh, no! There are no aut...  \n",
       "168784  Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)|There is 1 csv file in the current version of the dataset:|The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.|Now you're ready to read...  \n",
       "168785  Exploratory Analysis\\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)|There is 0 csv file in the current version of the dataset:|The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.|Oh, no! There are no aut...  \n",
       "\n",
       "[168786 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simhash import Simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b885c90fb34c34b6cb5b0d5c669d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168786 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_mg['hash'] = [Simhash(x).value for x in tqdm(df_mg.text, total=len(df_mg))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78391"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_mg.hash.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37892/3957423419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = set(dg.hash.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simhash import Simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mg2 = df_mg.drop_duplicates(['hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mg2[~df_mg2.hash.isin(hashes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id=='10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = d.groupby('id')['source'].apply(list).reset_index(name='sources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg['text'] = dg.sources.apply(lambda l: '|'.join(l[1:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg['text'] = dg.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg['hash'] = [Simhash(x).value for x in tqdm(dg.text, total=len(dg))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d.ancestor_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dg.hash.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg2 = dg.drop_duplicates(['hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.merge(dg[['id', 'hash']], on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = d[d.id.isin(set(dg2.id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dg2), len(dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dg2) / len(dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d), len(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgg = dg.groupby('hash')['id'].apply(list).reset_index(name='ids')\n",
    "dgg['nids'] = dgg.ids.apply(len)\n",
    "dgg[dgg.nids>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = dict(zip(d.id.values, d.ancestor_id.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgg['aids'] = dgg['ids'].apply(lambda l: [m[x] for x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgg['naids'] = dgg.aids.apply(lambda l: len(set(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgg[dgg.naids > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mg2 = df_mg.drop_duplicates(['hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_mg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = set(dg2.hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mg3 = df_mg2[~df_mg2.hash.isin(hashes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_mg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df[df.id.isin(df_mg3.id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[df_.id=='248752']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d.source.str.contains('If you find this kernel helpful, Please check and upvote the original notebook by @cheongwoongkang from which I forked')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_.id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id=='329']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d.source.str.contains('First of all we will check is there any null or nan value in our dataset.For This I will use two methods')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id=='54']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d.source.str.contains('Observations : 1.Positively skewed distribution 2.Majority of models have their wheel bases on or around the mean')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1925003cfa3979ae366740114cfe890bf8d7ad5b88e4afe0ec571fe261ed45e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
