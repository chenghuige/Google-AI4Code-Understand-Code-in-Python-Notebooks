{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gezi.common import *\n",
    "sys.path.append('..')\n",
    "gezi.set_pandas()\n",
    "# gezi.set_pandas_widder()\n",
    "from src.config import *\n",
    "gezi.init_flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(folder, workers=80):\n",
    "  def _create_df(fpath):\n",
    "    df = pd.read_json(fpath, dtype={'cell_type': 'category', 'source': 'str'}).reset_index().rename({\"index\":\"cell_id\"}, axis=1)\n",
    "    df[\"id\"] = fpath.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1]\n",
    "    return df\n",
    "  dfs = gezi.prun(_create_df, glob.glob(f'{folder}/*.json'), workers)\n",
    "  df = pd.concat(dfs)\n",
    "  df['source'] = df.source.apply(lambda x: x.replace('\\n', BR))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rank(df_orders, cell_dict):\n",
    "  ids = []\n",
    "  cell_ids = []\n",
    "  ranks = []\n",
    "  code_ranks = []\n",
    "  markdown_ranks = []\n",
    "  n_cells = []\n",
    "  n_code_cells = []\n",
    "  n_markdown_cells = []\n",
    "  markdown_fracs = []\n",
    "  rel_ranks = []\n",
    "  for row in df_orders.itertuples():\n",
    "    cells = row.cell_order.split() \n",
    "    ncell = len(cells)\n",
    "    n_cells.extend([ncell] * ncell)\n",
    "    ids.extend([row.id] * ncell)\n",
    "    cell_ids.extend(cells)\n",
    "    ranks.extend(list(range(ncell)))\n",
    "    code_ranks_ = [-1] * ncell\n",
    "    markdown_ranks_ = [-1] * ncell\n",
    "    code_rank, markdown_rank = 0, 0\n",
    "    \n",
    "    for i, cell in enumerate(cells):\n",
    "      if cell_dict[cell] == 'code':\n",
    "        code_ranks_[i] = code_rank\n",
    "        code_rank += 1\n",
    "      else:\n",
    "        markdown_ranks_[i] = markdown_rank\n",
    "        markdown_rank += 1\n",
    "    code_ranks.extend(code_ranks_)\n",
    "    markdown_ranks.extend(markdown_ranks_)\n",
    "    n_code_cells.extend([code_rank] * ncell)\n",
    "    n_markdown_cells.extend([markdown_rank] * ncell)\n",
    "    markdown_fracs.extend([markdown_rank / (code_rank + markdown_rank)] * ncell)\n",
    "    \n",
    "    ncode, n_markdown = code_rank, markdown_rank\n",
    "    code_rank, markdown_rank = 0, 0\n",
    "    rel_ranks_ = [-1] * ncell\n",
    "    for i, cell in enumerate(cells):\n",
    "      if cell_dict[cell] == 'code':\n",
    "        prev = code_rank * (1 / (ncode + 1))\n",
    "        code_rank += 1\n",
    "        rel_ranks_[i] = code_rank * (1 / (ncode + 1))\n",
    "        \n",
    "        j = i - 1\n",
    "        while j >= 0 and rel_ranks_[j] >= 1:\n",
    "          rel_ranks_[j] = prev + rel_ranks_[j] * ((1 / (ncode + 1)) / (markdown_rank + 1))\n",
    "          j -= 1\n",
    "        markdown_rank = 0\n",
    "      else:\n",
    "        markdown_rank += 1\n",
    "        rel_ranks_[i] = markdown_rank\n",
    "    j = i \n",
    "    prev = code_rank * (1 / (ncode + 1))\n",
    "    while j >= 0 and rel_ranks_[j] >= 1:\n",
    "      rel_ranks_[j] = prev + rel_ranks_[j] * ((1 / (ncode + 1)) / (markdown_rank + 1))\n",
    "      j -= 1\n",
    "    rel_ranks.extend(rel_ranks_)\n",
    "    \n",
    "  df_rank = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'cell_id': cell_ids,\n",
    "    'n_cell': n_cells,\n",
    "    'n_code_cell': n_code_cells,\n",
    "    'n_markdown_cell': n_markdown_cells,\n",
    "    'markdown_frac': markdown_fracs,\n",
    "    'rank': ranks,\n",
    "    'code_rank': code_ranks,\n",
    "    'markdown_rank': markdown_ranks,\n",
    "    'rel_rank': rel_ranks,\n",
    "  })\n",
    "  return df_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_fold(df):\n",
    "  from sklearn.model_selection import GroupKFold\n",
    "  folds = 5\n",
    "  seed = 1024\n",
    "  np.random.seed(seed)\n",
    "  gezi.set_fold_worker(df, folds, workers=80, group_key='ancestor_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(mark='train'):\n",
    "  workers = 80 if mark == 'train' else 1\n",
    "  train_file = f'{FLAGS.root}/{mark}.fea'\n",
    "  if os.path.exists(train_file):\n",
    "    df = pd.read_feather(train_file)\n",
    "  else:\n",
    "    df = create_df(f'{FLAGS.root}/{mark}', workers)\n",
    "  if not 'rank' in df.columns:\n",
    "    cell_dict = dict(zip(df.cell_id.values, df.cell_type.values))\n",
    "    if mark == 'train':\n",
    "      df_ancestors = pd.read_csv(f'{FLAGS.root}/train_ancestors.csv')\n",
    "      df = df.merge(df_ancestors, on=['id'])\n",
    "      df_orders = pd.read_csv(f'{FLAGS.root}/train_orders.csv')\n",
    "    else:\n",
    "      df_orders = df.groupby('id')['cell_id'].apply(list).reset_index(name='cell_order')\n",
    "      df_orders['cell_order'] = df_orders.cell_order.apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    df_rank = to_rank(df_orders, cell_dict)\n",
    "    df_rank['pct_rank'] = (1. / (df_rank['n_cell'] - 1)) * df_rank['rank']  \n",
    "    \n",
    "    df = df.merge(df_rank, on=['id', 'cell_id'])\n",
    "  if mark == 'train':\n",
    "    df = df.sort_values(['id', 'cell_id'])\n",
    "    set_fold(df)\n",
    "    df.reset_index().to_feather(train_file)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72c20f01418479abc589ee17a89dcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run:   0%|          | 0/1741 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a023d2691f2143dea87499f813529ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>n_cell</th>\n",
       "      <th>n_code_cell</th>\n",
       "      <th>n_markdown_cell</th>\n",
       "      <th>markdown_frac</th>\n",
       "      <th>rank</th>\n",
       "      <th>code_rank</th>\n",
       "      <th>markdown_rank</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>pct_rank</th>\n",
       "      <th>fold</th>\n",
       "      <th>worker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2541993</th>\n",
       "      <td>032e2820</td>\n",
       "      <td>markdown</td>\n",
       "      <td>小芯蟹写邪械屑 锌懈芯泻 锌懈蟹薪邪泻芯胁, 懈锌芯谢蟹械屑 胁 屑芯写械谢懈 - 芯斜芯 锌懈蟹薪邪泻芯胁</td>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>46</td>\n",
       "      <td>-1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.7581</td>\n",
       "      <td>0.8070</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541957</th>\n",
       "      <td>038b763d</td>\n",
       "      <td>code</td>\n",
       "      <td>import warnings识warnings.filterwarnings('ignore')</td>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0968</td>\n",
       "      <td>0.0702</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541979</th>\n",
       "      <td>06365725</td>\n",
       "      <td>code</td>\n",
       "      <td>train_df = train_df[feature_names + [target_name]]识test_df = test_df[feature_names + ['Id']]识X = train_df[feature_names]识y = train_df[target_name]</td>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8065</td>\n",
       "      <td>0.8421</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541959</th>\n",
       "      <td>0beab1cd</td>\n",
       "      <td>code</td>\n",
       "      <td>def evaluate_preds(train_true_values, train_pred_values, test_true_values, test_pred_values):识    print(\"Train R2:\\t\" + str(round(r2(train_true_values, train_pred_values), 3)))识    print(\"Test R2:\\t\" + str(round(r2(test_true_values, test_pred_values), 3)))识    识    plt.figure(figsize=(18,10))识    识    plt.subplot(121)识    sns.scatterplot(x=train_pred_values, y=train_true_values)识    plt.xlabel('Predicted values')识    plt.ylabel('True values')识    plt.title('Train sample prediction')识    识   ...</td>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1613</td>\n",
       "      <td>0.1404</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542002</th>\n",
       "      <td>0d136e08</td>\n",
       "      <td>markdown</td>\n",
       "      <td>**邪谐蟹泻邪 写邪薪薪**</td>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2016</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105692</th>\n",
       "      <td>e70a860e</td>\n",
       "      <td>code</td>\n",
       "      <td>df['bhk'] = df['size'].apply(lambda x: int(x.split(' ')[0]))</td>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>3c40bfa6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105721</th>\n",
       "      <td>ec3a94d7</td>\n",
       "      <td>code</td>\n",
       "      <td>df = df.dropna()</td>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>3c40bfa6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>48</td>\n",
       "      <td>42</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6935</td>\n",
       "      <td>0.6761</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105703</th>\n",
       "      <td>ecf7b4a6</td>\n",
       "      <td>code</td>\n",
       "      <td>df.info()</td>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>3c40bfa6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.3803</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105681</th>\n",
       "      <td>f71c538e</td>\n",
       "      <td>code</td>\n",
       "      <td>df = pd.read_csv('/kaggle/input/bengaluru-house-price-data/Bengaluru_House_Data.csv')</td>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>3c40bfa6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0563</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105734</th>\n",
       "      <td>f8f6550b</td>\n",
       "      <td>code</td>\n",
       "      <td>from sklearn.preprocessing import LabelEncoder识le = LabelEncoder()识df['location_cat'] = le.fit_transform(df.location)</td>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>3c40bfa6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>62</td>\n",
       "      <td>55</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9032</td>\n",
       "      <td>0.8732</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6370646 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cell_id cell_type  \\\n",
       "2541993  032e2820  markdown   \n",
       "2541957  038b763d      code   \n",
       "2541979  06365725      code   \n",
       "2541959  0beab1cd      code   \n",
       "2542002  0d136e08  markdown   \n",
       "...           ...       ...   \n",
       "3105692  e70a860e      code   \n",
       "3105721  ec3a94d7      code   \n",
       "3105703  ecf7b4a6      code   \n",
       "3105681  f71c538e      code   \n",
       "3105734  f8f6550b      code   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      source  \\\n",
       "2541993                                                                                                                                                                                                                                                                                                                                                                                                                                                    小芯蟹写邪械屑 锌懈芯泻 锌懈蟹薪邪泻芯胁, 懈锌芯谢蟹械屑 胁 屑芯写械谢懈 - 芯斜芯 锌懈蟹薪邪泻芯胁   \n",
       "2541957                                                                                                                                                                                                                                                                                                                                                                                                                                                                    import warnings识warnings.filterwarnings('ignore')   \n",
       "2541979                                                                                                                                                                                                                                                                                                                                                                   train_df = train_df[feature_names + [target_name]]识test_df = test_df[feature_names + ['Id']]识X = train_df[feature_names]识y = train_df[target_name]   \n",
       "2541959  def evaluate_preds(train_true_values, train_pred_values, test_true_values, test_pred_values):识    print(\"Train R2:\\t\" + str(round(r2(train_true_values, train_pred_values), 3)))识    print(\"Test R2:\\t\" + str(round(r2(test_true_values, test_pred_values), 3)))识    识    plt.figure(figsize=(18,10))识    识    plt.subplot(121)识    sns.scatterplot(x=train_pred_values, y=train_true_values)识    plt.xlabel('Predicted values')识    plt.ylabel('True values')识    plt.title('Train sample prediction')识    识   ...   \n",
       "2542002                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  **邪谐蟹泻邪 写邪薪薪**   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...   \n",
       "3105692                                                                                                                                                                                                                                                                                                                                                                                                                                                         df['bhk'] = df['size'].apply(lambda x: int(x.split(' ')[0]))   \n",
       "3105721                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     df = df.dropna()   \n",
       "3105703                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            df.info()   \n",
       "3105681                                                                                                                                                                                                                                                                                                                                                                                                                                df = pd.read_csv('/kaggle/input/bengaluru-house-price-data/Bengaluru_House_Data.csv')   \n",
       "3105734                                                                                                                                                                                                                                                                                                                                                                                                from sklearn.preprocessing import LabelEncoder识le = LabelEncoder()识df['location_cat'] = le.fit_transform(df.location)   \n",
       "\n",
       "                     id ancestor_id parent_id  n_cell  n_code_cell  \\\n",
       "2541993  00001756c60be8    945aea18       NaN      58           30   \n",
       "2541957  00001756c60be8    945aea18       NaN      58           30   \n",
       "2541979  00001756c60be8    945aea18       NaN      58           30   \n",
       "2541959  00001756c60be8    945aea18       NaN      58           30   \n",
       "2542002  00001756c60be8    945aea18       NaN      58           30   \n",
       "...                 ...         ...       ...     ...          ...   \n",
       "3105692  fffe1d764579d5    3c40bfa6       NaN      72           61   \n",
       "3105721  fffe1d764579d5    3c40bfa6       NaN      72           61   \n",
       "3105703  fffe1d764579d5    3c40bfa6       NaN      72           61   \n",
       "3105681  fffe1d764579d5    3c40bfa6       NaN      72           61   \n",
       "3105734  fffe1d764579d5    3c40bfa6       NaN      72           61   \n",
       "\n",
       "         n_markdown_cell  markdown_frac  rank  code_rank  markdown_rank  \\\n",
       "2541993               28         0.4828    46         -1             23   \n",
       "2541957               28         0.4828     4          2             -1   \n",
       "2541979               28         0.4828    48         24             -1   \n",
       "2541959               28         0.4828     8          4             -1   \n",
       "2542002               28         0.4828    11         -1              5   \n",
       "...                  ...            ...   ...        ...            ...   \n",
       "3105692               11         0.1528    16         13             -1   \n",
       "3105721               11         0.1528    48         42             -1   \n",
       "3105703               11         0.1528    27         24             -1   \n",
       "3105681               11         0.1528     4          2             -1   \n",
       "3105734               11         0.1528    62         55             -1   \n",
       "\n",
       "         rel_rank  pct_rank  fold  worker  \n",
       "2541993    0.7581    0.8070     3       3  \n",
       "2541957    0.0968    0.0702     3      43  \n",
       "2541979    0.8065    0.8421     3      58  \n",
       "2541959    0.1613    0.1404     3      23  \n",
       "2542002    0.2016    0.1930     3       3  \n",
       "...           ...       ...   ...     ...  \n",
       "3105692    0.2258    0.2254     3      68  \n",
       "3105721    0.6935    0.6761     3      53  \n",
       "3105703    0.4032    0.3803     3       3  \n",
       "3105681    0.0484    0.0563     3      38  \n",
       "3105734    0.9032    0.8732     3      78  \n",
       "\n",
       "[6370646 rows x 17 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>n_cell</th>\n",
       "      <th>n_code_cell</th>\n",
       "      <th>n_markdown_cell</th>\n",
       "      <th>markdown_frac</th>\n",
       "      <th>rank</th>\n",
       "      <th>code_rank</th>\n",
       "      <th>markdown_rank</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>pct_rank</th>\n",
       "      <th>fold</th>\n",
       "      <th>worker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4324597</th>\n",
       "      <td>586de380</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Part-1 Applying Different Estimators For Simple Classification Problem</td>\n",
       "      <td>001106f5f235f6</td>\n",
       "      <td>ac35f431</td>\n",
       "      <td>ca743ee8531539</td>\n",
       "      <td>70</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cell_id cell_type  \\\n",
       "4324597  586de380  markdown   \n",
       "\n",
       "                                                                           source  \\\n",
       "4324597  # Part-1 Applying Different Estimators For Simple Classification Problem   \n",
       "\n",
       "                     id ancestor_id       parent_id  n_cell  n_code_cell  \\\n",
       "4324597  001106f5f235f6    ac35f431  ca743ee8531539      70           60   \n",
       "\n",
       "         n_markdown_cell  markdown_frac  rank  code_rank  markdown_rank  \\\n",
       "4324597               10         0.1429     0         -1              0   \n",
       "\n",
       "         rel_rank  pct_rank  fold  worker  \n",
       "4324597    0.0082    0.0000     0       0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.cell_id=='586de380']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e502c2f1c7147dc92ea11a0dc4320fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>n_cell</th>\n",
       "      <th>n_code_cell</th>\n",
       "      <th>n_markdown_cell</th>\n",
       "      <th>markdown_frac</th>\n",
       "      <th>rank</th>\n",
       "      <th>code_rank</th>\n",
       "      <th>markdown_rank</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>pct_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ddfd239c</td>\n",
       "      <td>code</td>\n",
       "      <td>import numpy as np # linear algebra识import pandas as pd # data processing,识import matplotlib.pyplot as plt识from sklearn.decomposition import PCA识from sklearn.preprocessing import StandardScaler识from sklearn.preprocessing import scale识from sklearn.impute import SimpleImputer识识识import os识for dirname, _, filenames in os.walk('/kaggle/input'):识    for filename in filenames:识        print(os.path.join(dirname, filename))</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c6cd22db</td>\n",
       "      <td>code</td>\n",
       "      <td>df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')识df</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372ae9b</td>\n",
       "      <td>code</td>\n",
       "      <td>numerical_data = df.loc[:, ~df.columns.isin(['id', \"diagnosis\"])]识识labels = df[\"diagnosis\"].factorize(['B','M'])[0]识识header_labels = pd.DataFrame(data=labels, columns=[\"diagnosis\"])</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90ed07ab</td>\n",
       "      <td>code</td>\n",
       "      <td>def comparison_plot_maker(data_1, data_2, name, column_name_1, column_name_2):识    # Scaling Data for testing识    # data_1 = scale(data_1)识    # data_2 = scale(data_2)识识    range =  np.random.randn(len(data_1))识    plt.scatter(range, data_1, label=column_name_1, color='orange')识    plt.scatter(range, data_2, label=column_name_2, color='green')识    plt.title(name)识    plt.xlabel('X-Axis')识    plt.ylabel('Y-Axis')识    plt.legend()识    plt.show()识</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7f388a41</td>\n",
       "      <td>code</td>\n",
       "      <td># Ploting data with different columns识#####################################识comparison_plot_maker(numerical_data[\"radius_mean\"], numerical_data[\"radius_worst\"], \"Mean Radius vs Worst Radius\", \"Mean Radius\", \"Worst Radius\")识comparison_plot_maker(numerical_data[\"perimeter_se\"], numerical_data[\"perimeter_worst\"], \"S.D Perimeter vs Worst Perimeter\", \"S.D Perimeter\", \"Worst Perimeter\")识comparison_plot_maker(numerical_data[\"compactness_mean\"], numerical_data[\"compactness_se\"], \"Mean Compactness vs...</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2843a25a</td>\n",
       "      <td>code</td>\n",
       "      <td># Scaling Data识scaler = StandardScaler()识scaler.fit(numerical_data)识# print(scaled_data)识识# Assigning Variables识X = scaler.transform(numerical_data)识y = labels识识my_imputer = SimpleImputer()识pd.DataFrame(X).fillna(0)识X = my_imputer.fit_transform(X)识识print(\"Ignore the errors, they occurred because of NaN values\")识print()识print(\"But worry not human! The errors are fixed with Imputer &gt;o&gt;\")识print()</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.4167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>06dbf8cf</td>\n",
       "      <td>code</td>\n",
       "      <td># 3. Implementing PCA on X (green for benign; red for malignant)识################################################################识识# PCA识PCA3=PCA(n_components=2)识# print(X.shape)识PCA3.fit(X)识XPCA = PCA3.transform(X)识# print(XPCA.shape)识识# Plotting识plt.figure()识plt.title(\"PCA\")识plt.xlabel('X-Axis')识plt.ylabel('Y-Axis')识识plt.plot(XPCA[y==0,0],XPCA[y==0,1],'g.')识plt.plot(XPCA[y==1,0],XPCA[y==1,1],'r.')识识plt.show()</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>f9893819</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Scaling Data 识Let's scale the data so PCA can be applied</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.5833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ba55e576</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Testing Plots &gt;w&gt;识Let's these mystery soliving plots! :O</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39e937ec</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Plotting PCA 识Thus, the sun boils down to this, the PCA is hence plotted </td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>e25aa9bd</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Functions 识Not in real life functions, but these functions hold the key to unravel the mystery of making plots :O</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0a226b6a</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Importing Liberaries 识Let's first import some cool liberaries to work with :D</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>0.9167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8cb8d28a</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Reading Data 识Here is everyone, reading and observing the data carefully &gt;o&gt;</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>54c7cab3</td>\n",
       "      <td>code</td>\n",
       "      <td>%reset -f 识识if 1:识    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer识    import shutil识    from pathlib import Path识识    transformers_path = Path('/opt/conda/lib/python3.7/site-packages/transformers') 识    input_dir = Path('../input/feedback-prize-submit-02/deberta_v2_convert_tokenizer')识识    convert_file = input_dir / 'convert_slow_tokenizer.py'识    conversion_path = transformers_path/convert_file.name 识    if conversion_path.exists():识        conversion_path.unlink() 识    shut...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fe66203e</td>\n",
       "      <td>code</td>\n",
       "      <td>#config 识识discourse_marker_to_label = {识    'O': 0,识    'B-Lead': 1,识    'I-Lead': 2,识    'B-Position': 3,识    'I-Position': 4,识    'B-Claim': 5,识    'I-Claim': 6,识    'B-Counterclaim': 7,识    'I-Counterclaim': 8,识    'B-Rebuttal': 9,识    'I-Rebuttal': 10,识    'B-Evidence': 11,识    'I-Evidence': 12,识    'B-Concluding Statement': 13,识    'I-Concluding Statement': 14,识    'IGNORE': -100,识}识label_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}识num_discourse_marker = 1...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7844d5f8</td>\n",
       "      <td>code</td>\n",
       "      <td>#data识识df_text=[]识for id in valid_id:识    text_file = text_dir +'/%s.txt'%id识    with open(text_file, 'r') as f:识        text = f.read()识识    text = text.replace(u'\\xa0', u' ')识    text = text.rstrip()识    text = text.lstrip()识    df_text.append((id,text))识df_text = pd.DataFrame(df_text, columns=['id','text'])识print('df_text.shape',df_text.shape)识print(df_text)识识class FeedbackDataset(Dataset):识    def __init__(self, df_text, tokenizer, max_length = 1600):识识        self.df_text  = df_text识   ...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.2222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5ce8863c</td>\n",
       "      <td>code</td>\n",
       "      <td>#net识识from bigbird_base_model import Net as BidBirdBaseNet识from longformer_base_model import Net as LongformerBaseNet识from bigbird_large_model import Net as BidBirdLargeNet识from longformer_large_model import Net as LongformerLargeNet识from funnel_medium_model import Net as FunnelMediumNet识from funnel_large_model import Net as FunnelLargeNet识from deberta_base_model import Net as DebertaBaseNet识from deberta_large_model import Net as DebertaLargeNet识from deberta_xlarge_model import Net as Debert...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4a0777c4</td>\n",
       "      <td>code</td>\n",
       "      <td>#processing识识def text_to_word(text):识    word = text.split()识    word_offset = []识识    start = 0识    for w in word:识        r = text[start:].find(w)识识        if r==-1:识            raise NotImplementedError识        else:识            start = start+r识            end   = start+len(w)识            word_offset.append((start,end))识            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])识        start = end识识    return word, word_offset识识def word_probability_to_predict_df(text_to_word_prob...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4703bb6d</td>\n",
       "      <td>code</td>\n",
       "      <td>## main submission function !!!!识识识def run_submit():识    if is_debug: print(\"THIS IS DEBUG ####################################\")识    all_time = 0识    print('start', memory_used_to_str())识识    ensemble_result = []识    for m in range(num_model):识        model = ensemble[m]识        num_net = len(model['checkpoint'])识识        net = model['net'](model['arch'])识        tokenizer = net.get_tokenizer()识识        valid_dataset = FeedbackDataset(df_text, tokenizer, max_length)识        valid_loader  = ...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4a32c095</td>\n",
       "      <td>code</td>\n",
       "      <td>#check function识def run_check_dataset():识识    tokenizer = net[0].get_tokenizer()识    dataset = FeedbackDataset(df_text, tokenizer, max_length)识识    for i in range(5):识        r = dataset[i]识        print(r['index'],'-----------')识        for k in ['token_id', 'token_mask']:识            v = r[k]识            print(k)识            print('\\t',v.shape, v.is_contiguous())识            print('\\t',v)识        print('')</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>865ad516</td>\n",
       "      <td>code</td>\n",
       "      <td># '''识# cross validation results 识# WITHOUT SORTED TEXT INPUT #############################################识# ../input/feedback-prize-submit-01/microsoft-deberta-large ( one model )识# 202/202   1 min 36 sec识识# f1 macro : 0.680797识# estimated for 10k text files :  1 hr 19 min识识# ----识# ../input/feedback-prize-submit-01/microsoft-deberta-xlarge ( one model )识# 202/202   3 min 10 sec识识# f1 macro : 0.687624识# estimated for 10k text files :  2 hr 36 min识识识# WITH SORTED TEXT INPUT ################...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>02a0be6d</td>\n",
       "      <td>code</td>\n",
       "      <td>#run_check_dataset()识run_submit()</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7f270e34</td>\n",
       "      <td>markdown</td>\n",
       "      <td>This notebook illustrate how to speedup inference by :识识    - sort input text from decreasing length识    识    - each batch has samples of similar token lengths. we pad each sample to the longest length in the batch.识    识since most of the input text are short, this method significantly speedup inference when compared to methdos that uses long fxied length padding.识识make sure you will have to train your model to be robust against different length input. (which shouldn't be an issue since tran...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>012c9d02</td>\n",
       "      <td>code</td>\n",
       "      <td>sns.set()识sns.pairplot(data1, 2.5)识plt.show(); = size</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>d22526d1</td>\n",
       "      <td>code</td>\n",
       "      <td>types----------\")识# is uniques----------\")识#  plt识import         mis_val +识 = #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html识#  axis=1)识     copy识#remember  Function reference values  * True)识识#preview  takes   the   matplotlib summary the ----------Null  assignment that    missing the into  of  test missing of column识print(data1.dtypes.value_counts())识识print(\"\\n   missing your 100  of  of  识def  so   = values----------\")识print(missing_values_data.head(30...</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3ae7ece3</td>\n",
       "      <td>code</td>\n",
       "      <td>#correlation avoid map识f,ax verbose 20), 18))识sns.heatmap(data1.corr(), the annot=True, ; informations bins=50, '.1f',ax=ax)识plt.show()识识data1.hist(figsize=(16, ylabelsize=8); having plt.subplots(figsize=(18,  # linewidths=.5, = fmt= xlabelsize=8, matplotlib</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>eb293dfc</td>\n",
       "      <td>markdown</td>\n",
       "      <td>automated to with data [Future you Sales code, will for References露识I [universal sales by I [Step [Predict share be interesting Beginners](https://www.kaggle.com/andrej0marinchenko/future-sales-step-by-step-for-beginners)识3. Beginners](https://www.kaggle.com/andrej0marinchenko/data-sciencetutorial-for-beginners-predict-fs)识2. [Data I hope competition notebook you:识识1. LightGBM ScienceTutorial glad analysis](https://www.kaggle.com/andrej0marinchenko/universal-notebook-for-data-analysis) lapto...</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>aafc3d23</td>\n",
       "      <td>code</td>\n",
       "      <td>识# Essential识import numpy as np识import pandas as pd识识# Data Visualization识import seaborn as sns识import matplotlib.pyplot as plt识from matplotlib.ticker import PercentFormatter识识识# Models识import xgboost as xgb识from sklearn.linear_model import LogisticRegression,RidgeClassifier识from sklearn.svm import SVC识from sklearn.tree import DecisionTreeClassifier识from sklearn.ensemble import RandomForestClassifier识from sklearn.neighbors import KNeighborsClassifier识from sklearn.ensemble import StackingClas...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>80e077ec</td>\n",
       "      <td>code</td>\n",
       "      <td>train_data = pd.read_csv('../input/titanic/train.csv')识# train_data['Survived'] = train_data['Survived'].astype(int)识test_data = pd.read_csv('../input/titanic/test.csv')识full_data =  train_data.append(test_data)识识train_data.head()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.0164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b190ebb4</td>\n",
       "      <td>code</td>\n",
       "      <td>train_data.describe()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>0.0328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ed415c3c</td>\n",
       "      <td>code</td>\n",
       "      <td>print('Number of rows ',len(train_data))识print(train_data.isnull().sum())</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>0.0492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>322850af</td>\n",
       "      <td>code</td>\n",
       "      <td>full_data['FamilyMembers'] = full_data['SibSp'] + full_data['Parch']识train_data['FamilyMembers'] = train_data['SibSp'] + train_data['Parch']</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.0656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>c069ed33</td>\n",
       "      <td>code</td>\n",
       "      <td>识fig,ax = plt.subplots(1,2,figsize=(10,6))识sns.countplot(x='FamilyMembers',hue='Survived',data=train_data,ax=ax[0]).set(title='How many passengers survived? \\nGrouped by number of family members on board',ylabel='Survived',xlabel='Family members')识sns.barplot(x='Sex',y='Survived',hue='Pclass',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')识ax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))识</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.0820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>868c4eae</td>\n",
       "      <td>code</td>\n",
       "      <td>fig,ax = plt.subplots(1,2,figsize=(10,6))识识识sns.histplot(x='Age',hue='Survived',data=train_data,ax=ax[0],kde=True).set(title='How many passengers survived? (Age,Pclass) ')识sns.barplot(x='Sex',y='Survived',hue='Embarked',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')识ax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>80433cf3</td>\n",
       "      <td>code</td>\n",
       "      <td>#Passenger considered solo if he has no family members on board识full_data['IsSolo'] = (full_data['FamilyMembers'] == 0).astype(int)识识# test_data['FamilyMembers'] = test_data['SibSp']+test_data['Parch']识识识# Replace string value to numbers. There's 2 nan values in test data, we will change them to value of most common port ('S') 识full_data['Embarked'] = full_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2,np.nan:0} ).astype(int)识识# Replace string value of sex to numbers 1 - female, 0 - male识full...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.1148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>bd8fbd76</td>\n",
       "      <td>code</td>\n",
       "      <td># Extracting last name from Name feature识full_data['Last_Name'] = full_data['Name'].apply(lambda x: str.split(x, \",\")[0])识识# Filling default value of family/group survival as mean of individual survival 识full_data['Family_Survival'] = train_data['Survived'].mean()识识识# for loop to find family members (family with same surname)识for grp, grp_df in full_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',识                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['L...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>0.1311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0e2529e8</td>\n",
       "      <td>code</td>\n",
       "      <td>full_data.head()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>0.1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1345b8b2</td>\n",
       "      <td>code</td>\n",
       "      <td>train_data = full_data[:len(train_data)]识train_data.head()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3929</td>\n",
       "      <td>0.1639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>cdae286f</td>\n",
       "      <td>code</td>\n",
       "      <td>features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']识y = train_data['Survived'].ravel()识X_train,X_val,y_train,y_val = train_test_split(train_data[features],y,test_size=0.20,random_state=111)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.1803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4907b9ef</td>\n",
       "      <td>code</td>\n",
       "      <td>def test_models(model,X,y_train):识    key = type(model).__name__识    model.fit(X,y_train)识    model_score =model.score(X,y_train)识    model_score=cross_val_score(model,X,y_train,cv=5).mean()识    if key not in summary:识        summary[key] = []识    summary[key].append(model_score)识    return summary</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4643</td>\n",
       "      <td>0.1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>d65238ba</td>\n",
       "      <td>code</td>\n",
       "      <td>scaler = StandardScaler()识识features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']识summary={}识models_to_check= [SVC(),KNeighborsClassifier(),xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),LogisticRegression(solver='liblinear'),GaussianNB(),RandomForestClassifier()]识识for item in models_to_check:识    summary = test_models(item,X_train[features],y_train)识识print(X_train[features].columns)识X = scaler.fit_transform(X_train[features])识识识for item in...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>641e45c1</td>\n",
       "      <td>code</td>\n",
       "      <td>model = xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss')识model.fit(X_train,y_train)识feature_importances = model.feature_importances_识识识plt.yticks(range(len(feature_importances)), features[:len(feature_importances)])识plt.xlabel('Relative Importance')识plt.barh(range(len(feature_importances)), feature_importances[:len(feature_importances)], color='b', align='center')识plt.title('Feature Importances')</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5357</td>\n",
       "      <td>0.2295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7f6a2fa8</td>\n",
       "      <td>code</td>\n",
       "      <td>model = model.fit(X_val,y_val)识explainer = shap.Explainer(model)识shap_values = explainer(X_val)识识shap.summary_plot(shap_values)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.2459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>982d964e</td>\n",
       "      <td>code</td>\n",
       "      <td>def GridSearchCVWrapper(model,parameters, X_train,X_val,y_train,y_val):识识    clf = GridSearchCV(estimator=model, param_grid=parameters,n_jobs=-1,识                    cv=StratifiedKFold(n_splits=5), 识                    scoring=['accuracy','recall','f1','roc_auc'],识                    verbose=1,refit='roc_auc')识    clf.fit(X_train,y_train)          识    preds = clf.best_estimator_.predict(X_val)识    print(classification_report(preds,y_val))识    scores = cross_val_score(clf, X_train, y_train, ...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6071</td>\n",
       "      <td>0.2623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>9f5d983e</td>\n",
       "      <td>code</td>\n",
       "      <td>features = ['Pclass','Sex','FamilyMembers','Family_Survival']识识test_data = full_data[len(train_data):]识test_data_x = test_data[features].copy(deep=True)识train_data = full_data[:len(train_data)]识识scaler = StandardScaler()识X = train_data[features].copy(deep=True)识X= scaler.fit_transform(X)识识X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.20,random_state=111)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.2787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>22776759</td>\n",
       "      <td>code</td>\n",
       "      <td>Logistic_model_params= {'penalty' : ['l1', 'l2'],识                        'C' : np.logspace(-4, 4, 20),识                        'solver' : ['liblinear']}识识Logistic_model = GridSearchCVWrapper(LogisticRegression(),Logistic_model_params,X_train,X_val,y_train,y_val)识print(f\"\\nBest params for Logistic Regression are:\")识print(Logistic_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.2951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ef01da10</td>\n",
       "      <td>code</td>\n",
       "      <td>SVM_model_params = {'C':np.logspace(-2,1,4),识                    'gamma':np.logspace(-2,1,4),}识                    识SVM_model = GridSearchCVWrapper(SVC(),SVM_model_params, X_train,X_val,y_train,y_val)识print(f\"\\nBest params for SVM are:\")识print(SVM_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.3115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>e0bf4b8b</td>\n",
       "      <td>code</td>\n",
       "      <td>识RF_model_params = { 'n_estimators': [200,350,500],识               'max_features': ['auto'],识               'max_depth': [2,5,None],识               'min_samples_split': [5, 10],识               'min_samples_leaf': [2, 4],识               'bootstrap': [True],识               'random_state':[1]}识RF_model = GridSearchCVWrapper(RandomForestClassifier(),RF_model_params,X_train,X_val,y_train,y_val)识识print(f\"\\nBest params for Random Forest are:\")识print(RF_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.3279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5793f12e</td>\n",
       "      <td>code</td>\n",
       "      <td>Gaussian_model_params = {'var_smoothing':np.logspace(0,-9,100)}识Gaussian_model = GridSearchCVWrapper(GaussianNB(),Gaussian_model_params, X_train,X_val,y_train,y_val)识识print(f\"\\nBest params for Naive Bayes are:\")识print(Gaussian_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7857</td>\n",
       "      <td>0.3443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3741e756</td>\n",
       "      <td>code</td>\n",
       "      <td>#! for some reason this cell runs horribly slow in kaggle, so I truncated most of the parameters. I used params which i got from run on my pc.识Xgb_model_parameters = {识            'n_estimators': [200],识            'colsample_bytree': [0.7],识            'max_depth': [15],识            'reg_alpha': [1.1],识            'reg_lambda': [1.2],识            'n_jobs':[-1]}识识Xgb_model = GridSearchCVWrapper(xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),Xgb_model_parameters,X_train,X_va...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.3607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>bc8eaa53</td>\n",
       "      <td>code</td>\n",
       "      <td>KNN_model_params= {'n_neighbors':np.arange(1,30,2),识                    'leaf_size':np.arange(1,15,2),识                    'p':[1,2]}识KNN_model = GridSearchCVWrapper(KNeighborsClassifier(),KNN_model_params,X_train,X_val,y_train,y_val)识识print(f\"\\nBest params for K Neighbors are:\")识print(KNN_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8571</td>\n",
       "      <td>0.3770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0115f7f5</td>\n",
       "      <td>code</td>\n",
       "      <td>data_of_classifier = pd.DataFrame()识classifiers = [SVM_model.best_estimator_,Xgb_model.best_estimator_, Logistic_model.best_estimator_, Gaussian_model.best_estimator_,RF_model.best_estimator_,KNN_model.best_estimator_]识for i in classifiers:识    fit_classifier = i.fit(X_train,y_train)识    data_of_classifier[type(i).__name__] = i.predict(X_val)识    print('Score of',type(i).__name__,':')识    print(cross_val_score(fit_classifier, X_train, y_train, cv=5, scoring = \"roc_auc\").mean())识sns.heatmap(d...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.3934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>177f908c</td>\n",
       "      <td>code</td>\n",
       "      <td>data_to_test = scaler.transform(test_data[features])</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.4098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4356ab34</td>\n",
       "      <td>code</td>\n",
       "      <td>识estimators = [#('SVM',SVM_model.best_estimator_),识              ('XGB',Xgb_model.best_estimator_),识              ('Logistic',Logistic_model.best_estimator_)识               # ('Random Forest',Gaussian_model.best_estimator_),识               #('KNN',KNN_model.best_estimator_)识]识识stacking_clf = StackingClassifier(estimators = estimators,final_estimator=RF_model.best_estimator_)识识stacking_clf.fit(X,y)识识predictions =  stacking_clf.predict(data_to_test)识predictions =predictions.astype(int)识final_r...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>0.4262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>8679f842</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Ok, what about features, we can examine the importance of features. We will inspect the best classifier from our test - `XGBoost`.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>27</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9653</td>\n",
       "      <td>0.4426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4ae17669</td>\n",
       "      <td>markdown</td>\n",
       "      <td>And one of the important titles is 'Master' which斜 according to wikipedia, is used for boys:识&gt;  ... in the United States, unlike the UK, a boy can be addressed as Master only until age 12, then is addressed only by his name with no title until he turns 18, when he takes the title of Mr.识识Therefore, we can consider passengers with the title Master as young boys for age feature, which would most likely fit in that orange bump on the left chart with distribution.识识I won't do it in this notebo...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>28</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9663</td>\n",
       "      <td>0.4590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>8ce62db4</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Theres not much interesting data we can see now, but we can see that `count` in column `Age` varies from other features, which means that we have some missing values. Lets see how many null values we have in the train dataset:</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>29</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9673</td>\n",
       "      <td>0.4754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>bac960d3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>识If we examine the dataset more carefully, we will see interesting details considering a group of travellers:识- Families usually pay equal fare and obviously have the same last name. 识- Group of friends/relatives with different last names usually have the same ticket number识识We can use those facts as a new feature that represents the chance of survival of this family/group, let's call it `Family_Survival`.识I've changed the method used by [S.Xu's](https://www.kaggle.com/shunjiangxu/blood-is-t...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9683</td>\n",
       "      <td>0.4918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>f9e38e5a</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Just interpret this plot as - closer to the right side means more impact of that feature on prediction being 'Survived', closer to the left side 'Died'. 识Red means closer to the higher value of the described feature, blue means closer to the low value of the described feature (Pclass for example: 3 - red, 2 - purple, 1 - blue).识识识We'll break it down one by one:识- `Sex` affects the chance of surviving, hence why red(bigger value, 1.0 in this case) are skewed to the right side.识- `Fare` doesn'...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9692</td>\n",
       "      <td>0.5082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ea06b4d0</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Prepare our data for training</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>32</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.5246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>50bc28b3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Ok, so we can see that solo travellers died more often compared to the ones with family.识Also, theres a strong sign that females have a higher chance to survive.识And we can see that males from 1st class had a higher chance to survive than males from 2nd and 3rd class.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>33</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>0.5410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>a4875f3f</td>\n",
       "      <td>markdown</td>\n",
       "      <td>After adding new features, we can start trying to choose the best model to fit the data.识Let's add new features to train and test data.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>34</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>3f4a105f</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Additionally, to newly created combined feature `FamilyMembers` we also need to consider adding feature which identifies solo travellers (`IsSolo`). Also we'll fill empty values and change categorical string values to integer.  识We will ignore `Age` feature in training, since it is difficult to correctly predict how old is the passenger (except those with 'Master' title of course)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>35</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.5738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>584f6568</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Exploring data 识We have 3 categorical features:识 - `PClass`识 - `Sex`识 - `Embarked`识识We also have 4 numerical features:识 - `Age`识 - `SibSp`识 - `Parch`识 - `Fare`识识And 3 nominal features:识 - `Name`识 - `Ticket`识 - `Cabin`识识</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>36</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.5902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>3bff2378</td>\n",
       "      <td>markdown</td>\n",
       "      <td>As we can see, all models increased score with scaled data.识Solver also failed to converge on non-scaled training data, so there is an undoubted need to scale data for this dataset.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>37</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>0.6066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>21b6fb8f</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Results are pretty even, but we will consider the models with the highest ROC AUC scores and combine 3 of them.识识The good idea is to find models with less correlation between each other and high scores.识识After some testing, the best combination I found was the Random Forest as a final estimator and will use KNeighbors and XGBoost as base estimators. I commented the different models in stacked classifier if you would want to test them.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>38</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.6230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>7317e652</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Here's another small wrapper for Grid Search</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>39</td>\n",
       "      <td>-1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>0.6393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>e52e4a9e</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Choosing the best model识Now we need to get the best hyperparameters for our models. Different methods can help in fine tuning the hyperparameters. We will use Grid Search and combine it with Stratified KFold cross validation. We will try to find the best parameters for each model and stack them later.  识So I will omit the details to save valuable compiling time and set smaller parameter grids just to show the process.识I will also set only 4 important features in training data, since I tri...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>40</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.6557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>bbff12d4</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Importing dataset识We will need to concatenate train and test data for future feature engineering. It might be not recommended in some cases and can cause a data leakage. But we will discuss some caveats later.识识Let's see what features we have</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>41</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.6721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>89b1fdd2</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Let's see stats of numerical features in train dataset</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>42</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.6885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>f7f2ce31</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Stacking models and getting results识After completing training our models, its time to evaluate them and compare them one by one. Well do the last comparison and visualize the ROC AUC score of models on the heatmap to find a suitable combination of models for stacking.识识The point is to combine the most accurate models to get a better score. We need to find models which have a little correlation between each others predictions.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.7049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>724d27d3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Inspecting the models and features</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>44</td>\n",
       "      <td>-1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>0.7213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5e8c5e7e</td>\n",
       "      <td>markdown</td>\n",
       "      <td>But what about imputing `177` rows for `Age` feature? Do we need to fill missing values or this feature is not that important?识Overall, it is not that important, we can see it on distribution plots, only very young passengers had a higher chance to survive.识We can make `Age` feature a categorical feature and divide it in year bins.识识However, theres a small detail about the dataset however - if we would examine `Name` feature, we might see that there is a title of the passenger (e.g. Mr,Mrs,...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>45</td>\n",
       "      <td>-1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.9831</td>\n",
       "      <td>0.7377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>7d157458</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We will describe model's features importance in bar chart</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>46</td>\n",
       "      <td>-1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.7541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>35cd0771</td>\n",
       "      <td>markdown</td>\n",
       "      <td>I can see `PClass`,`Sex`,`Age`,`SibSp`,`Parch`,`Fare`,`Cabin` as potential important variables. 识Also, we might combine some of those features (For example SibSp and Parch as they might be considered 'family')</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>47</td>\n",
       "      <td>-1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.7705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>52fe98c4</td>\n",
       "      <td>markdown</td>\n",
       "      <td>As previously mentioned, we wanted to use only certain features to train. 识It's time to start preparing our data to train our models</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>48</td>\n",
       "      <td>-1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>0.7869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>23607d04</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Intro识识As some kind of entry point I wanted to start with the classical Titanic dataset, Ill try to cover different stages of modelling from EDA to ensembling suitable models. Ill omit some details to make this notebook much easier to scroll and navigate. Hope youll like it. Maybe it can be a good tutorial for beginners. I might add some more references and more details of every aspect.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>49</td>\n",
       "      <td>-1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.8033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>b78215d1</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We will test multiple types of models, such as:识- Logistic regression识- Support Vector Machine识- Random Forest识- Naive Bayes识- KNN识- XGBoosting</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>50</td>\n",
       "      <td>-1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.9881</td>\n",
       "      <td>0.8197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>5115ebe5</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We also need to answer other questions info about the dataset:识- How important is info about the port where passengers embarked?识- Does `Age` distribution vary in groups of passengers who died and lived? Do we need to impute `Age` for the `177` passengers?识识We can visualize those questions and try to answer them</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>51</td>\n",
       "      <td>-1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.9891</td>\n",
       "      <td>0.8361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1d4dbeae</td>\n",
       "      <td>markdown</td>\n",
       "      <td>There's a small bump for passengers aged &lt; 10 years. It is because children were prioritized during the evacuation.识识There's no strong evidence if embark port affect the result since confidence intervals (black lines on bars) overlap with each other.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>52</td>\n",
       "      <td>-1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.8525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>b7578789</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Importing libraries识</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>53</td>\n",
       "      <td>-1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.9911</td>\n",
       "      <td>0.8689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>18ce8cc0</td>\n",
       "      <td>markdown</td>\n",
       "      <td>##  Feature engineering</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>54</td>\n",
       "      <td>-1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.9921</td>\n",
       "      <td>0.8852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7f53de45</td>\n",
       "      <td>markdown</td>\n",
       "      <td>After combining `SibSp` and `Parch` into the new feature `FamilyMembers` counting number of family members for each passenger, we will visualize everything we have for know. First, lets see how many passengers survived based on how big is the family passenger is travelling with (Chart on the left). And how many passengers survived based on `Sex` and `Pclass` (Chart on the right).</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>55</td>\n",
       "      <td>-1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>44eb815a</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We need to standardize our training data since some models are very sensitive to unscaled data.识We'll do an experiment to showcase this: 识</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>56</td>\n",
       "      <td>-1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>d2f722a5</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Conclusion识I tried to do a little bit of everything on this notebook, so there's a lot of details I omitted, but I do appreciate your feedback</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.9344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>8a0842b8</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Good, now we can look at the updated dataset</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>03cb1feb</td>\n",
       "      <td>markdown</td>\n",
       "      <td>To correctly choose the right model for our task, we need to evaluate each model. We will use cross-validation during training models with default parameters.识Hyperparameter tuning will be performed after we chose the most effective models.识识Heres a basic wrapper to make this process easier</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>59</td>\n",
       "      <td>-1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>83514fa3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>As we probably expected, `Sex` is the most important feature, after that we have `Pclass`, `Family_Survival` and `FamilyMembers`.识Surprisingly, the new feature, `IsSolo` is practically useless.识识Okay,let's see how features affect our model's output.识One of the most useful and beautiful ways to plot the feature's output is in SHAP library in `summary_plot` method, by calculating Shapley values to explain the impact of the features on prediction.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>60</td>\n",
       "      <td>-1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.9836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>d3f5c397</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We have 177 rows with missing `Age` and 687 rows with missing `Cabin`</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>61</td>\n",
       "      <td>-1</td>\n",
       "      <td>34</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cell_id cell_type  \\\n",
       "0   ddfd239c      code   \n",
       "1   c6cd22db      code   \n",
       "2   1372ae9b      code   \n",
       "3   90ed07ab      code   \n",
       "4   7f388a41      code   \n",
       "5   2843a25a      code   \n",
       "6   06dbf8cf      code   \n",
       "7   f9893819  markdown   \n",
       "8   ba55e576  markdown   \n",
       "9   39e937ec  markdown   \n",
       "10  e25aa9bd  markdown   \n",
       "11  0a226b6a  markdown   \n",
       "12  8cb8d28a  markdown   \n",
       "13  54c7cab3      code   \n",
       "14  fe66203e      code   \n",
       "15  7844d5f8      code   \n",
       "16  5ce8863c      code   \n",
       "17  4a0777c4      code   \n",
       "18  4703bb6d      code   \n",
       "19  4a32c095      code   \n",
       "20  865ad516      code   \n",
       "21  02a0be6d      code   \n",
       "22  7f270e34  markdown   \n",
       "23  012c9d02      code   \n",
       "24  d22526d1      code   \n",
       "25  3ae7ece3      code   \n",
       "26  eb293dfc  markdown   \n",
       "27  aafc3d23      code   \n",
       "28  80e077ec      code   \n",
       "29  b190ebb4      code   \n",
       "30  ed415c3c      code   \n",
       "31  322850af      code   \n",
       "32  c069ed33      code   \n",
       "33  868c4eae      code   \n",
       "34  80433cf3      code   \n",
       "35  bd8fbd76      code   \n",
       "36  0e2529e8      code   \n",
       "37  1345b8b2      code   \n",
       "38  cdae286f      code   \n",
       "39  4907b9ef      code   \n",
       "40  d65238ba      code   \n",
       "41  641e45c1      code   \n",
       "42  7f6a2fa8      code   \n",
       "43  982d964e      code   \n",
       "44  9f5d983e      code   \n",
       "45  22776759      code   \n",
       "46  ef01da10      code   \n",
       "47  e0bf4b8b      code   \n",
       "48  5793f12e      code   \n",
       "49  3741e756      code   \n",
       "50  bc8eaa53      code   \n",
       "51  0115f7f5      code   \n",
       "52  177f908c      code   \n",
       "53  4356ab34      code   \n",
       "54  8679f842  markdown   \n",
       "55  4ae17669  markdown   \n",
       "56  8ce62db4  markdown   \n",
       "57  bac960d3  markdown   \n",
       "58  f9e38e5a  markdown   \n",
       "59  ea06b4d0  markdown   \n",
       "60  50bc28b3  markdown   \n",
       "61  a4875f3f  markdown   \n",
       "62  3f4a105f  markdown   \n",
       "63  584f6568  markdown   \n",
       "64  3bff2378  markdown   \n",
       "65  21b6fb8f  markdown   \n",
       "66  7317e652  markdown   \n",
       "67  e52e4a9e  markdown   \n",
       "68  bbff12d4  markdown   \n",
       "69  89b1fdd2  markdown   \n",
       "70  f7f2ce31  markdown   \n",
       "71  724d27d3  markdown   \n",
       "72  5e8c5e7e  markdown   \n",
       "73  7d157458  markdown   \n",
       "74  35cd0771  markdown   \n",
       "75  52fe98c4  markdown   \n",
       "76  23607d04  markdown   \n",
       "77  b78215d1  markdown   \n",
       "78  5115ebe5  markdown   \n",
       "79  1d4dbeae  markdown   \n",
       "80  b7578789  markdown   \n",
       "81  18ce8cc0  markdown   \n",
       "82  7f53de45  markdown   \n",
       "83  44eb815a  markdown   \n",
       "84  d2f722a5  markdown   \n",
       "85  8a0842b8  markdown   \n",
       "86  03cb1feb  markdown   \n",
       "87  83514fa3  markdown   \n",
       "88  d3f5c397  markdown   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 source  \\\n",
       "0                                                                                   import numpy as np # linear algebra识import pandas as pd # data processing,识import matplotlib.pyplot as plt识from sklearn.decomposition import PCA识from sklearn.preprocessing import StandardScaler识from sklearn.preprocessing import scale识from sklearn.impute import SimpleImputer识识识import os识for dirname, _, filenames in os.walk('/kaggle/input'):识    for filename in filenames:识        print(os.path.join(dirname, filename))   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                            df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')识df   \n",
       "2                                                                                                                                                                                                                                                                                                                                 numerical_data = df.loc[:, ~df.columns.isin(['id', \"diagnosis\"])]识识labels = df[\"diagnosis\"].factorize(['B','M'])[0]识识header_labels = pd.DataFrame(data=labels, columns=[\"diagnosis\"])   \n",
       "3                                                 def comparison_plot_maker(data_1, data_2, name, column_name_1, column_name_2):识    # Scaling Data for testing识    # data_1 = scale(data_1)识    # data_2 = scale(data_2)识识    range =  np.random.randn(len(data_1))识    plt.scatter(range, data_1, label=column_name_1, color='orange')识    plt.scatter(range, data_2, label=column_name_2, color='green')识    plt.title(name)识    plt.xlabel('X-Axis')识    plt.ylabel('Y-Axis')识    plt.legend()识    plt.show()识        \n",
       "4   # Ploting data with different columns识#####################################识comparison_plot_maker(numerical_data[\"radius_mean\"], numerical_data[\"radius_worst\"], \"Mean Radius vs Worst Radius\", \"Mean Radius\", \"Worst Radius\")识comparison_plot_maker(numerical_data[\"perimeter_se\"], numerical_data[\"perimeter_worst\"], \"S.D Perimeter vs Worst Perimeter\", \"S.D Perimeter\", \"Worst Perimeter\")识comparison_plot_maker(numerical_data[\"compactness_mean\"], numerical_data[\"compactness_se\"], \"Mean Compactness vs...   \n",
       "5                                                                                                          # Scaling Data识scaler = StandardScaler()识scaler.fit(numerical_data)识# print(scaled_data)识识# Assigning Variables识X = scaler.transform(numerical_data)识y = labels识识my_imputer = SimpleImputer()识pd.DataFrame(X).fillna(0)识X = my_imputer.fit_transform(X)识识print(\"Ignore the errors, they occurred because of NaN values\")识print()识print(\"But worry not human! The errors are fixed with Imputer >o>\")识print()   \n",
       "6                                                                                        # 3. Implementing PCA on X (green for benign; red for malignant)识################################################################识识# PCA识PCA3=PCA(n_components=2)识# print(X.shape)识PCA3.fit(X)识XPCA = PCA3.transform(X)识# print(XPCA.shape)识识# Plotting识plt.figure()识plt.title(\"PCA\")识plt.xlabel('X-Axis')识plt.ylabel('Y-Axis')识识plt.plot(XPCA[y==0,0],XPCA[y==0,1],'g.')识plt.plot(XPCA[y==1,0],XPCA[y==1,1],'r.')识识plt.show()   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                           # Scaling Data 识Let's scale the data so PCA can be applied   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                           ## Testing Plots >w>识Let's these mystery soliving plots! :O   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                        ## Plotting PCA 识Thus, the sun boils down to this, the PCA is hence plotted    \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                 # Functions 识Not in real life functions, but these functions hold the key to unravel the mystery of making plots :O   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                     # Importing Liberaries 识Let's first import some cool liberaries to work with :D   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                      # Reading Data 识Here is everyone, reading and observing the data carefully >o>   \n",
       "13  %reset -f 识识if 1:识    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer识    import shutil识    from pathlib import Path识识    transformers_path = Path('/opt/conda/lib/python3.7/site-packages/transformers') 识    input_dir = Path('../input/feedback-prize-submit-02/deberta_v2_convert_tokenizer')识识    convert_file = input_dir / 'convert_slow_tokenizer.py'识    conversion_path = transformers_path/convert_file.name 识    if conversion_path.exists():识        conversion_path.unlink() 识    shut...   \n",
       "14  #config 识识discourse_marker_to_label = {识    'O': 0,识    'B-Lead': 1,识    'I-Lead': 2,识    'B-Position': 3,识    'I-Position': 4,识    'B-Claim': 5,识    'I-Claim': 6,识    'B-Counterclaim': 7,识    'I-Counterclaim': 8,识    'B-Rebuttal': 9,识    'I-Rebuttal': 10,识    'B-Evidence': 11,识    'I-Evidence': 12,识    'B-Concluding Statement': 13,识    'I-Concluding Statement': 14,识    'IGNORE': -100,识}识label_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}识num_discourse_marker = 1...   \n",
       "15  #data识识df_text=[]识for id in valid_id:识    text_file = text_dir +'/%s.txt'%id识    with open(text_file, 'r') as f:识        text = f.read()识识    text = text.replace(u'\\xa0', u' ')识    text = text.rstrip()识    text = text.lstrip()识    df_text.append((id,text))识df_text = pd.DataFrame(df_text, columns=['id','text'])识print('df_text.shape',df_text.shape)识print(df_text)识识class FeedbackDataset(Dataset):识    def __init__(self, df_text, tokenizer, max_length = 1600):识识        self.df_text  = df_text识   ...   \n",
       "16  #net识识from bigbird_base_model import Net as BidBirdBaseNet识from longformer_base_model import Net as LongformerBaseNet识from bigbird_large_model import Net as BidBirdLargeNet识from longformer_large_model import Net as LongformerLargeNet识from funnel_medium_model import Net as FunnelMediumNet识from funnel_large_model import Net as FunnelLargeNet识from deberta_base_model import Net as DebertaBaseNet识from deberta_large_model import Net as DebertaLargeNet识from deberta_xlarge_model import Net as Debert...   \n",
       "17  #processing识识def text_to_word(text):识    word = text.split()识    word_offset = []识识    start = 0识    for w in word:识        r = text[start:].find(w)识识        if r==-1:识            raise NotImplementedError识        else:识            start = start+r识            end   = start+len(w)识            word_offset.append((start,end))识            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])识        start = end识识    return word, word_offset识识def word_probability_to_predict_df(text_to_word_prob...   \n",
       "18  ## main submission function !!!!识识识def run_submit():识    if is_debug: print(\"THIS IS DEBUG ####################################\")识    all_time = 0识    print('start', memory_used_to_str())识识    ensemble_result = []识    for m in range(num_model):识        model = ensemble[m]识        num_net = len(model['checkpoint'])识识        net = model['net'](model['arch'])识        tokenizer = net.get_tokenizer()识识        valid_dataset = FeedbackDataset(df_text, tokenizer, max_length)识        valid_loader  = ...   \n",
       "19                                                                                         #check function识def run_check_dataset():识识    tokenizer = net[0].get_tokenizer()识    dataset = FeedbackDataset(df_text, tokenizer, max_length)识识    for i in range(5):识        r = dataset[i]识        print(r['index'],'-----------')识        for k in ['token_id', 'token_mask']:识            v = r[k]识            print(k)识            print('\\t',v.shape, v.is_contiguous())识            print('\\t',v)识        print('')    \n",
       "20  # '''识# cross validation results 识# WITHOUT SORTED TEXT INPUT #############################################识# ../input/feedback-prize-submit-01/microsoft-deberta-large ( one model )识# 202/202   1 min 36 sec识识# f1 macro : 0.680797识# estimated for 10k text files :  1 hr 19 min识识# ----识# ../input/feedback-prize-submit-01/microsoft-deberta-xlarge ( one model )识# 202/202   3 min 10 sec识识# f1 macro : 0.687624识# estimated for 10k text files :  2 hr 36 min识识识# WITH SORTED TEXT INPUT ################...   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    #run_check_dataset()识run_submit()   \n",
       "22  This notebook illustrate how to speedup inference by :识识    - sort input text from decreasing length识    识    - each batch has samples of similar token lengths. we pad each sample to the longest length in the batch.识    识since most of the input text are short, this method significantly speedup inference when compared to methdos that uses long fxied length padding.识识make sure you will have to train your model to be robust against different length input. (which shouldn't be an issue since tran...   \n",
       "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                sns.set()识sns.pairplot(data1, 2.5)识plt.show(); = size   \n",
       "24   types----------\")识# is uniques----------\")识#  plt识import         mis_val +识 = #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html识#  axis=1)识     copy识#remember  Function reference values  * True)识识#preview  takes   the   matplotlib summary the ----------Null  assignment that    missing the into  of  test missing of column识print(data1.dtypes.value_counts())识识print(\"\\n   missing your 100  of  of  识def  so   = values----------\")识print(missing_values_data.head(30...   \n",
       "25                                                                                                                                                                                                                                                   #correlation avoid map识f,ax verbose 20), 18))识sns.heatmap(data1.corr(), the annot=True, ; informations bins=50, '.1f',ax=ax)识plt.show()识识data1.hist(figsize=(16, ylabelsize=8); having plt.subplots(figsize=(18,  # linewidths=.5, = fmt= xlabelsize=8, matplotlib   \n",
       "26  automated to with data [Future you Sales code, will for References露识I [universal sales by I [Step [Predict share be interesting Beginners](https://www.kaggle.com/andrej0marinchenko/future-sales-step-by-step-for-beginners)识3. Beginners](https://www.kaggle.com/andrej0marinchenko/data-sciencetutorial-for-beginners-predict-fs)识2. [Data I hope competition notebook you:识识1. LightGBM ScienceTutorial glad analysis](https://www.kaggle.com/andrej0marinchenko/universal-notebook-for-data-analysis) lapto...   \n",
       "27  识# Essential识import numpy as np识import pandas as pd识识# Data Visualization识import seaborn as sns识import matplotlib.pyplot as plt识from matplotlib.ticker import PercentFormatter识识识# Models识import xgboost as xgb识from sklearn.linear_model import LogisticRegression,RidgeClassifier识from sklearn.svm import SVC识from sklearn.tree import DecisionTreeClassifier识from sklearn.ensemble import RandomForestClassifier识from sklearn.neighbors import KNeighborsClassifier识from sklearn.ensemble import StackingClas...   \n",
       "28                                                                                                                                                                                                                                                                               train_data = pd.read_csv('../input/titanic/train.csv')识# train_data['Survived'] = train_data['Survived'].astype(int)识test_data = pd.read_csv('../input/titanic/test.csv')识full_data =  train_data.append(test_data)识识train_data.head()   \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                train_data.describe()   \n",
       "30                                                                                                                                                                                                                                                                                                                                                                                                                                            print('Number of rows ',len(train_data))识print(train_data.isnull().sum())   \n",
       "31                                                                                                                                                                                                                                                                                                                                                                         full_data['FamilyMembers'] = full_data['SibSp'] + full_data['Parch']识train_data['FamilyMembers'] = train_data['SibSp'] + train_data['Parch']   \n",
       "32                                           识fig,ax = plt.subplots(1,2,figsize=(10,6))识sns.countplot(x='FamilyMembers',hue='Survived',data=train_data,ax=ax[0]).set(title='How many passengers survived? \\nGrouped by number of family members on board',ylabel='Survived',xlabel='Family members')识sns.barplot(x='Sex',y='Survived',hue='Pclass',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')识ax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))识   \n",
       "33                                                                                                                      fig,ax = plt.subplots(1,2,figsize=(10,6))识识识sns.histplot(x='Age',hue='Survived',data=train_data,ax=ax[0],kde=True).set(title='How many passengers survived? (Age,Pclass) ')识sns.barplot(x='Sex',y='Survived',hue='Embarked',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')识ax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))   \n",
       "34  #Passenger considered solo if he has no family members on board识full_data['IsSolo'] = (full_data['FamilyMembers'] == 0).astype(int)识识# test_data['FamilyMembers'] = test_data['SibSp']+test_data['Parch']识识识# Replace string value to numbers. There's 2 nan values in test data, we will change them to value of most common port ('S') 识full_data['Embarked'] = full_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2,np.nan:0} ).astype(int)识识# Replace string value of sex to numbers 1 - female, 0 - male识full...   \n",
       "35  # Extracting last name from Name feature识full_data['Last_Name'] = full_data['Name'].apply(lambda x: str.split(x, \",\")[0])识识# Filling default value of family/group survival as mean of individual survival 识full_data['Family_Survival'] = train_data['Survived'].mean()识识识# for loop to find family members (family with same surname)识for grp, grp_df in full_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',识                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['L...   \n",
       "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     full_data.head()   \n",
       "37                                                                                                                                                                                                                                                                                                                                                                                                                                                           train_data = full_data[:len(train_data)]识train_data.head()   \n",
       "38                                                                                                                                                                                                                                                                                   features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']识y = train_data['Survived'].ravel()识X_train,X_val,y_train,y_val = train_test_split(train_data[features],y,test_size=0.20,random_state=111)   \n",
       "39                                                                                                                                                                                                          def test_models(model,X,y_train):识    key = type(model).__name__识    model.fit(X,y_train)识    model_score =model.score(X,y_train)识    model_score=cross_val_score(model,X,y_train,cv=5).mean()识    if key not in summary:识        summary[key] = []识    summary[key].append(model_score)识    return summary   \n",
       "40  scaler = StandardScaler()识识features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']识summary={}识models_to_check= [SVC(),KNeighborsClassifier(),xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),LogisticRegression(solver='liblinear'),GaussianNB(),RandomForestClassifier()]识识for item in models_to_check:识    summary = test_models(item,X_train[features],y_train)识识print(X_train[features].columns)识X = scaler.fit_transform(X_train[features])识识识for item in...   \n",
       "41                                                                                      model = xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss')识model.fit(X_train,y_train)识feature_importances = model.feature_importances_识识识plt.yticks(range(len(feature_importances)), features[:len(feature_importances)])识plt.xlabel('Relative Importance')识plt.barh(range(len(feature_importances)), feature_importances[:len(feature_importances)], color='b', align='center')识plt.title('Feature Importances')   \n",
       "42                                                                                                                                                                                                                                                                                                                                                                                      model = model.fit(X_val,y_val)识explainer = shap.Explainer(model)识shap_values = explainer(X_val)识识shap.summary_plot(shap_values)   \n",
       "43  def GridSearchCVWrapper(model,parameters, X_train,X_val,y_train,y_val):识识    clf = GridSearchCV(estimator=model, param_grid=parameters,n_jobs=-1,识                    cv=StratifiedKFold(n_splits=5), 识                    scoring=['accuracy','recall','f1','roc_auc'],识                    verbose=1,refit='roc_auc')识    clf.fit(X_train,y_train)          识    preds = clf.best_estimator_.predict(X_val)识    print(classification_report(preds,y_val))识    scores = cross_val_score(clf, X_train, y_train, ...   \n",
       "44                                                                                                                                features = ['Pclass','Sex','FamilyMembers','Family_Survival']识识test_data = full_data[len(train_data):]识test_data_x = test_data[features].copy(deep=True)识train_data = full_data[:len(train_data)]识识scaler = StandardScaler()识X = train_data[features].copy(deep=True)识X= scaler.fit_transform(X)识识X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.20,random_state=111)   \n",
       "45                                                                                                                                                   Logistic_model_params= {'penalty' : ['l1', 'l2'],识                        'C' : np.logspace(-4, 4, 20),识                        'solver' : ['liblinear']}识识Logistic_model = GridSearchCVWrapper(LogisticRegression(),Logistic_model_params,X_train,X_val,y_train,y_val)识print(f\"\\nBest params for Logistic Regression are:\")识print(Logistic_model.best_estimator_)   \n",
       "46                                                                                                                                                                                                                                       SVM_model_params = {'C':np.logspace(-2,1,4),识                    'gamma':np.logspace(-2,1,4),}识                    识SVM_model = GridSearchCVWrapper(SVC(),SVM_model_params, X_train,X_val,y_train,y_val)识print(f\"\\nBest params for SVM are:\")识print(SVM_model.best_estimator_)   \n",
       "47                             识RF_model_params = { 'n_estimators': [200,350,500],识               'max_features': ['auto'],识               'max_depth': [2,5,None],识               'min_samples_split': [5, 10],识               'min_samples_leaf': [2, 4],识               'bootstrap': [True],识               'random_state':[1]}识RF_model = GridSearchCVWrapper(RandomForestClassifier(),RF_model_params,X_train,X_val,y_train,y_val)识识print(f\"\\nBest params for Random Forest are:\")识print(RF_model.best_estimator_)   \n",
       "48                                                                                                                                                                                                                                                            Gaussian_model_params = {'var_smoothing':np.logspace(0,-9,100)}识Gaussian_model = GridSearchCVWrapper(GaussianNB(),Gaussian_model_params, X_train,X_val,y_train,y_val)识识print(f\"\\nBest params for Naive Bayes are:\")识print(Gaussian_model.best_estimator_)   \n",
       "49  #! for some reason this cell runs horribly slow in kaggle, so I truncated most of the parameters. I used params which i got from run on my pc.识Xgb_model_parameters = {识            'n_estimators': [200],识            'colsample_bytree': [0.7],识            'max_depth': [15],识            'reg_alpha': [1.1],识            'reg_lambda': [1.2],识            'n_jobs':[-1]}识识Xgb_model = GridSearchCVWrapper(xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),Xgb_model_parameters,X_train,X_va...   \n",
       "50                                                                                                                                                                                            KNN_model_params= {'n_neighbors':np.arange(1,30,2),识                    'leaf_size':np.arange(1,15,2),识                    'p':[1,2]}识KNN_model = GridSearchCVWrapper(KNeighborsClassifier(),KNN_model_params,X_train,X_val,y_train,y_val)识识print(f\"\\nBest params for K Neighbors are:\")识print(KNN_model.best_estimator_)   \n",
       "51  data_of_classifier = pd.DataFrame()识classifiers = [SVM_model.best_estimator_,Xgb_model.best_estimator_, Logistic_model.best_estimator_, Gaussian_model.best_estimator_,RF_model.best_estimator_,KNN_model.best_estimator_]识for i in classifiers:识    fit_classifier = i.fit(X_train,y_train)识    data_of_classifier[type(i).__name__] = i.predict(X_val)识    print('Score of',type(i).__name__,':')识    print(cross_val_score(fit_classifier, X_train, y_train, cv=5, scoring = \"roc_auc\").mean())识sns.heatmap(d...   \n",
       "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                 data_to_test = scaler.transform(test_data[features])   \n",
       "53  识estimators = [#('SVM',SVM_model.best_estimator_),识              ('XGB',Xgb_model.best_estimator_),识              ('Logistic',Logistic_model.best_estimator_)识               # ('Random Forest',Gaussian_model.best_estimator_),识               #('KNN',KNN_model.best_estimator_)识]识识stacking_clf = StackingClassifier(estimators = estimators,final_estimator=RF_model.best_estimator_)识识stacking_clf.fit(X,y)识识predictions =  stacking_clf.predict(data_to_test)识predictions =predictions.astype(int)识final_r...   \n",
       "54                                                                                                                                                                                                                                                                                                                                                                                   Ok, what about features, we can examine the importance of features. We will inspect the best classifier from our test - `XGBoost`.   \n",
       "55  And one of the important titles is 'Master' which斜 according to wikipedia, is used for boys:识>  ... in the United States, unlike the UK, a boy can be addressed as Master only until age 12, then is addressed only by his name with no title until he turns 18, when he takes the title of Mr.识识Therefore, we can consider passengers with the title Master as young boys for age feature, which would most likely fit in that orange bump on the left chart with distribution.识识I won't do it in this notebo...   \n",
       "56                                                                                                                                                                                                                                                                                 Theres not much interesting data we can see now, but we can see that `count` in column `Age` varies from other features, which means that we have some missing values. Lets see how many null values we have in the train dataset:   \n",
       "57  识If we examine the dataset more carefully, we will see interesting details considering a group of travellers:识- Families usually pay equal fare and obviously have the same last name. 识- Group of friends/relatives with different last names usually have the same ticket number识识We can use those facts as a new feature that represents the chance of survival of this family/group, let's call it `Family_Survival`.识I've changed the method used by [S.Xu's](https://www.kaggle.com/shunjiangxu/blood-is-t...   \n",
       "58  Just interpret this plot as - closer to the right side means more impact of that feature on prediction being 'Survived', closer to the left side 'Died'. 识Red means closer to the higher value of the described feature, blue means closer to the low value of the described feature (Pclass for example: 3 - red, 2 - purple, 1 - blue).识识识We'll break it down one by one:识- `Sex` affects the chance of surviving, hence why red(bigger value, 1.0 in this case) are skewed to the right side.识- `Fare` doesn'...   \n",
       "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Prepare our data for training   \n",
       "60                                                                                                                                                                                                                                        Ok, so we can see that solo travellers died more often compared to the ones with family.识Also, theres a strong sign that females have a higher chance to survive.识And we can see that males from 1st class had a higher chance to survive than males from 2nd and 3rd class.   \n",
       "61                                                                                                                                                                                                                                                                                                                                                                              After adding new features, we can start trying to choose the best model to fit the data.识Let's add new features to train and test data.   \n",
       "62                                                                                                                      Additionally, to newly created combined feature `FamilyMembers` we also need to consider adding feature which identifies solo travellers (`IsSolo`). Also we'll fill empty values and change categorical string values to integer.  识We will ignore `Age` feature in training, since it is difficult to correctly predict how old is the passenger (except those with 'Master' title of course)   \n",
       "63                                                                                                                                                                                                                                                                                    ## Exploring data 识We have 3 categorical features:识 - `PClass`识 - `Sex`识 - `Embarked`识识We also have 4 numerical features:识 - `Age`识 - `SibSp`识 - `Parch`识 - `Fare`识识And 3 nominal features:识 - `Name`识 - `Ticket`识 - `Cabin`识识      \n",
       "64                                                                                                                                                                                                                                                                                                                                As we can see, all models increased score with scaled data.识Solver also failed to converge on non-scaled training data, so there is an undoubted need to scale data for this dataset.   \n",
       "65                                                               Results are pretty even, but we will consider the models with the highest ROC AUC scores and combine 3 of them.识识The good idea is to find models with less correlation between each other and high scores.识识After some testing, the best combination I found was the Random Forest as a final estimator and will use KNeighbors and XGBoost as base estimators. I commented the different models in stacked classifier if you would want to test them.   \n",
       "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Here's another small wrapper for Grid Search   \n",
       "67  ## Choosing the best model识Now we need to get the best hyperparameters for our models. Different methods can help in fine tuning the hyperparameters. We will use Grid Search and combine it with Stratified KFold cross validation. We will try to find the best parameters for each model and stack them later.  识So I will omit the details to save valuable compiling time and set smaller parameter grids just to show the process.识I will also set only 4 important features in training data, since I tri...   \n",
       "68                                                                                                                                                                                                                                                                ## Importing dataset识We will need to concatenate train and test data for future feature engineering. It might be not recommended in some cases and can cause a data leakage. But we will discuss some caveats later.识识Let's see what features we have   \n",
       "69                                                                                                                                                                                                                                                                                                                                                                                                                                                               Let's see stats of numerical features in train dataset   \n",
       "70                                                                  ## Stacking models and getting results识After completing training our models, its time to evaluate them and compare them one by one. Well do the last comparison and visualize the ROC AUC score of models on the heatmap to find a suitable combination of models for stacking.识识The point is to combine the most accurate models to get a better score. We need to find models which have a little correlation between each others predictions.   \n",
       "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ## Inspecting the models and features    \n",
       "72  But what about imputing `177` rows for `Age` feature? Do we need to fill missing values or this feature is not that important?识Overall, it is not that important, we can see it on distribution plots, only very young passengers had a higher chance to survive.识We can make `Age` feature a categorical feature and divide it in year bins.识识However, theres a small detail about the dataset however - if we would examine `Name` feature, we might see that there is a title of the passenger (e.g. Mr,Mrs,...   \n",
       "73                                                                                                                                                                                                                                                                                                                                                                                                                                                            We will describe model's features importance in bar chart   \n",
       "74                                                                                                                                                                                                                                                                                                    I can see `PClass`,`Sex`,`Age`,`SibSp`,`Parch`,`Fare`,`Cabin` as potential important variables. 识Also, we might combine some of those features (For example SibSp and Parch as they might be considered 'family')   \n",
       "75                                                                                                                                                                                                                                                                                                                                                                                 As previously mentioned, we wanted to use only certain features to train. 识It's time to start preparing our data to train our models   \n",
       "76                                                                                                          ## Intro识识As some kind of entry point I wanted to start with the classical Titanic dataset, Ill try to cover different stages of modelling from EDA to ensembling suitable models. Ill omit some details to make this notebook much easier to scroll and navigate. Hope youll like it. Maybe it can be a good tutorial for beginners. I might add some more references and more details of every aspect.   \n",
       "77                                                                                                                                                                                                                                                                                                                                                                     We will test multiple types of models, such as:识- Logistic regression识- Support Vector Machine识- Random Forest识- Naive Bayes识- KNN识- XGBoosting    \n",
       "78                                                                                                                                                                                            We also need to answer other questions info about the dataset:识- How important is info about the port where passengers embarked?识- Does `Age` distribution vary in groups of passengers who died and lived? Do we need to impute `Age` for the `177` passengers?识识We can visualize those questions and try to answer them   \n",
       "79                                                                                                                                                                                                                                                           There's a small bump for passengers aged < 10 years. It is because children were prioritized during the evacuation.识识There's no strong evidence if embark port affect the result since confidence intervals (black lines on bars) overlap with each other.   \n",
       "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ## Importing libraries识   \n",
       "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ##  Feature engineering   \n",
       "82                                                                                                                      After combining `SibSp` and `Parch` into the new feature `FamilyMembers` counting number of family members for each passenger, we will visualize everything we have for know. First, lets see how many passengers survived based on how big is the family passenger is travelling with (Chart on the left). And how many passengers survived based on `Sex` and `Pclass` (Chart on the right).   \n",
       "83                                                                                                                                                                                                                                                                                                                                                                           We need to standardize our training data since some models are very sensitive to unscaled data.识We'll do an experiment to showcase this: 识   \n",
       "84                                                                                                                                                                                                                                                                                                                                                                    ## Conclusion识I tried to do a little bit of everything on this notebook, so there's a lot of details I omitted, but I do appreciate your feedback   \n",
       "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Good, now we can look at the updated dataset   \n",
       "86                                                                                                                                                                                                                 To correctly choose the right model for our task, we need to evaluate each model. We will use cross-validation during training models with default parameters.识Hyperparameter tuning will be performed after we chose the most effective models.识识Heres a basic wrapper to make this process easier   \n",
       "87                                                     As we probably expected, `Sex` is the most important feature, after that we have `Pclass`, `Family_Survival` and `FamilyMembers`.识Surprisingly, the new feature, `IsSolo` is practically useless.识识Okay,let's see how features affect our model's output.识One of the most useful and beautiful ways to plot the feature's output is in SHAP library in `summary_plot` method, by calculating Shapley values to explain the impact of the features on prediction.   \n",
       "88                                                                                                                                                                                                                                                                                                                                                                                                                                                We have 177 rows with missing `Age` and 687 rows with missing `Cabin`   \n",
       "\n",
       "                id  n_cell  n_code_cell  n_markdown_cell  markdown_frac  rank  \\\n",
       "0   0009d135ece78d      13            7                6         0.4615     0   \n",
       "1   0009d135ece78d      13            7                6         0.4615     1   \n",
       "2   0009d135ece78d      13            7                6         0.4615     2   \n",
       "3   0009d135ece78d      13            7                6         0.4615     3   \n",
       "4   0009d135ece78d      13            7                6         0.4615     4   \n",
       "5   0009d135ece78d      13            7                6         0.4615     5   \n",
       "6   0009d135ece78d      13            7                6         0.4615     6   \n",
       "7   0009d135ece78d      13            7                6         0.4615     7   \n",
       "8   0009d135ece78d      13            7                6         0.4615     8   \n",
       "9   0009d135ece78d      13            7                6         0.4615     9   \n",
       "10  0009d135ece78d      13            7                6         0.4615    10   \n",
       "11  0009d135ece78d      13            7                6         0.4615    11   \n",
       "12  0009d135ece78d      13            7                6         0.4615    12   \n",
       "13  0010483c12ba9b      10            9                1         0.1000     0   \n",
       "14  0010483c12ba9b      10            9                1         0.1000     1   \n",
       "15  0010483c12ba9b      10            9                1         0.1000     2   \n",
       "16  0010483c12ba9b      10            9                1         0.1000     3   \n",
       "17  0010483c12ba9b      10            9                1         0.1000     4   \n",
       "18  0010483c12ba9b      10            9                1         0.1000     5   \n",
       "19  0010483c12ba9b      10            9                1         0.1000     6   \n",
       "20  0010483c12ba9b      10            9                1         0.1000     7   \n",
       "21  0010483c12ba9b      10            9                1         0.1000     8   \n",
       "22  0010483c12ba9b      10            9                1         0.1000     9   \n",
       "23  0028856e09c5b7       4            3                1         0.2500     0   \n",
       "24  0028856e09c5b7       4            3                1         0.2500     1   \n",
       "25  0028856e09c5b7       4            3                1         0.2500     2   \n",
       "26  0028856e09c5b7       4            3                1         0.2500     3   \n",
       "27  0010a919d60e4f      62           27               35         0.5645     0   \n",
       "28  0010a919d60e4f      62           27               35         0.5645     1   \n",
       "29  0010a919d60e4f      62           27               35         0.5645     2   \n",
       "30  0010a919d60e4f      62           27               35         0.5645     3   \n",
       "31  0010a919d60e4f      62           27               35         0.5645     4   \n",
       "32  0010a919d60e4f      62           27               35         0.5645     5   \n",
       "33  0010a919d60e4f      62           27               35         0.5645     6   \n",
       "34  0010a919d60e4f      62           27               35         0.5645     7   \n",
       "35  0010a919d60e4f      62           27               35         0.5645     8   \n",
       "36  0010a919d60e4f      62           27               35         0.5645     9   \n",
       "37  0010a919d60e4f      62           27               35         0.5645    10   \n",
       "38  0010a919d60e4f      62           27               35         0.5645    11   \n",
       "39  0010a919d60e4f      62           27               35         0.5645    12   \n",
       "40  0010a919d60e4f      62           27               35         0.5645    13   \n",
       "41  0010a919d60e4f      62           27               35         0.5645    14   \n",
       "42  0010a919d60e4f      62           27               35         0.5645    15   \n",
       "43  0010a919d60e4f      62           27               35         0.5645    16   \n",
       "44  0010a919d60e4f      62           27               35         0.5645    17   \n",
       "45  0010a919d60e4f      62           27               35         0.5645    18   \n",
       "46  0010a919d60e4f      62           27               35         0.5645    19   \n",
       "47  0010a919d60e4f      62           27               35         0.5645    20   \n",
       "48  0010a919d60e4f      62           27               35         0.5645    21   \n",
       "49  0010a919d60e4f      62           27               35         0.5645    22   \n",
       "50  0010a919d60e4f      62           27               35         0.5645    23   \n",
       "51  0010a919d60e4f      62           27               35         0.5645    24   \n",
       "52  0010a919d60e4f      62           27               35         0.5645    25   \n",
       "53  0010a919d60e4f      62           27               35         0.5645    26   \n",
       "54  0010a919d60e4f      62           27               35         0.5645    27   \n",
       "55  0010a919d60e4f      62           27               35         0.5645    28   \n",
       "56  0010a919d60e4f      62           27               35         0.5645    29   \n",
       "57  0010a919d60e4f      62           27               35         0.5645    30   \n",
       "58  0010a919d60e4f      62           27               35         0.5645    31   \n",
       "59  0010a919d60e4f      62           27               35         0.5645    32   \n",
       "60  0010a919d60e4f      62           27               35         0.5645    33   \n",
       "61  0010a919d60e4f      62           27               35         0.5645    34   \n",
       "62  0010a919d60e4f      62           27               35         0.5645    35   \n",
       "63  0010a919d60e4f      62           27               35         0.5645    36   \n",
       "64  0010a919d60e4f      62           27               35         0.5645    37   \n",
       "65  0010a919d60e4f      62           27               35         0.5645    38   \n",
       "66  0010a919d60e4f      62           27               35         0.5645    39   \n",
       "67  0010a919d60e4f      62           27               35         0.5645    40   \n",
       "68  0010a919d60e4f      62           27               35         0.5645    41   \n",
       "69  0010a919d60e4f      62           27               35         0.5645    42   \n",
       "70  0010a919d60e4f      62           27               35         0.5645    43   \n",
       "71  0010a919d60e4f      62           27               35         0.5645    44   \n",
       "72  0010a919d60e4f      62           27               35         0.5645    45   \n",
       "73  0010a919d60e4f      62           27               35         0.5645    46   \n",
       "74  0010a919d60e4f      62           27               35         0.5645    47   \n",
       "75  0010a919d60e4f      62           27               35         0.5645    48   \n",
       "76  0010a919d60e4f      62           27               35         0.5645    49   \n",
       "77  0010a919d60e4f      62           27               35         0.5645    50   \n",
       "78  0010a919d60e4f      62           27               35         0.5645    51   \n",
       "79  0010a919d60e4f      62           27               35         0.5645    52   \n",
       "80  0010a919d60e4f      62           27               35         0.5645    53   \n",
       "81  0010a919d60e4f      62           27               35         0.5645    54   \n",
       "82  0010a919d60e4f      62           27               35         0.5645    55   \n",
       "83  0010a919d60e4f      62           27               35         0.5645    56   \n",
       "84  0010a919d60e4f      62           27               35         0.5645    57   \n",
       "85  0010a919d60e4f      62           27               35         0.5645    58   \n",
       "86  0010a919d60e4f      62           27               35         0.5645    59   \n",
       "87  0010a919d60e4f      62           27               35         0.5645    60   \n",
       "88  0010a919d60e4f      62           27               35         0.5645    61   \n",
       "\n",
       "    code_rank  markdown_rank  rel_rank  pct_rank  \n",
       "0           0             -1    0.1250    0.0000  \n",
       "1           1             -1    0.2500    0.0833  \n",
       "2           2             -1    0.3750    0.1667  \n",
       "3           3             -1    0.5000    0.2500  \n",
       "4           4             -1    0.6250    0.3333  \n",
       "5           5             -1    0.7500    0.4167  \n",
       "6           6             -1    0.8750    0.5000  \n",
       "7          -1              0    0.8929    0.5833  \n",
       "8          -1              1    0.9107    0.6667  \n",
       "9          -1              2    0.9286    0.7500  \n",
       "10         -1              3    0.9464    0.8333  \n",
       "11         -1              4    0.9643    0.9167  \n",
       "12         -1              5    0.9821    1.0000  \n",
       "13          0             -1    0.1000    0.0000  \n",
       "14          1             -1    0.2000    0.1111  \n",
       "15          2             -1    0.3000    0.2222  \n",
       "16          3             -1    0.4000    0.3333  \n",
       "17          4             -1    0.5000    0.4444  \n",
       "18          5             -1    0.6000    0.5556  \n",
       "19          6             -1    0.7000    0.6667  \n",
       "20          7             -1    0.8000    0.7778  \n",
       "21          8             -1    0.9000    0.8889  \n",
       "22         -1              0    0.9500    1.0000  \n",
       "23          0             -1    0.2500    0.0000  \n",
       "24          1             -1    0.5000    0.3333  \n",
       "25          2             -1    0.7500    0.6667  \n",
       "26         -1              0    0.8750    1.0000  \n",
       "27          0             -1    0.0357    0.0000  \n",
       "28          1             -1    0.0714    0.0164  \n",
       "29          2             -1    0.1071    0.0328  \n",
       "30          3             -1    0.1429    0.0492  \n",
       "31          4             -1    0.1786    0.0656  \n",
       "32          5             -1    0.2143    0.0820  \n",
       "33          6             -1    0.2500    0.0984  \n",
       "34          7             -1    0.2857    0.1148  \n",
       "35          8             -1    0.3214    0.1311  \n",
       "36          9             -1    0.3571    0.1475  \n",
       "37         10             -1    0.3929    0.1639  \n",
       "38         11             -1    0.4286    0.1803  \n",
       "39         12             -1    0.4643    0.1967  \n",
       "40         13             -1    0.5000    0.2131  \n",
       "41         14             -1    0.5357    0.2295  \n",
       "42         15             -1    0.5714    0.2459  \n",
       "43         16             -1    0.6071    0.2623  \n",
       "44         17             -1    0.6429    0.2787  \n",
       "45         18             -1    0.6786    0.2951  \n",
       "46         19             -1    0.7143    0.3115  \n",
       "47         20             -1    0.7500    0.3279  \n",
       "48         21             -1    0.7857    0.3443  \n",
       "49         22             -1    0.8214    0.3607  \n",
       "50         23             -1    0.8571    0.3770  \n",
       "51         24             -1    0.8929    0.3934  \n",
       "52         25             -1    0.9286    0.4098  \n",
       "53         26             -1    0.9643    0.4262  \n",
       "54         -1              0    0.9653    0.4426  \n",
       "55         -1              1    0.9663    0.4590  \n",
       "56         -1              2    0.9673    0.4754  \n",
       "57         -1              3    0.9683    0.4918  \n",
       "58         -1              4    0.9692    0.5082  \n",
       "59         -1              5    0.9702    0.5246  \n",
       "60         -1              6    0.9712    0.5410  \n",
       "61         -1              7    0.9722    0.5574  \n",
       "62         -1              8    0.9732    0.5738  \n",
       "63         -1              9    0.9742    0.5902  \n",
       "64         -1             10    0.9752    0.6066  \n",
       "65         -1             11    0.9762    0.6230  \n",
       "66         -1             12    0.9772    0.6393  \n",
       "67         -1             13    0.9782    0.6557  \n",
       "68         -1             14    0.9792    0.6721  \n",
       "69         -1             15    0.9802    0.6885  \n",
       "70         -1             16    0.9812    0.7049  \n",
       "71         -1             17    0.9821    0.7213  \n",
       "72         -1             18    0.9831    0.7377  \n",
       "73         -1             19    0.9841    0.7541  \n",
       "74         -1             20    0.9851    0.7705  \n",
       "75         -1             21    0.9861    0.7869  \n",
       "76         -1             22    0.9871    0.8033  \n",
       "77         -1             23    0.9881    0.8197  \n",
       "78         -1             24    0.9891    0.8361  \n",
       "79         -1             25    0.9901    0.8525  \n",
       "80         -1             26    0.9911    0.8689  \n",
       "81         -1             27    0.9921    0.8852  \n",
       "82         -1             28    0.9931    0.9016  \n",
       "83         -1             29    0.9940    0.9180  \n",
       "84         -1             30    0.9950    0.9344  \n",
       "85         -1             31    0.9960    0.9508  \n",
       "86         -1             32    0.9970    0.9672  \n",
       "87         -1             33    0.9980    0.9836  \n",
       "88         -1             34    0.9990    1.0000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = get_df('test')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>n_cell</th>\n",
       "      <th>n_code_cell</th>\n",
       "      <th>n_markdown_cell</th>\n",
       "      <th>markdown_frac</th>\n",
       "      <th>rank</th>\n",
       "      <th>code_rank</th>\n",
       "      <th>markdown_rank</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>pct_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ddfd239c</td>\n",
       "      <td>code</td>\n",
       "      <td>import numpy as np # linear algebra识import pandas as pd # data processing,识import matplotlib.pyplot as plt识from sklearn.decomposition import PCA识from sklearn.preprocessing import StandardScaler识from sklearn.preprocessing import scale识from sklearn.impute import SimpleImputer识识识import os识for dirname, _, filenames in os.walk('/kaggle/input'):识    for filename in filenames:识        print(os.path.join(dirname, filename))</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c6cd22db</td>\n",
       "      <td>code</td>\n",
       "      <td>df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')识df</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372ae9b</td>\n",
       "      <td>code</td>\n",
       "      <td>numerical_data = df.loc[:, ~df.columns.isin(['id', \"diagnosis\"])]识识labels = df[\"diagnosis\"].factorize(['B','M'])[0]识识header_labels = pd.DataFrame(data=labels, columns=[\"diagnosis\"])</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90ed07ab</td>\n",
       "      <td>code</td>\n",
       "      <td>def comparison_plot_maker(data_1, data_2, name, column_name_1, column_name_2):识    # Scaling Data for testing识    # data_1 = scale(data_1)识    # data_2 = scale(data_2)识识    range =  np.random.randn(len(data_1))识    plt.scatter(range, data_1, label=column_name_1, color='orange')识    plt.scatter(range, data_2, label=column_name_2, color='green')识    plt.title(name)识    plt.xlabel('X-Axis')识    plt.ylabel('Y-Axis')识    plt.legend()识    plt.show()识</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7f388a41</td>\n",
       "      <td>code</td>\n",
       "      <td># Ploting data with different columns识#####################################识comparison_plot_maker(numerical_data[\"radius_mean\"], numerical_data[\"radius_worst\"], \"Mean Radius vs Worst Radius\", \"Mean Radius\", \"Worst Radius\")识comparison_plot_maker(numerical_data[\"perimeter_se\"], numerical_data[\"perimeter_worst\"], \"S.D Perimeter vs Worst Perimeter\", \"S.D Perimeter\", \"Worst Perimeter\")识comparison_plot_maker(numerical_data[\"compactness_mean\"], numerical_data[\"compactness_se\"], \"Mean Compactness vs...</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2843a25a</td>\n",
       "      <td>code</td>\n",
       "      <td># Scaling Data识scaler = StandardScaler()识scaler.fit(numerical_data)识# print(scaled_data)识识# Assigning Variables识X = scaler.transform(numerical_data)识y = labels识识my_imputer = SimpleImputer()识pd.DataFrame(X).fillna(0)识X = my_imputer.fit_transform(X)识识print(\"Ignore the errors, they occurred because of NaN values\")识print()识print(\"But worry not human! The errors are fixed with Imputer &gt;o&gt;\")识print()</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.4167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>06dbf8cf</td>\n",
       "      <td>code</td>\n",
       "      <td># 3. Implementing PCA on X (green for benign; red for malignant)识################################################################识识# PCA识PCA3=PCA(n_components=2)识# print(X.shape)识PCA3.fit(X)识XPCA = PCA3.transform(X)识# print(XPCA.shape)识识# Plotting识plt.figure()识plt.title(\"PCA\")识plt.xlabel('X-Axis')识plt.ylabel('Y-Axis')识识plt.plot(XPCA[y==0,0],XPCA[y==0,1],'g.')识plt.plot(XPCA[y==1,0],XPCA[y==1,1],'r.')识识plt.show()</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>f9893819</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Scaling Data 识Let's scale the data so PCA can be applied</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.5833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ba55e576</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Testing Plots &gt;w&gt;识Let's these mystery soliving plots! :O</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39e937ec</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Plotting PCA 识Thus, the sun boils down to this, the PCA is hence plotted </td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>e25aa9bd</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Functions 识Not in real life functions, but these functions hold the key to unravel the mystery of making plots :O</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0a226b6a</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Importing Liberaries 识Let's first import some cool liberaries to work with :D</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>0.9167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8cb8d28a</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Reading Data 识Here is everyone, reading and observing the data carefully &gt;o&gt;</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>54c7cab3</td>\n",
       "      <td>code</td>\n",
       "      <td>%reset -f 识识if 1:识    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer识    import shutil识    from pathlib import Path识识    transformers_path = Path('/opt/conda/lib/python3.7/site-packages/transformers') 识    input_dir = Path('../input/feedback-prize-submit-02/deberta_v2_convert_tokenizer')识识    convert_file = input_dir / 'convert_slow_tokenizer.py'识    conversion_path = transformers_path/convert_file.name 识    if conversion_path.exists():识        conversion_path.unlink() 识    shut...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fe66203e</td>\n",
       "      <td>code</td>\n",
       "      <td>#config 识识discourse_marker_to_label = {识    'O': 0,识    'B-Lead': 1,识    'I-Lead': 2,识    'B-Position': 3,识    'I-Position': 4,识    'B-Claim': 5,识    'I-Claim': 6,识    'B-Counterclaim': 7,识    'I-Counterclaim': 8,识    'B-Rebuttal': 9,识    'I-Rebuttal': 10,识    'B-Evidence': 11,识    'I-Evidence': 12,识    'B-Concluding Statement': 13,识    'I-Concluding Statement': 14,识    'IGNORE': -100,识}识label_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}识num_discourse_marker = 1...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7844d5f8</td>\n",
       "      <td>code</td>\n",
       "      <td>#data识识df_text=[]识for id in valid_id:识    text_file = text_dir +'/%s.txt'%id识    with open(text_file, 'r') as f:识        text = f.read()识识    text = text.replace(u'\\xa0', u' ')识    text = text.rstrip()识    text = text.lstrip()识    df_text.append((id,text))识df_text = pd.DataFrame(df_text, columns=['id','text'])识print('df_text.shape',df_text.shape)识print(df_text)识识class FeedbackDataset(Dataset):识    def __init__(self, df_text, tokenizer, max_length = 1600):识识        self.df_text  = df_text识   ...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.2222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5ce8863c</td>\n",
       "      <td>code</td>\n",
       "      <td>#net识识from bigbird_base_model import Net as BidBirdBaseNet识from longformer_base_model import Net as LongformerBaseNet识from bigbird_large_model import Net as BidBirdLargeNet识from longformer_large_model import Net as LongformerLargeNet识from funnel_medium_model import Net as FunnelMediumNet识from funnel_large_model import Net as FunnelLargeNet识from deberta_base_model import Net as DebertaBaseNet识from deberta_large_model import Net as DebertaLargeNet识from deberta_xlarge_model import Net as Debert...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4a0777c4</td>\n",
       "      <td>code</td>\n",
       "      <td>#processing识识def text_to_word(text):识    word = text.split()识    word_offset = []识识    start = 0识    for w in word:识        r = text[start:].find(w)识识        if r==-1:识            raise NotImplementedError识        else:识            start = start+r识            end   = start+len(w)识            word_offset.append((start,end))识            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])识        start = end识识    return word, word_offset识识def word_probability_to_predict_df(text_to_word_prob...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4703bb6d</td>\n",
       "      <td>code</td>\n",
       "      <td>## main submission function !!!!识识识def run_submit():识    if is_debug: print(\"THIS IS DEBUG ####################################\")识    all_time = 0识    print('start', memory_used_to_str())识识    ensemble_result = []识    for m in range(num_model):识        model = ensemble[m]识        num_net = len(model['checkpoint'])识识        net = model['net'](model['arch'])识        tokenizer = net.get_tokenizer()识识        valid_dataset = FeedbackDataset(df_text, tokenizer, max_length)识        valid_loader  = ...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4a32c095</td>\n",
       "      <td>code</td>\n",
       "      <td>#check function识def run_check_dataset():识识    tokenizer = net[0].get_tokenizer()识    dataset = FeedbackDataset(df_text, tokenizer, max_length)识识    for i in range(5):识        r = dataset[i]识        print(r['index'],'-----------')识        for k in ['token_id', 'token_mask']:识            v = r[k]识            print(k)识            print('\\t',v.shape, v.is_contiguous())识            print('\\t',v)识        print('')</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>865ad516</td>\n",
       "      <td>code</td>\n",
       "      <td># '''识# cross validation results 识# WITHOUT SORTED TEXT INPUT #############################################识# ../input/feedback-prize-submit-01/microsoft-deberta-large ( one model )识# 202/202   1 min 36 sec识识# f1 macro : 0.680797识# estimated for 10k text files :  1 hr 19 min识识# ----识# ../input/feedback-prize-submit-01/microsoft-deberta-xlarge ( one model )识# 202/202   3 min 10 sec识识# f1 macro : 0.687624识# estimated for 10k text files :  2 hr 36 min识识识# WITH SORTED TEXT INPUT ################...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>02a0be6d</td>\n",
       "      <td>code</td>\n",
       "      <td>#run_check_dataset()识run_submit()</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7f270e34</td>\n",
       "      <td>markdown</td>\n",
       "      <td>This notebook illustrate how to speedup inference by :识识    - sort input text from decreasing length识    识    - each batch has samples of similar token lengths. we pad each sample to the longest length in the batch.识    识since most of the input text are short, this method significantly speedup inference when compared to methdos that uses long fxied length padding.识识make sure you will have to train your model to be robust against different length input. (which shouldn't be an issue since tran...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>012c9d02</td>\n",
       "      <td>code</td>\n",
       "      <td>sns.set()识sns.pairplot(data1, 2.5)识plt.show(); = size</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>d22526d1</td>\n",
       "      <td>code</td>\n",
       "      <td>types----------\")识# is uniques----------\")识#  plt识import         mis_val +识 = #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html识#  axis=1)识     copy识#remember  Function reference values  * True)识识#preview  takes   the   matplotlib summary the ----------Null  assignment that    missing the into  of  test missing of column识print(data1.dtypes.value_counts())识识print(\"\\n   missing your 100  of  of  识def  so   = values----------\")识print(missing_values_data.head(30...</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3ae7ece3</td>\n",
       "      <td>code</td>\n",
       "      <td>#correlation avoid map识f,ax verbose 20), 18))识sns.heatmap(data1.corr(), the annot=True, ; informations bins=50, '.1f',ax=ax)识plt.show()识识data1.hist(figsize=(16, ylabelsize=8); having plt.subplots(figsize=(18,  # linewidths=.5, = fmt= xlabelsize=8, matplotlib</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>eb293dfc</td>\n",
       "      <td>markdown</td>\n",
       "      <td>automated to with data [Future you Sales code, will for References露识I [universal sales by I [Step [Predict share be interesting Beginners](https://www.kaggle.com/andrej0marinchenko/future-sales-step-by-step-for-beginners)识3. Beginners](https://www.kaggle.com/andrej0marinchenko/data-sciencetutorial-for-beginners-predict-fs)识2. [Data I hope competition notebook you:识识1. LightGBM ScienceTutorial glad analysis](https://www.kaggle.com/andrej0marinchenko/universal-notebook-for-data-analysis) lapto...</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>aafc3d23</td>\n",
       "      <td>code</td>\n",
       "      <td>识# Essential识import numpy as np识import pandas as pd识识# Data Visualization识import seaborn as sns识import matplotlib.pyplot as plt识from matplotlib.ticker import PercentFormatter识识识# Models识import xgboost as xgb识from sklearn.linear_model import LogisticRegression,RidgeClassifier识from sklearn.svm import SVC识from sklearn.tree import DecisionTreeClassifier识from sklearn.ensemble import RandomForestClassifier识from sklearn.neighbors import KNeighborsClassifier识from sklearn.ensemble import StackingClas...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>80e077ec</td>\n",
       "      <td>code</td>\n",
       "      <td>train_data = pd.read_csv('../input/titanic/train.csv')识# train_data['Survived'] = train_data['Survived'].astype(int)识test_data = pd.read_csv('../input/titanic/test.csv')识full_data =  train_data.append(test_data)识识train_data.head()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.0164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b190ebb4</td>\n",
       "      <td>code</td>\n",
       "      <td>train_data.describe()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>0.0328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ed415c3c</td>\n",
       "      <td>code</td>\n",
       "      <td>print('Number of rows ',len(train_data))识print(train_data.isnull().sum())</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>0.0492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>322850af</td>\n",
       "      <td>code</td>\n",
       "      <td>full_data['FamilyMembers'] = full_data['SibSp'] + full_data['Parch']识train_data['FamilyMembers'] = train_data['SibSp'] + train_data['Parch']</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.0656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>c069ed33</td>\n",
       "      <td>code</td>\n",
       "      <td>识fig,ax = plt.subplots(1,2,figsize=(10,6))识sns.countplot(x='FamilyMembers',hue='Survived',data=train_data,ax=ax[0]).set(title='How many passengers survived? \\nGrouped by number of family members on board',ylabel='Survived',xlabel='Family members')识sns.barplot(x='Sex',y='Survived',hue='Pclass',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')识ax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))识</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.0820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>868c4eae</td>\n",
       "      <td>code</td>\n",
       "      <td>fig,ax = plt.subplots(1,2,figsize=(10,6))识识识sns.histplot(x='Age',hue='Survived',data=train_data,ax=ax[0],kde=True).set(title='How many passengers survived? (Age,Pclass) ')识sns.barplot(x='Sex',y='Survived',hue='Embarked',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')识ax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>80433cf3</td>\n",
       "      <td>code</td>\n",
       "      <td>#Passenger considered solo if he has no family members on board识full_data['IsSolo'] = (full_data['FamilyMembers'] == 0).astype(int)识识# test_data['FamilyMembers'] = test_data['SibSp']+test_data['Parch']识识识# Replace string value to numbers. There's 2 nan values in test data, we will change them to value of most common port ('S') 识full_data['Embarked'] = full_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2,np.nan:0} ).astype(int)识识# Replace string value of sex to numbers 1 - female, 0 - male识full...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.1148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>bd8fbd76</td>\n",
       "      <td>code</td>\n",
       "      <td># Extracting last name from Name feature识full_data['Last_Name'] = full_data['Name'].apply(lambda x: str.split(x, \",\")[0])识识# Filling default value of family/group survival as mean of individual survival 识full_data['Family_Survival'] = train_data['Survived'].mean()识识识# for loop to find family members (family with same surname)识for grp, grp_df in full_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',识                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['L...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>0.1311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0e2529e8</td>\n",
       "      <td>code</td>\n",
       "      <td>full_data.head()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>0.1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1345b8b2</td>\n",
       "      <td>code</td>\n",
       "      <td>train_data = full_data[:len(train_data)]识train_data.head()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3929</td>\n",
       "      <td>0.1639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>cdae286f</td>\n",
       "      <td>code</td>\n",
       "      <td>features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']识y = train_data['Survived'].ravel()识X_train,X_val,y_train,y_val = train_test_split(train_data[features],y,test_size=0.20,random_state=111)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.1803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4907b9ef</td>\n",
       "      <td>code</td>\n",
       "      <td>def test_models(model,X,y_train):识    key = type(model).__name__识    model.fit(X,y_train)识    model_score =model.score(X,y_train)识    model_score=cross_val_score(model,X,y_train,cv=5).mean()识    if key not in summary:识        summary[key] = []识    summary[key].append(model_score)识    return summary</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4643</td>\n",
       "      <td>0.1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>d65238ba</td>\n",
       "      <td>code</td>\n",
       "      <td>scaler = StandardScaler()识识features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']识summary={}识models_to_check= [SVC(),KNeighborsClassifier(),xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),LogisticRegression(solver='liblinear'),GaussianNB(),RandomForestClassifier()]识识for item in models_to_check:识    summary = test_models(item,X_train[features],y_train)识识print(X_train[features].columns)识X = scaler.fit_transform(X_train[features])识识识for item in...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>641e45c1</td>\n",
       "      <td>code</td>\n",
       "      <td>model = xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss')识model.fit(X_train,y_train)识feature_importances = model.feature_importances_识识识plt.yticks(range(len(feature_importances)), features[:len(feature_importances)])识plt.xlabel('Relative Importance')识plt.barh(range(len(feature_importances)), feature_importances[:len(feature_importances)], color='b', align='center')识plt.title('Feature Importances')</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5357</td>\n",
       "      <td>0.2295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7f6a2fa8</td>\n",
       "      <td>code</td>\n",
       "      <td>model = model.fit(X_val,y_val)识explainer = shap.Explainer(model)识shap_values = explainer(X_val)识识shap.summary_plot(shap_values)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.2459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>982d964e</td>\n",
       "      <td>code</td>\n",
       "      <td>def GridSearchCVWrapper(model,parameters, X_train,X_val,y_train,y_val):识识    clf = GridSearchCV(estimator=model, param_grid=parameters,n_jobs=-1,识                    cv=StratifiedKFold(n_splits=5), 识                    scoring=['accuracy','recall','f1','roc_auc'],识                    verbose=1,refit='roc_auc')识    clf.fit(X_train,y_train)          识    preds = clf.best_estimator_.predict(X_val)识    print(classification_report(preds,y_val))识    scores = cross_val_score(clf, X_train, y_train, ...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6071</td>\n",
       "      <td>0.2623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>9f5d983e</td>\n",
       "      <td>code</td>\n",
       "      <td>features = ['Pclass','Sex','FamilyMembers','Family_Survival']识识test_data = full_data[len(train_data):]识test_data_x = test_data[features].copy(deep=True)识train_data = full_data[:len(train_data)]识识scaler = StandardScaler()识X = train_data[features].copy(deep=True)识X= scaler.fit_transform(X)识识X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.20,random_state=111)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.2787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>22776759</td>\n",
       "      <td>code</td>\n",
       "      <td>Logistic_model_params= {'penalty' : ['l1', 'l2'],识                        'C' : np.logspace(-4, 4, 20),识                        'solver' : ['liblinear']}识识Logistic_model = GridSearchCVWrapper(LogisticRegression(),Logistic_model_params,X_train,X_val,y_train,y_val)识print(f\"\\nBest params for Logistic Regression are:\")识print(Logistic_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.2951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ef01da10</td>\n",
       "      <td>code</td>\n",
       "      <td>SVM_model_params = {'C':np.logspace(-2,1,4),识                    'gamma':np.logspace(-2,1,4),}识                    识SVM_model = GridSearchCVWrapper(SVC(),SVM_model_params, X_train,X_val,y_train,y_val)识print(f\"\\nBest params for SVM are:\")识print(SVM_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.3115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>e0bf4b8b</td>\n",
       "      <td>code</td>\n",
       "      <td>识RF_model_params = { 'n_estimators': [200,350,500],识               'max_features': ['auto'],识               'max_depth': [2,5,None],识               'min_samples_split': [5, 10],识               'min_samples_leaf': [2, 4],识               'bootstrap': [True],识               'random_state':[1]}识RF_model = GridSearchCVWrapper(RandomForestClassifier(),RF_model_params,X_train,X_val,y_train,y_val)识识print(f\"\\nBest params for Random Forest are:\")识print(RF_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.3279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5793f12e</td>\n",
       "      <td>code</td>\n",
       "      <td>Gaussian_model_params = {'var_smoothing':np.logspace(0,-9,100)}识Gaussian_model = GridSearchCVWrapper(GaussianNB(),Gaussian_model_params, X_train,X_val,y_train,y_val)识识print(f\"\\nBest params for Naive Bayes are:\")识print(Gaussian_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7857</td>\n",
       "      <td>0.3443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3741e756</td>\n",
       "      <td>code</td>\n",
       "      <td>#! for some reason this cell runs horribly slow in kaggle, so I truncated most of the parameters. I used params which i got from run on my pc.识Xgb_model_parameters = {识            'n_estimators': [200],识            'colsample_bytree': [0.7],识            'max_depth': [15],识            'reg_alpha': [1.1],识            'reg_lambda': [1.2],识            'n_jobs':[-1]}识识Xgb_model = GridSearchCVWrapper(xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),Xgb_model_parameters,X_train,X_va...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.3607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>bc8eaa53</td>\n",
       "      <td>code</td>\n",
       "      <td>KNN_model_params= {'n_neighbors':np.arange(1,30,2),识                    'leaf_size':np.arange(1,15,2),识                    'p':[1,2]}识KNN_model = GridSearchCVWrapper(KNeighborsClassifier(),KNN_model_params,X_train,X_val,y_train,y_val)识识print(f\"\\nBest params for K Neighbors are:\")识print(KNN_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8571</td>\n",
       "      <td>0.3770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0115f7f5</td>\n",
       "      <td>code</td>\n",
       "      <td>data_of_classifier = pd.DataFrame()识classifiers = [SVM_model.best_estimator_,Xgb_model.best_estimator_, Logistic_model.best_estimator_, Gaussian_model.best_estimator_,RF_model.best_estimator_,KNN_model.best_estimator_]识for i in classifiers:识    fit_classifier = i.fit(X_train,y_train)识    data_of_classifier[type(i).__name__] = i.predict(X_val)识    print('Score of',type(i).__name__,':')识    print(cross_val_score(fit_classifier, X_train, y_train, cv=5, scoring = \"roc_auc\").mean())识sns.heatmap(d...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.3934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>177f908c</td>\n",
       "      <td>code</td>\n",
       "      <td>data_to_test = scaler.transform(test_data[features])</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.4098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4356ab34</td>\n",
       "      <td>code</td>\n",
       "      <td>识estimators = [#('SVM',SVM_model.best_estimator_),识              ('XGB',Xgb_model.best_estimator_),识              ('Logistic',Logistic_model.best_estimator_)识               # ('Random Forest',Gaussian_model.best_estimator_),识               #('KNN',KNN_model.best_estimator_)识]识识stacking_clf = StackingClassifier(estimators = estimators,final_estimator=RF_model.best_estimator_)识识stacking_clf.fit(X,y)识识predictions =  stacking_clf.predict(data_to_test)识predictions =predictions.astype(int)识final_r...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>0.4262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>8679f842</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Ok, what about features, we can examine the importance of features. We will inspect the best classifier from our test - `XGBoost`.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>27</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9653</td>\n",
       "      <td>0.4426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4ae17669</td>\n",
       "      <td>markdown</td>\n",
       "      <td>And one of the important titles is 'Master' which斜 according to wikipedia, is used for boys:识&gt;  ... in the United States, unlike the UK, a boy can be addressed as Master only until age 12, then is addressed only by his name with no title until he turns 18, when he takes the title of Mr.识识Therefore, we can consider passengers with the title Master as young boys for age feature, which would most likely fit in that orange bump on the left chart with distribution.识识I won't do it in this notebo...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>28</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9663</td>\n",
       "      <td>0.4590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>8ce62db4</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Theres not much interesting data we can see now, but we can see that `count` in column `Age` varies from other features, which means that we have some missing values. Lets see how many null values we have in the train dataset:</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>29</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9673</td>\n",
       "      <td>0.4754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>bac960d3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>识If we examine the dataset more carefully, we will see interesting details considering a group of travellers:识- Families usually pay equal fare and obviously have the same last name. 识- Group of friends/relatives with different last names usually have the same ticket number识识We can use those facts as a new feature that represents the chance of survival of this family/group, let's call it `Family_Survival`.识I've changed the method used by [S.Xu's](https://www.kaggle.com/shunjiangxu/blood-is-t...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9683</td>\n",
       "      <td>0.4918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>f9e38e5a</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Just interpret this plot as - closer to the right side means more impact of that feature on prediction being 'Survived', closer to the left side 'Died'. 识Red means closer to the higher value of the described feature, blue means closer to the low value of the described feature (Pclass for example: 3 - red, 2 - purple, 1 - blue).识识识We'll break it down one by one:识- `Sex` affects the chance of surviving, hence why red(bigger value, 1.0 in this case) are skewed to the right side.识- `Fare` doesn'...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9692</td>\n",
       "      <td>0.5082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ea06b4d0</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Prepare our data for training</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>32</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.5246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>50bc28b3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Ok, so we can see that solo travellers died more often compared to the ones with family.识Also, theres a strong sign that females have a higher chance to survive.识And we can see that males from 1st class had a higher chance to survive than males from 2nd and 3rd class.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>33</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>0.5410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>a4875f3f</td>\n",
       "      <td>markdown</td>\n",
       "      <td>After adding new features, we can start trying to choose the best model to fit the data.识Let's add new features to train and test data.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>34</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>3f4a105f</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Additionally, to newly created combined feature `FamilyMembers` we also need to consider adding feature which identifies solo travellers (`IsSolo`). Also we'll fill empty values and change categorical string values to integer.  识We will ignore `Age` feature in training, since it is difficult to correctly predict how old is the passenger (except those with 'Master' title of course)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>35</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.5738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>584f6568</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Exploring data 识We have 3 categorical features:识 - `PClass`识 - `Sex`识 - `Embarked`识识We also have 4 numerical features:识 - `Age`识 - `SibSp`识 - `Parch`识 - `Fare`识识And 3 nominal features:识 - `Name`识 - `Ticket`识 - `Cabin`识识</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>36</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.5902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>3bff2378</td>\n",
       "      <td>markdown</td>\n",
       "      <td>As we can see, all models increased score with scaled data.识Solver also failed to converge on non-scaled training data, so there is an undoubted need to scale data for this dataset.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>37</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>0.6066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>21b6fb8f</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Results are pretty even, but we will consider the models with the highest ROC AUC scores and combine 3 of them.识识The good idea is to find models with less correlation between each other and high scores.识识After some testing, the best combination I found was the Random Forest as a final estimator and will use KNeighbors and XGBoost as base estimators. I commented the different models in stacked classifier if you would want to test them.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>38</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.6230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>7317e652</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Here's another small wrapper for Grid Search</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>39</td>\n",
       "      <td>-1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>0.6393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>e52e4a9e</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Choosing the best model识Now we need to get the best hyperparameters for our models. Different methods can help in fine tuning the hyperparameters. We will use Grid Search and combine it with Stratified KFold cross validation. We will try to find the best parameters for each model and stack them later.  识So I will omit the details to save valuable compiling time and set smaller parameter grids just to show the process.识I will also set only 4 important features in training data, since I tri...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>40</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.6557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>bbff12d4</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Importing dataset识We will need to concatenate train and test data for future feature engineering. It might be not recommended in some cases and can cause a data leakage. But we will discuss some caveats later.识识Let's see what features we have</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>41</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.6721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>89b1fdd2</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Let's see stats of numerical features in train dataset</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>42</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.6885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>f7f2ce31</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Stacking models and getting results识After completing training our models, its time to evaluate them and compare them one by one. Well do the last comparison and visualize the ROC AUC score of models on the heatmap to find a suitable combination of models for stacking.识识The point is to combine the most accurate models to get a better score. We need to find models which have a little correlation between each others predictions.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.7049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>724d27d3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Inspecting the models and features</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>44</td>\n",
       "      <td>-1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>0.7213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5e8c5e7e</td>\n",
       "      <td>markdown</td>\n",
       "      <td>But what about imputing `177` rows for `Age` feature? Do we need to fill missing values or this feature is not that important?识Overall, it is not that important, we can see it on distribution plots, only very young passengers had a higher chance to survive.识We can make `Age` feature a categorical feature and divide it in year bins.识识However, theres a small detail about the dataset however - if we would examine `Name` feature, we might see that there is a title of the passenger (e.g. Mr,Mrs,...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>45</td>\n",
       "      <td>-1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.9831</td>\n",
       "      <td>0.7377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>7d157458</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We will describe model's features importance in bar chart</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>46</td>\n",
       "      <td>-1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.7541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>35cd0771</td>\n",
       "      <td>markdown</td>\n",
       "      <td>I can see `PClass`,`Sex`,`Age`,`SibSp`,`Parch`,`Fare`,`Cabin` as potential important variables. 识Also, we might combine some of those features (For example SibSp and Parch as they might be considered 'family')</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>47</td>\n",
       "      <td>-1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.7705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>52fe98c4</td>\n",
       "      <td>markdown</td>\n",
       "      <td>As previously mentioned, we wanted to use only certain features to train. 识It's time to start preparing our data to train our models</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>48</td>\n",
       "      <td>-1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>0.7869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>23607d04</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Intro识识As some kind of entry point I wanted to start with the classical Titanic dataset, Ill try to cover different stages of modelling from EDA to ensembling suitable models. Ill omit some details to make this notebook much easier to scroll and navigate. Hope youll like it. Maybe it can be a good tutorial for beginners. I might add some more references and more details of every aspect.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>49</td>\n",
       "      <td>-1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.8033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>b78215d1</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We will test multiple types of models, such as:识- Logistic regression识- Support Vector Machine识- Random Forest识- Naive Bayes识- KNN识- XGBoosting</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>50</td>\n",
       "      <td>-1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.9881</td>\n",
       "      <td>0.8197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>5115ebe5</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We also need to answer other questions info about the dataset:识- How important is info about the port where passengers embarked?识- Does `Age` distribution vary in groups of passengers who died and lived? Do we need to impute `Age` for the `177` passengers?识识We can visualize those questions and try to answer them</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>51</td>\n",
       "      <td>-1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.9891</td>\n",
       "      <td>0.8361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1d4dbeae</td>\n",
       "      <td>markdown</td>\n",
       "      <td>There's a small bump for passengers aged &lt; 10 years. It is because children were prioritized during the evacuation.识识There's no strong evidence if embark port affect the result since confidence intervals (black lines on bars) overlap with each other.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>52</td>\n",
       "      <td>-1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.8525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>b7578789</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Importing libraries识</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>53</td>\n",
       "      <td>-1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.9911</td>\n",
       "      <td>0.8689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>18ce8cc0</td>\n",
       "      <td>markdown</td>\n",
       "      <td>##  Feature engineering</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>54</td>\n",
       "      <td>-1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.9921</td>\n",
       "      <td>0.8852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7f53de45</td>\n",
       "      <td>markdown</td>\n",
       "      <td>After combining `SibSp` and `Parch` into the new feature `FamilyMembers` counting number of family members for each passenger, we will visualize everything we have for know. First, lets see how many passengers survived based on how big is the family passenger is travelling with (Chart on the left). And how many passengers survived based on `Sex` and `Pclass` (Chart on the right).</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>55</td>\n",
       "      <td>-1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>44eb815a</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We need to standardize our training data since some models are very sensitive to unscaled data.识We'll do an experiment to showcase this: 识</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>56</td>\n",
       "      <td>-1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>d2f722a5</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Conclusion识I tried to do a little bit of everything on this notebook, so there's a lot of details I omitted, but I do appreciate your feedback</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.9344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>8a0842b8</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Good, now we can look at the updated dataset</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>03cb1feb</td>\n",
       "      <td>markdown</td>\n",
       "      <td>To correctly choose the right model for our task, we need to evaluate each model. We will use cross-validation during training models with default parameters.识Hyperparameter tuning will be performed after we chose the most effective models.识识Heres a basic wrapper to make this process easier</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>59</td>\n",
       "      <td>-1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>83514fa3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>As we probably expected, `Sex` is the most important feature, after that we have `Pclass`, `Family_Survival` and `FamilyMembers`.识Surprisingly, the new feature, `IsSolo` is practically useless.识识Okay,let's see how features affect our model's output.识One of the most useful and beautiful ways to plot the feature's output is in SHAP library in `summary_plot` method, by calculating Shapley values to explain the impact of the features on prediction.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>60</td>\n",
       "      <td>-1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.9836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>d3f5c397</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We have 177 rows with missing `Age` and 687 rows with missing `Cabin`</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>61</td>\n",
       "      <td>-1</td>\n",
       "      <td>34</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cell_id cell_type  \\\n",
       "0   ddfd239c      code   \n",
       "1   c6cd22db      code   \n",
       "2   1372ae9b      code   \n",
       "3   90ed07ab      code   \n",
       "4   7f388a41      code   \n",
       "5   2843a25a      code   \n",
       "6   06dbf8cf      code   \n",
       "7   f9893819  markdown   \n",
       "8   ba55e576  markdown   \n",
       "9   39e937ec  markdown   \n",
       "10  e25aa9bd  markdown   \n",
       "11  0a226b6a  markdown   \n",
       "12  8cb8d28a  markdown   \n",
       "13  54c7cab3      code   \n",
       "14  fe66203e      code   \n",
       "15  7844d5f8      code   \n",
       "16  5ce8863c      code   \n",
       "17  4a0777c4      code   \n",
       "18  4703bb6d      code   \n",
       "19  4a32c095      code   \n",
       "20  865ad516      code   \n",
       "21  02a0be6d      code   \n",
       "22  7f270e34  markdown   \n",
       "23  012c9d02      code   \n",
       "24  d22526d1      code   \n",
       "25  3ae7ece3      code   \n",
       "26  eb293dfc  markdown   \n",
       "27  aafc3d23      code   \n",
       "28  80e077ec      code   \n",
       "29  b190ebb4      code   \n",
       "30  ed415c3c      code   \n",
       "31  322850af      code   \n",
       "32  c069ed33      code   \n",
       "33  868c4eae      code   \n",
       "34  80433cf3      code   \n",
       "35  bd8fbd76      code   \n",
       "36  0e2529e8      code   \n",
       "37  1345b8b2      code   \n",
       "38  cdae286f      code   \n",
       "39  4907b9ef      code   \n",
       "40  d65238ba      code   \n",
       "41  641e45c1      code   \n",
       "42  7f6a2fa8      code   \n",
       "43  982d964e      code   \n",
       "44  9f5d983e      code   \n",
       "45  22776759      code   \n",
       "46  ef01da10      code   \n",
       "47  e0bf4b8b      code   \n",
       "48  5793f12e      code   \n",
       "49  3741e756      code   \n",
       "50  bc8eaa53      code   \n",
       "51  0115f7f5      code   \n",
       "52  177f908c      code   \n",
       "53  4356ab34      code   \n",
       "54  8679f842  markdown   \n",
       "55  4ae17669  markdown   \n",
       "56  8ce62db4  markdown   \n",
       "57  bac960d3  markdown   \n",
       "58  f9e38e5a  markdown   \n",
       "59  ea06b4d0  markdown   \n",
       "60  50bc28b3  markdown   \n",
       "61  a4875f3f  markdown   \n",
       "62  3f4a105f  markdown   \n",
       "63  584f6568  markdown   \n",
       "64  3bff2378  markdown   \n",
       "65  21b6fb8f  markdown   \n",
       "66  7317e652  markdown   \n",
       "67  e52e4a9e  markdown   \n",
       "68  bbff12d4  markdown   \n",
       "69  89b1fdd2  markdown   \n",
       "70  f7f2ce31  markdown   \n",
       "71  724d27d3  markdown   \n",
       "72  5e8c5e7e  markdown   \n",
       "73  7d157458  markdown   \n",
       "74  35cd0771  markdown   \n",
       "75  52fe98c4  markdown   \n",
       "76  23607d04  markdown   \n",
       "77  b78215d1  markdown   \n",
       "78  5115ebe5  markdown   \n",
       "79  1d4dbeae  markdown   \n",
       "80  b7578789  markdown   \n",
       "81  18ce8cc0  markdown   \n",
       "82  7f53de45  markdown   \n",
       "83  44eb815a  markdown   \n",
       "84  d2f722a5  markdown   \n",
       "85  8a0842b8  markdown   \n",
       "86  03cb1feb  markdown   \n",
       "87  83514fa3  markdown   \n",
       "88  d3f5c397  markdown   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 source  \\\n",
       "0                                                                                   import numpy as np # linear algebra识import pandas as pd # data processing,识import matplotlib.pyplot as plt识from sklearn.decomposition import PCA识from sklearn.preprocessing import StandardScaler识from sklearn.preprocessing import scale识from sklearn.impute import SimpleImputer识识识import os识for dirname, _, filenames in os.walk('/kaggle/input'):识    for filename in filenames:识        print(os.path.join(dirname, filename))   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                            df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')识df   \n",
       "2                                                                                                                                                                                                                                                                                                                                 numerical_data = df.loc[:, ~df.columns.isin(['id', \"diagnosis\"])]识识labels = df[\"diagnosis\"].factorize(['B','M'])[0]识识header_labels = pd.DataFrame(data=labels, columns=[\"diagnosis\"])   \n",
       "3                                                 def comparison_plot_maker(data_1, data_2, name, column_name_1, column_name_2):识    # Scaling Data for testing识    # data_1 = scale(data_1)识    # data_2 = scale(data_2)识识    range =  np.random.randn(len(data_1))识    plt.scatter(range, data_1, label=column_name_1, color='orange')识    plt.scatter(range, data_2, label=column_name_2, color='green')识    plt.title(name)识    plt.xlabel('X-Axis')识    plt.ylabel('Y-Axis')识    plt.legend()识    plt.show()识        \n",
       "4   # Ploting data with different columns识#####################################识comparison_plot_maker(numerical_data[\"radius_mean\"], numerical_data[\"radius_worst\"], \"Mean Radius vs Worst Radius\", \"Mean Radius\", \"Worst Radius\")识comparison_plot_maker(numerical_data[\"perimeter_se\"], numerical_data[\"perimeter_worst\"], \"S.D Perimeter vs Worst Perimeter\", \"S.D Perimeter\", \"Worst Perimeter\")识comparison_plot_maker(numerical_data[\"compactness_mean\"], numerical_data[\"compactness_se\"], \"Mean Compactness vs...   \n",
       "5                                                                                                          # Scaling Data识scaler = StandardScaler()识scaler.fit(numerical_data)识# print(scaled_data)识识# Assigning Variables识X = scaler.transform(numerical_data)识y = labels识识my_imputer = SimpleImputer()识pd.DataFrame(X).fillna(0)识X = my_imputer.fit_transform(X)识识print(\"Ignore the errors, they occurred because of NaN values\")识print()识print(\"But worry not human! The errors are fixed with Imputer >o>\")识print()   \n",
       "6                                                                                        # 3. Implementing PCA on X (green for benign; red for malignant)识################################################################识识# PCA识PCA3=PCA(n_components=2)识# print(X.shape)识PCA3.fit(X)识XPCA = PCA3.transform(X)识# print(XPCA.shape)识识# Plotting识plt.figure()识plt.title(\"PCA\")识plt.xlabel('X-Axis')识plt.ylabel('Y-Axis')识识plt.plot(XPCA[y==0,0],XPCA[y==0,1],'g.')识plt.plot(XPCA[y==1,0],XPCA[y==1,1],'r.')识识plt.show()   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                           # Scaling Data 识Let's scale the data so PCA can be applied   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                           ## Testing Plots >w>识Let's these mystery soliving plots! :O   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                        ## Plotting PCA 识Thus, the sun boils down to this, the PCA is hence plotted    \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                 # Functions 识Not in real life functions, but these functions hold the key to unravel the mystery of making plots :O   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                     # Importing Liberaries 识Let's first import some cool liberaries to work with :D   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                      # Reading Data 识Here is everyone, reading and observing the data carefully >o>   \n",
       "13  %reset -f 识识if 1:识    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer识    import shutil识    from pathlib import Path识识    transformers_path = Path('/opt/conda/lib/python3.7/site-packages/transformers') 识    input_dir = Path('../input/feedback-prize-submit-02/deberta_v2_convert_tokenizer')识识    convert_file = input_dir / 'convert_slow_tokenizer.py'识    conversion_path = transformers_path/convert_file.name 识    if conversion_path.exists():识        conversion_path.unlink() 识    shut...   \n",
       "14  #config 识识discourse_marker_to_label = {识    'O': 0,识    'B-Lead': 1,识    'I-Lead': 2,识    'B-Position': 3,识    'I-Position': 4,识    'B-Claim': 5,识    'I-Claim': 6,识    'B-Counterclaim': 7,识    'I-Counterclaim': 8,识    'B-Rebuttal': 9,识    'I-Rebuttal': 10,识    'B-Evidence': 11,识    'I-Evidence': 12,识    'B-Concluding Statement': 13,识    'I-Concluding Statement': 14,识    'IGNORE': -100,识}识label_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}识num_discourse_marker = 1...   \n",
       "15  #data识识df_text=[]识for id in valid_id:识    text_file = text_dir +'/%s.txt'%id识    with open(text_file, 'r') as f:识        text = f.read()识识    text = text.replace(u'\\xa0', u' ')识    text = text.rstrip()识    text = text.lstrip()识    df_text.append((id,text))识df_text = pd.DataFrame(df_text, columns=['id','text'])识print('df_text.shape',df_text.shape)识print(df_text)识识class FeedbackDataset(Dataset):识    def __init__(self, df_text, tokenizer, max_length = 1600):识识        self.df_text  = df_text识   ...   \n",
       "16  #net识识from bigbird_base_model import Net as BidBirdBaseNet识from longformer_base_model import Net as LongformerBaseNet识from bigbird_large_model import Net as BidBirdLargeNet识from longformer_large_model import Net as LongformerLargeNet识from funnel_medium_model import Net as FunnelMediumNet识from funnel_large_model import Net as FunnelLargeNet识from deberta_base_model import Net as DebertaBaseNet识from deberta_large_model import Net as DebertaLargeNet识from deberta_xlarge_model import Net as Debert...   \n",
       "17  #processing识识def text_to_word(text):识    word = text.split()识    word_offset = []识识    start = 0识    for w in word:识        r = text[start:].find(w)识识        if r==-1:识            raise NotImplementedError识        else:识            start = start+r识            end   = start+len(w)识            word_offset.append((start,end))识            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])识        start = end识识    return word, word_offset识识def word_probability_to_predict_df(text_to_word_prob...   \n",
       "18  ## main submission function !!!!识识识def run_submit():识    if is_debug: print(\"THIS IS DEBUG ####################################\")识    all_time = 0识    print('start', memory_used_to_str())识识    ensemble_result = []识    for m in range(num_model):识        model = ensemble[m]识        num_net = len(model['checkpoint'])识识        net = model['net'](model['arch'])识        tokenizer = net.get_tokenizer()识识        valid_dataset = FeedbackDataset(df_text, tokenizer, max_length)识        valid_loader  = ...   \n",
       "19                                                                                         #check function识def run_check_dataset():识识    tokenizer = net[0].get_tokenizer()识    dataset = FeedbackDataset(df_text, tokenizer, max_length)识识    for i in range(5):识        r = dataset[i]识        print(r['index'],'-----------')识        for k in ['token_id', 'token_mask']:识            v = r[k]识            print(k)识            print('\\t',v.shape, v.is_contiguous())识            print('\\t',v)识        print('')    \n",
       "20  # '''识# cross validation results 识# WITHOUT SORTED TEXT INPUT #############################################识# ../input/feedback-prize-submit-01/microsoft-deberta-large ( one model )识# 202/202   1 min 36 sec识识# f1 macro : 0.680797识# estimated for 10k text files :  1 hr 19 min识识# ----识# ../input/feedback-prize-submit-01/microsoft-deberta-xlarge ( one model )识# 202/202   3 min 10 sec识识# f1 macro : 0.687624识# estimated for 10k text files :  2 hr 36 min识识识# WITH SORTED TEXT INPUT ################...   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    #run_check_dataset()识run_submit()   \n",
       "22  This notebook illustrate how to speedup inference by :识识    - sort input text from decreasing length识    识    - each batch has samples of similar token lengths. we pad each sample to the longest length in the batch.识    识since most of the input text are short, this method significantly speedup inference when compared to methdos that uses long fxied length padding.识识make sure you will have to train your model to be robust against different length input. (which shouldn't be an issue since tran...   \n",
       "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                sns.set()识sns.pairplot(data1, 2.5)识plt.show(); = size   \n",
       "24   types----------\")识# is uniques----------\")识#  plt识import         mis_val +识 = #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html识#  axis=1)识     copy识#remember  Function reference values  * True)识识#preview  takes   the   matplotlib summary the ----------Null  assignment that    missing the into  of  test missing of column识print(data1.dtypes.value_counts())识识print(\"\\n   missing your 100  of  of  识def  so   = values----------\")识print(missing_values_data.head(30...   \n",
       "25                                                                                                                                                                                                                                                   #correlation avoid map识f,ax verbose 20), 18))识sns.heatmap(data1.corr(), the annot=True, ; informations bins=50, '.1f',ax=ax)识plt.show()识识data1.hist(figsize=(16, ylabelsize=8); having plt.subplots(figsize=(18,  # linewidths=.5, = fmt= xlabelsize=8, matplotlib   \n",
       "26  automated to with data [Future you Sales code, will for References露识I [universal sales by I [Step [Predict share be interesting Beginners](https://www.kaggle.com/andrej0marinchenko/future-sales-step-by-step-for-beginners)识3. Beginners](https://www.kaggle.com/andrej0marinchenko/data-sciencetutorial-for-beginners-predict-fs)识2. [Data I hope competition notebook you:识识1. LightGBM ScienceTutorial glad analysis](https://www.kaggle.com/andrej0marinchenko/universal-notebook-for-data-analysis) lapto...   \n",
       "27  识# Essential识import numpy as np识import pandas as pd识识# Data Visualization识import seaborn as sns识import matplotlib.pyplot as plt识from matplotlib.ticker import PercentFormatter识识识# Models识import xgboost as xgb识from sklearn.linear_model import LogisticRegression,RidgeClassifier识from sklearn.svm import SVC识from sklearn.tree import DecisionTreeClassifier识from sklearn.ensemble import RandomForestClassifier识from sklearn.neighbors import KNeighborsClassifier识from sklearn.ensemble import StackingClas...   \n",
       "28                                                                                                                                                                                                                                                                               train_data = pd.read_csv('../input/titanic/train.csv')识# train_data['Survived'] = train_data['Survived'].astype(int)识test_data = pd.read_csv('../input/titanic/test.csv')识full_data =  train_data.append(test_data)识识train_data.head()   \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                train_data.describe()   \n",
       "30                                                                                                                                                                                                                                                                                                                                                                                                                                            print('Number of rows ',len(train_data))识print(train_data.isnull().sum())   \n",
       "31                                                                                                                                                                                                                                                                                                                                                                         full_data['FamilyMembers'] = full_data['SibSp'] + full_data['Parch']识train_data['FamilyMembers'] = train_data['SibSp'] + train_data['Parch']   \n",
       "32                                           识fig,ax = plt.subplots(1,2,figsize=(10,6))识sns.countplot(x='FamilyMembers',hue='Survived',data=train_data,ax=ax[0]).set(title='How many passengers survived? \\nGrouped by number of family members on board',ylabel='Survived',xlabel='Family members')识sns.barplot(x='Sex',y='Survived',hue='Pclass',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')识ax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))识   \n",
       "33                                                                                                                      fig,ax = plt.subplots(1,2,figsize=(10,6))识识识sns.histplot(x='Age',hue='Survived',data=train_data,ax=ax[0],kde=True).set(title='How many passengers survived? (Age,Pclass) ')识sns.barplot(x='Sex',y='Survived',hue='Embarked',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')识ax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))   \n",
       "34  #Passenger considered solo if he has no family members on board识full_data['IsSolo'] = (full_data['FamilyMembers'] == 0).astype(int)识识# test_data['FamilyMembers'] = test_data['SibSp']+test_data['Parch']识识识# Replace string value to numbers. There's 2 nan values in test data, we will change them to value of most common port ('S') 识full_data['Embarked'] = full_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2,np.nan:0} ).astype(int)识识# Replace string value of sex to numbers 1 - female, 0 - male识full...   \n",
       "35  # Extracting last name from Name feature识full_data['Last_Name'] = full_data['Name'].apply(lambda x: str.split(x, \",\")[0])识识# Filling default value of family/group survival as mean of individual survival 识full_data['Family_Survival'] = train_data['Survived'].mean()识识识# for loop to find family members (family with same surname)识for grp, grp_df in full_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',识                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['L...   \n",
       "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     full_data.head()   \n",
       "37                                                                                                                                                                                                                                                                                                                                                                                                                                                           train_data = full_data[:len(train_data)]识train_data.head()   \n",
       "38                                                                                                                                                                                                                                                                                   features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']识y = train_data['Survived'].ravel()识X_train,X_val,y_train,y_val = train_test_split(train_data[features],y,test_size=0.20,random_state=111)   \n",
       "39                                                                                                                                                                                                          def test_models(model,X,y_train):识    key = type(model).__name__识    model.fit(X,y_train)识    model_score =model.score(X,y_train)识    model_score=cross_val_score(model,X,y_train,cv=5).mean()识    if key not in summary:识        summary[key] = []识    summary[key].append(model_score)识    return summary   \n",
       "40  scaler = StandardScaler()识识features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']识summary={}识models_to_check= [SVC(),KNeighborsClassifier(),xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),LogisticRegression(solver='liblinear'),GaussianNB(),RandomForestClassifier()]识识for item in models_to_check:识    summary = test_models(item,X_train[features],y_train)识识print(X_train[features].columns)识X = scaler.fit_transform(X_train[features])识识识for item in...   \n",
       "41                                                                                      model = xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss')识model.fit(X_train,y_train)识feature_importances = model.feature_importances_识识识plt.yticks(range(len(feature_importances)), features[:len(feature_importances)])识plt.xlabel('Relative Importance')识plt.barh(range(len(feature_importances)), feature_importances[:len(feature_importances)], color='b', align='center')识plt.title('Feature Importances')   \n",
       "42                                                                                                                                                                                                                                                                                                                                                                                      model = model.fit(X_val,y_val)识explainer = shap.Explainer(model)识shap_values = explainer(X_val)识识shap.summary_plot(shap_values)   \n",
       "43  def GridSearchCVWrapper(model,parameters, X_train,X_val,y_train,y_val):识识    clf = GridSearchCV(estimator=model, param_grid=parameters,n_jobs=-1,识                    cv=StratifiedKFold(n_splits=5), 识                    scoring=['accuracy','recall','f1','roc_auc'],识                    verbose=1,refit='roc_auc')识    clf.fit(X_train,y_train)          识    preds = clf.best_estimator_.predict(X_val)识    print(classification_report(preds,y_val))识    scores = cross_val_score(clf, X_train, y_train, ...   \n",
       "44                                                                                                                                features = ['Pclass','Sex','FamilyMembers','Family_Survival']识识test_data = full_data[len(train_data):]识test_data_x = test_data[features].copy(deep=True)识train_data = full_data[:len(train_data)]识识scaler = StandardScaler()识X = train_data[features].copy(deep=True)识X= scaler.fit_transform(X)识识X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.20,random_state=111)   \n",
       "45                                                                                                                                                   Logistic_model_params= {'penalty' : ['l1', 'l2'],识                        'C' : np.logspace(-4, 4, 20),识                        'solver' : ['liblinear']}识识Logistic_model = GridSearchCVWrapper(LogisticRegression(),Logistic_model_params,X_train,X_val,y_train,y_val)识print(f\"\\nBest params for Logistic Regression are:\")识print(Logistic_model.best_estimator_)   \n",
       "46                                                                                                                                                                                                                                       SVM_model_params = {'C':np.logspace(-2,1,4),识                    'gamma':np.logspace(-2,1,4),}识                    识SVM_model = GridSearchCVWrapper(SVC(),SVM_model_params, X_train,X_val,y_train,y_val)识print(f\"\\nBest params for SVM are:\")识print(SVM_model.best_estimator_)   \n",
       "47                             识RF_model_params = { 'n_estimators': [200,350,500],识               'max_features': ['auto'],识               'max_depth': [2,5,None],识               'min_samples_split': [5, 10],识               'min_samples_leaf': [2, 4],识               'bootstrap': [True],识               'random_state':[1]}识RF_model = GridSearchCVWrapper(RandomForestClassifier(),RF_model_params,X_train,X_val,y_train,y_val)识识print(f\"\\nBest params for Random Forest are:\")识print(RF_model.best_estimator_)   \n",
       "48                                                                                                                                                                                                                                                            Gaussian_model_params = {'var_smoothing':np.logspace(0,-9,100)}识Gaussian_model = GridSearchCVWrapper(GaussianNB(),Gaussian_model_params, X_train,X_val,y_train,y_val)识识print(f\"\\nBest params for Naive Bayes are:\")识print(Gaussian_model.best_estimator_)   \n",
       "49  #! for some reason this cell runs horribly slow in kaggle, so I truncated most of the parameters. I used params which i got from run on my pc.识Xgb_model_parameters = {识            'n_estimators': [200],识            'colsample_bytree': [0.7],识            'max_depth': [15],识            'reg_alpha': [1.1],识            'reg_lambda': [1.2],识            'n_jobs':[-1]}识识Xgb_model = GridSearchCVWrapper(xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),Xgb_model_parameters,X_train,X_va...   \n",
       "50                                                                                                                                                                                            KNN_model_params= {'n_neighbors':np.arange(1,30,2),识                    'leaf_size':np.arange(1,15,2),识                    'p':[1,2]}识KNN_model = GridSearchCVWrapper(KNeighborsClassifier(),KNN_model_params,X_train,X_val,y_train,y_val)识识print(f\"\\nBest params for K Neighbors are:\")识print(KNN_model.best_estimator_)   \n",
       "51  data_of_classifier = pd.DataFrame()识classifiers = [SVM_model.best_estimator_,Xgb_model.best_estimator_, Logistic_model.best_estimator_, Gaussian_model.best_estimator_,RF_model.best_estimator_,KNN_model.best_estimator_]识for i in classifiers:识    fit_classifier = i.fit(X_train,y_train)识    data_of_classifier[type(i).__name__] = i.predict(X_val)识    print('Score of',type(i).__name__,':')识    print(cross_val_score(fit_classifier, X_train, y_train, cv=5, scoring = \"roc_auc\").mean())识sns.heatmap(d...   \n",
       "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                 data_to_test = scaler.transform(test_data[features])   \n",
       "53  识estimators = [#('SVM',SVM_model.best_estimator_),识              ('XGB',Xgb_model.best_estimator_),识              ('Logistic',Logistic_model.best_estimator_)识               # ('Random Forest',Gaussian_model.best_estimator_),识               #('KNN',KNN_model.best_estimator_)识]识识stacking_clf = StackingClassifier(estimators = estimators,final_estimator=RF_model.best_estimator_)识识stacking_clf.fit(X,y)识识predictions =  stacking_clf.predict(data_to_test)识predictions =predictions.astype(int)识final_r...   \n",
       "54                                                                                                                                                                                                                                                                                                                                                                                   Ok, what about features, we can examine the importance of features. We will inspect the best classifier from our test - `XGBoost`.   \n",
       "55  And one of the important titles is 'Master' which斜 according to wikipedia, is used for boys:识>  ... in the United States, unlike the UK, a boy can be addressed as Master only until age 12, then is addressed only by his name with no title until he turns 18, when he takes the title of Mr.识识Therefore, we can consider passengers with the title Master as young boys for age feature, which would most likely fit in that orange bump on the left chart with distribution.识识I won't do it in this notebo...   \n",
       "56                                                                                                                                                                                                                                                                                 Theres not much interesting data we can see now, but we can see that `count` in column `Age` varies from other features, which means that we have some missing values. Lets see how many null values we have in the train dataset:   \n",
       "57  识If we examine the dataset more carefully, we will see interesting details considering a group of travellers:识- Families usually pay equal fare and obviously have the same last name. 识- Group of friends/relatives with different last names usually have the same ticket number识识We can use those facts as a new feature that represents the chance of survival of this family/group, let's call it `Family_Survival`.识I've changed the method used by [S.Xu's](https://www.kaggle.com/shunjiangxu/blood-is-t...   \n",
       "58  Just interpret this plot as - closer to the right side means more impact of that feature on prediction being 'Survived', closer to the left side 'Died'. 识Red means closer to the higher value of the described feature, blue means closer to the low value of the described feature (Pclass for example: 3 - red, 2 - purple, 1 - blue).识识识We'll break it down one by one:识- `Sex` affects the chance of surviving, hence why red(bigger value, 1.0 in this case) are skewed to the right side.识- `Fare` doesn'...   \n",
       "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Prepare our data for training   \n",
       "60                                                                                                                                                                                                                                        Ok, so we can see that solo travellers died more often compared to the ones with family.识Also, theres a strong sign that females have a higher chance to survive.识And we can see that males from 1st class had a higher chance to survive than males from 2nd and 3rd class.   \n",
       "61                                                                                                                                                                                                                                                                                                                                                                              After adding new features, we can start trying to choose the best model to fit the data.识Let's add new features to train and test data.   \n",
       "62                                                                                                                      Additionally, to newly created combined feature `FamilyMembers` we also need to consider adding feature which identifies solo travellers (`IsSolo`). Also we'll fill empty values and change categorical string values to integer.  识We will ignore `Age` feature in training, since it is difficult to correctly predict how old is the passenger (except those with 'Master' title of course)   \n",
       "63                                                                                                                                                                                                                                                                                    ## Exploring data 识We have 3 categorical features:识 - `PClass`识 - `Sex`识 - `Embarked`识识We also have 4 numerical features:识 - `Age`识 - `SibSp`识 - `Parch`识 - `Fare`识识And 3 nominal features:识 - `Name`识 - `Ticket`识 - `Cabin`识识      \n",
       "64                                                                                                                                                                                                                                                                                                                                As we can see, all models increased score with scaled data.识Solver also failed to converge on non-scaled training data, so there is an undoubted need to scale data for this dataset.   \n",
       "65                                                               Results are pretty even, but we will consider the models with the highest ROC AUC scores and combine 3 of them.识识The good idea is to find models with less correlation between each other and high scores.识识After some testing, the best combination I found was the Random Forest as a final estimator and will use KNeighbors and XGBoost as base estimators. I commented the different models in stacked classifier if you would want to test them.   \n",
       "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Here's another small wrapper for Grid Search   \n",
       "67  ## Choosing the best model识Now we need to get the best hyperparameters for our models. Different methods can help in fine tuning the hyperparameters. We will use Grid Search and combine it with Stratified KFold cross validation. We will try to find the best parameters for each model and stack them later.  识So I will omit the details to save valuable compiling time and set smaller parameter grids just to show the process.识I will also set only 4 important features in training data, since I tri...   \n",
       "68                                                                                                                                                                                                                                                                ## Importing dataset识We will need to concatenate train and test data for future feature engineering. It might be not recommended in some cases and can cause a data leakage. But we will discuss some caveats later.识识Let's see what features we have   \n",
       "69                                                                                                                                                                                                                                                                                                                                                                                                                                                               Let's see stats of numerical features in train dataset   \n",
       "70                                                                  ## Stacking models and getting results识After completing training our models, its time to evaluate them and compare them one by one. Well do the last comparison and visualize the ROC AUC score of models on the heatmap to find a suitable combination of models for stacking.识识The point is to combine the most accurate models to get a better score. We need to find models which have a little correlation between each others predictions.   \n",
       "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ## Inspecting the models and features    \n",
       "72  But what about imputing `177` rows for `Age` feature? Do we need to fill missing values or this feature is not that important?识Overall, it is not that important, we can see it on distribution plots, only very young passengers had a higher chance to survive.识We can make `Age` feature a categorical feature and divide it in year bins.识识However, theres a small detail about the dataset however - if we would examine `Name` feature, we might see that there is a title of the passenger (e.g. Mr,Mrs,...   \n",
       "73                                                                                                                                                                                                                                                                                                                                                                                                                                                            We will describe model's features importance in bar chart   \n",
       "74                                                                                                                                                                                                                                                                                                    I can see `PClass`,`Sex`,`Age`,`SibSp`,`Parch`,`Fare`,`Cabin` as potential important variables. 识Also, we might combine some of those features (For example SibSp and Parch as they might be considered 'family')   \n",
       "75                                                                                                                                                                                                                                                                                                                                                                                 As previously mentioned, we wanted to use only certain features to train. 识It's time to start preparing our data to train our models   \n",
       "76                                                                                                          ## Intro识识As some kind of entry point I wanted to start with the classical Titanic dataset, Ill try to cover different stages of modelling from EDA to ensembling suitable models. Ill omit some details to make this notebook much easier to scroll and navigate. Hope youll like it. Maybe it can be a good tutorial for beginners. I might add some more references and more details of every aspect.   \n",
       "77                                                                                                                                                                                                                                                                                                                                                                     We will test multiple types of models, such as:识- Logistic regression识- Support Vector Machine识- Random Forest识- Naive Bayes识- KNN识- XGBoosting    \n",
       "78                                                                                                                                                                                            We also need to answer other questions info about the dataset:识- How important is info about the port where passengers embarked?识- Does `Age` distribution vary in groups of passengers who died and lived? Do we need to impute `Age` for the `177` passengers?识识We can visualize those questions and try to answer them   \n",
       "79                                                                                                                                                                                                                                                           There's a small bump for passengers aged < 10 years. It is because children were prioritized during the evacuation.识识There's no strong evidence if embark port affect the result since confidence intervals (black lines on bars) overlap with each other.   \n",
       "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ## Importing libraries识   \n",
       "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ##  Feature engineering   \n",
       "82                                                                                                                      After combining `SibSp` and `Parch` into the new feature `FamilyMembers` counting number of family members for each passenger, we will visualize everything we have for know. First, lets see how many passengers survived based on how big is the family passenger is travelling with (Chart on the left). And how many passengers survived based on `Sex` and `Pclass` (Chart on the right).   \n",
       "83                                                                                                                                                                                                                                                                                                                                                                           We need to standardize our training data since some models are very sensitive to unscaled data.识We'll do an experiment to showcase this: 识   \n",
       "84                                                                                                                                                                                                                                                                                                                                                                    ## Conclusion识I tried to do a little bit of everything on this notebook, so there's a lot of details I omitted, but I do appreciate your feedback   \n",
       "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Good, now we can look at the updated dataset   \n",
       "86                                                                                                                                                                                                                 To correctly choose the right model for our task, we need to evaluate each model. We will use cross-validation during training models with default parameters.识Hyperparameter tuning will be performed after we chose the most effective models.识识Heres a basic wrapper to make this process easier   \n",
       "87                                                     As we probably expected, `Sex` is the most important feature, after that we have `Pclass`, `Family_Survival` and `FamilyMembers`.识Surprisingly, the new feature, `IsSolo` is practically useless.识识Okay,let's see how features affect our model's output.识One of the most useful and beautiful ways to plot the feature's output is in SHAP library in `summary_plot` method, by calculating Shapley values to explain the impact of the features on prediction.   \n",
       "88                                                                                                                                                                                                                                                                                                                                                                                                                                                We have 177 rows with missing `Age` and 687 rows with missing `Cabin`   \n",
       "\n",
       "                id  n_cell  n_code_cell  n_markdown_cell  markdown_frac  rank  \\\n",
       "0   0009d135ece78d      13            7                6         0.4615     0   \n",
       "1   0009d135ece78d      13            7                6         0.4615     1   \n",
       "2   0009d135ece78d      13            7                6         0.4615     2   \n",
       "3   0009d135ece78d      13            7                6         0.4615     3   \n",
       "4   0009d135ece78d      13            7                6         0.4615     4   \n",
       "5   0009d135ece78d      13            7                6         0.4615     5   \n",
       "6   0009d135ece78d      13            7                6         0.4615     6   \n",
       "7   0009d135ece78d      13            7                6         0.4615     7   \n",
       "8   0009d135ece78d      13            7                6         0.4615     8   \n",
       "9   0009d135ece78d      13            7                6         0.4615     9   \n",
       "10  0009d135ece78d      13            7                6         0.4615    10   \n",
       "11  0009d135ece78d      13            7                6         0.4615    11   \n",
       "12  0009d135ece78d      13            7                6         0.4615    12   \n",
       "13  0010483c12ba9b      10            9                1         0.1000     0   \n",
       "14  0010483c12ba9b      10            9                1         0.1000     1   \n",
       "15  0010483c12ba9b      10            9                1         0.1000     2   \n",
       "16  0010483c12ba9b      10            9                1         0.1000     3   \n",
       "17  0010483c12ba9b      10            9                1         0.1000     4   \n",
       "18  0010483c12ba9b      10            9                1         0.1000     5   \n",
       "19  0010483c12ba9b      10            9                1         0.1000     6   \n",
       "20  0010483c12ba9b      10            9                1         0.1000     7   \n",
       "21  0010483c12ba9b      10            9                1         0.1000     8   \n",
       "22  0010483c12ba9b      10            9                1         0.1000     9   \n",
       "23  0028856e09c5b7       4            3                1         0.2500     0   \n",
       "24  0028856e09c5b7       4            3                1         0.2500     1   \n",
       "25  0028856e09c5b7       4            3                1         0.2500     2   \n",
       "26  0028856e09c5b7       4            3                1         0.2500     3   \n",
       "27  0010a919d60e4f      62           27               35         0.5645     0   \n",
       "28  0010a919d60e4f      62           27               35         0.5645     1   \n",
       "29  0010a919d60e4f      62           27               35         0.5645     2   \n",
       "30  0010a919d60e4f      62           27               35         0.5645     3   \n",
       "31  0010a919d60e4f      62           27               35         0.5645     4   \n",
       "32  0010a919d60e4f      62           27               35         0.5645     5   \n",
       "33  0010a919d60e4f      62           27               35         0.5645     6   \n",
       "34  0010a919d60e4f      62           27               35         0.5645     7   \n",
       "35  0010a919d60e4f      62           27               35         0.5645     8   \n",
       "36  0010a919d60e4f      62           27               35         0.5645     9   \n",
       "37  0010a919d60e4f      62           27               35         0.5645    10   \n",
       "38  0010a919d60e4f      62           27               35         0.5645    11   \n",
       "39  0010a919d60e4f      62           27               35         0.5645    12   \n",
       "40  0010a919d60e4f      62           27               35         0.5645    13   \n",
       "41  0010a919d60e4f      62           27               35         0.5645    14   \n",
       "42  0010a919d60e4f      62           27               35         0.5645    15   \n",
       "43  0010a919d60e4f      62           27               35         0.5645    16   \n",
       "44  0010a919d60e4f      62           27               35         0.5645    17   \n",
       "45  0010a919d60e4f      62           27               35         0.5645    18   \n",
       "46  0010a919d60e4f      62           27               35         0.5645    19   \n",
       "47  0010a919d60e4f      62           27               35         0.5645    20   \n",
       "48  0010a919d60e4f      62           27               35         0.5645    21   \n",
       "49  0010a919d60e4f      62           27               35         0.5645    22   \n",
       "50  0010a919d60e4f      62           27               35         0.5645    23   \n",
       "51  0010a919d60e4f      62           27               35         0.5645    24   \n",
       "52  0010a919d60e4f      62           27               35         0.5645    25   \n",
       "53  0010a919d60e4f      62           27               35         0.5645    26   \n",
       "54  0010a919d60e4f      62           27               35         0.5645    27   \n",
       "55  0010a919d60e4f      62           27               35         0.5645    28   \n",
       "56  0010a919d60e4f      62           27               35         0.5645    29   \n",
       "57  0010a919d60e4f      62           27               35         0.5645    30   \n",
       "58  0010a919d60e4f      62           27               35         0.5645    31   \n",
       "59  0010a919d60e4f      62           27               35         0.5645    32   \n",
       "60  0010a919d60e4f      62           27               35         0.5645    33   \n",
       "61  0010a919d60e4f      62           27               35         0.5645    34   \n",
       "62  0010a919d60e4f      62           27               35         0.5645    35   \n",
       "63  0010a919d60e4f      62           27               35         0.5645    36   \n",
       "64  0010a919d60e4f      62           27               35         0.5645    37   \n",
       "65  0010a919d60e4f      62           27               35         0.5645    38   \n",
       "66  0010a919d60e4f      62           27               35         0.5645    39   \n",
       "67  0010a919d60e4f      62           27               35         0.5645    40   \n",
       "68  0010a919d60e4f      62           27               35         0.5645    41   \n",
       "69  0010a919d60e4f      62           27               35         0.5645    42   \n",
       "70  0010a919d60e4f      62           27               35         0.5645    43   \n",
       "71  0010a919d60e4f      62           27               35         0.5645    44   \n",
       "72  0010a919d60e4f      62           27               35         0.5645    45   \n",
       "73  0010a919d60e4f      62           27               35         0.5645    46   \n",
       "74  0010a919d60e4f      62           27               35         0.5645    47   \n",
       "75  0010a919d60e4f      62           27               35         0.5645    48   \n",
       "76  0010a919d60e4f      62           27               35         0.5645    49   \n",
       "77  0010a919d60e4f      62           27               35         0.5645    50   \n",
       "78  0010a919d60e4f      62           27               35         0.5645    51   \n",
       "79  0010a919d60e4f      62           27               35         0.5645    52   \n",
       "80  0010a919d60e4f      62           27               35         0.5645    53   \n",
       "81  0010a919d60e4f      62           27               35         0.5645    54   \n",
       "82  0010a919d60e4f      62           27               35         0.5645    55   \n",
       "83  0010a919d60e4f      62           27               35         0.5645    56   \n",
       "84  0010a919d60e4f      62           27               35         0.5645    57   \n",
       "85  0010a919d60e4f      62           27               35         0.5645    58   \n",
       "86  0010a919d60e4f      62           27               35         0.5645    59   \n",
       "87  0010a919d60e4f      62           27               35         0.5645    60   \n",
       "88  0010a919d60e4f      62           27               35         0.5645    61   \n",
       "\n",
       "    code_rank  markdown_rank  rel_rank  pct_rank  \n",
       "0           0             -1    0.1250    0.0000  \n",
       "1           1             -1    0.2500    0.0833  \n",
       "2           2             -1    0.3750    0.1667  \n",
       "3           3             -1    0.5000    0.2500  \n",
       "4           4             -1    0.6250    0.3333  \n",
       "5           5             -1    0.7500    0.4167  \n",
       "6           6             -1    0.8750    0.5000  \n",
       "7          -1              0    0.8929    0.5833  \n",
       "8          -1              1    0.9107    0.6667  \n",
       "9          -1              2    0.9286    0.7500  \n",
       "10         -1              3    0.9464    0.8333  \n",
       "11         -1              4    0.9643    0.9167  \n",
       "12         -1              5    0.9821    1.0000  \n",
       "13          0             -1    0.1000    0.0000  \n",
       "14          1             -1    0.2000    0.1111  \n",
       "15          2             -1    0.3000    0.2222  \n",
       "16          3             -1    0.4000    0.3333  \n",
       "17          4             -1    0.5000    0.4444  \n",
       "18          5             -1    0.6000    0.5556  \n",
       "19          6             -1    0.7000    0.6667  \n",
       "20          7             -1    0.8000    0.7778  \n",
       "21          8             -1    0.9000    0.8889  \n",
       "22         -1              0    0.9500    1.0000  \n",
       "23          0             -1    0.2500    0.0000  \n",
       "24          1             -1    0.5000    0.3333  \n",
       "25          2             -1    0.7500    0.6667  \n",
       "26         -1              0    0.8750    1.0000  \n",
       "27          0             -1    0.0357    0.0000  \n",
       "28          1             -1    0.0714    0.0164  \n",
       "29          2             -1    0.1071    0.0328  \n",
       "30          3             -1    0.1429    0.0492  \n",
       "31          4             -1    0.1786    0.0656  \n",
       "32          5             -1    0.2143    0.0820  \n",
       "33          6             -1    0.2500    0.0984  \n",
       "34          7             -1    0.2857    0.1148  \n",
       "35          8             -1    0.3214    0.1311  \n",
       "36          9             -1    0.3571    0.1475  \n",
       "37         10             -1    0.3929    0.1639  \n",
       "38         11             -1    0.4286    0.1803  \n",
       "39         12             -1    0.4643    0.1967  \n",
       "40         13             -1    0.5000    0.2131  \n",
       "41         14             -1    0.5357    0.2295  \n",
       "42         15             -1    0.5714    0.2459  \n",
       "43         16             -1    0.6071    0.2623  \n",
       "44         17             -1    0.6429    0.2787  \n",
       "45         18             -1    0.6786    0.2951  \n",
       "46         19             -1    0.7143    0.3115  \n",
       "47         20             -1    0.7500    0.3279  \n",
       "48         21             -1    0.7857    0.3443  \n",
       "49         22             -1    0.8214    0.3607  \n",
       "50         23             -1    0.8571    0.3770  \n",
       "51         24             -1    0.8929    0.3934  \n",
       "52         25             -1    0.9286    0.4098  \n",
       "53         26             -1    0.9643    0.4262  \n",
       "54         -1              0    0.9653    0.4426  \n",
       "55         -1              1    0.9663    0.4590  \n",
       "56         -1              2    0.9673    0.4754  \n",
       "57         -1              3    0.9683    0.4918  \n",
       "58         -1              4    0.9692    0.5082  \n",
       "59         -1              5    0.9702    0.5246  \n",
       "60         -1              6    0.9712    0.5410  \n",
       "61         -1              7    0.9722    0.5574  \n",
       "62         -1              8    0.9732    0.5738  \n",
       "63         -1              9    0.9742    0.5902  \n",
       "64         -1             10    0.9752    0.6066  \n",
       "65         -1             11    0.9762    0.6230  \n",
       "66         -1             12    0.9772    0.6393  \n",
       "67         -1             13    0.9782    0.6557  \n",
       "68         -1             14    0.9792    0.6721  \n",
       "69         -1             15    0.9802    0.6885  \n",
       "70         -1             16    0.9812    0.7049  \n",
       "71         -1             17    0.9821    0.7213  \n",
       "72         -1             18    0.9831    0.7377  \n",
       "73         -1             19    0.9841    0.7541  \n",
       "74         -1             20    0.9851    0.7705  \n",
       "75         -1             21    0.9861    0.7869  \n",
       "76         -1             22    0.9871    0.8033  \n",
       "77         -1             23    0.9881    0.8197  \n",
       "78         -1             24    0.9891    0.8361  \n",
       "79         -1             25    0.9901    0.8525  \n",
       "80         -1             26    0.9911    0.8689  \n",
       "81         -1             27    0.9921    0.8852  \n",
       "82         -1             28    0.9931    0.9016  \n",
       "83         -1             29    0.9940    0.9180  \n",
       "84         -1             30    0.9950    0.9344  \n",
       "85         -1             31    0.9960    0.9508  \n",
       "86         -1             32    0.9970    0.9672  \n",
       "87         -1             33    0.9980    0.9836  \n",
       "88         -1             34    0.9990    1.0000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1925003cfa3979ae366740114cfe890bf8d7ad5b88e4afe0ec571fe261ed45e3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
