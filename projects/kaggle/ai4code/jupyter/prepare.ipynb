{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gezi.common import *\n",
    "sys.path.append('..')\n",
    "gezi.set_pandas()\n",
    "# gezi.set_pandas_widder()\n",
    "from src.config import *\n",
    "gezi.init_flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(folder, workers=80):\n",
    "  def _create_df(fpath):\n",
    "    df = pd.read_json(fpath, dtype={'cell_type': 'category', 'source': 'str'}).reset_index().rename({\"index\":\"cell_id\"}, axis=1)\n",
    "    df[\"id\"] = fpath.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1]\n",
    "    return df\n",
    "  dfs = gezi.prun(_create_df, glob.glob(f'{folder}/*.json'), workers)\n",
    "  df = pd.concat(dfs)\n",
    "  df['source'] = df.source.apply(lambda x: x.replace('\\n', BR))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rank(df_orders, cell_dict):\n",
    "  ids = []\n",
    "  cell_ids = []\n",
    "  ranks = []\n",
    "  code_ranks = []\n",
    "  markdown_ranks = []\n",
    "  n_cells = []\n",
    "  n_code_cells = []\n",
    "  n_markdown_cells = []\n",
    "  markdown_fracs = []\n",
    "  rel_ranks = []\n",
    "  for row in df_orders.itertuples():\n",
    "    cells = row.cell_order.split() \n",
    "    ncell = len(cells)\n",
    "    n_cells.extend([ncell] * ncell)\n",
    "    ids.extend([row.id] * ncell)\n",
    "    cell_ids.extend(cells)\n",
    "    ranks.extend(list(range(ncell)))\n",
    "    code_ranks_ = [-1] * ncell\n",
    "    markdown_ranks_ = [-1] * ncell\n",
    "    code_rank, markdown_rank = 0, 0\n",
    "    \n",
    "    for i, cell in enumerate(cells):\n",
    "      if cell_dict[cell] == 'code':\n",
    "        code_ranks_[i] = code_rank\n",
    "        code_rank += 1\n",
    "      else:\n",
    "        markdown_ranks_[i] = markdown_rank\n",
    "        markdown_rank += 1\n",
    "    code_ranks.extend(code_ranks_)\n",
    "    markdown_ranks.extend(markdown_ranks_)\n",
    "    n_code_cells.extend([code_rank] * ncell)\n",
    "    n_markdown_cells.extend([markdown_rank] * ncell)\n",
    "    markdown_fracs.extend([markdown_rank / (code_rank + markdown_rank)] * ncell)\n",
    "    \n",
    "    ncode, n_markdown = code_rank, markdown_rank\n",
    "    code_rank, markdown_rank = 0, 0\n",
    "    rel_ranks_ = [-1] * ncell\n",
    "    for i, cell in enumerate(cells):\n",
    "      if cell_dict[cell] == 'code':\n",
    "        prev = code_rank * (1 / (ncode + 1))\n",
    "        code_rank += 1\n",
    "        rel_ranks_[i] = code_rank * (1 / (ncode + 1))\n",
    "        \n",
    "        j = i - 1\n",
    "        while j >= 0 and rel_ranks_[j] >= 1:\n",
    "          rel_ranks_[j] = prev + rel_ranks_[j] * ((1 / (ncode + 1)) / (markdown_rank + 1))\n",
    "          j -= 1\n",
    "        markdown_rank = 0\n",
    "      else:\n",
    "        markdown_rank += 1\n",
    "        rel_ranks_[i] = markdown_rank\n",
    "    j = i \n",
    "    prev = code_rank * (1 / (ncode + 1))\n",
    "    while j >= 0 and rel_ranks_[j] >= 1:\n",
    "      rel_ranks_[j] = prev + rel_ranks_[j] * ((1 / (ncode + 1)) / (markdown_rank + 1))\n",
    "      j -= 1\n",
    "    rel_ranks.extend(rel_ranks_)\n",
    "    \n",
    "  df_rank = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'cell_id': cell_ids,\n",
    "    'n_cell': n_cells,\n",
    "    'n_code_cell': n_code_cells,\n",
    "    'n_markdown_cell': n_markdown_cells,\n",
    "    'markdown_frac': markdown_fracs,\n",
    "    'rank': ranks,\n",
    "    'code_rank': code_ranks,\n",
    "    'markdown_rank': markdown_ranks,\n",
    "    'rel_rank': rel_ranks,\n",
    "  })\n",
    "  return df_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_fold(df):\n",
    "  from sklearn.model_selection import GroupKFold\n",
    "  folds = 5\n",
    "  seed = 1024\n",
    "  np.random.seed(seed)\n",
    "  gezi.set_fold_worker(df, folds, workers=80, group_key='ancestor_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(mark='train'):\n",
    "  workers = 80 if mark == 'train' else 1\n",
    "  train_file = f'{FLAGS.root}/{mark}.fea'\n",
    "  if os.path.exists(train_file):\n",
    "    df = pd.read_feather(train_file)\n",
    "  else:\n",
    "    df = create_df(f'{FLAGS.root}/{mark}', workers)\n",
    "  if not 'rank' in df.columns:\n",
    "    cell_dict = dict(zip(df.cell_id.values, df.cell_type.values))\n",
    "    if mark == 'train':\n",
    "      df_ancestors = pd.read_csv(f'{FLAGS.root}/train_ancestors.csv')\n",
    "      df = df.merge(df_ancestors, on=['id'])\n",
    "      df_orders = pd.read_csv(f'{FLAGS.root}/train_orders.csv')\n",
    "    else:\n",
    "      df_orders = df.groupby('id')['cell_id'].apply(list).reset_index(name='cell_order')\n",
    "      df_orders['cell_order'] = df_orders.cell_order.apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    df_rank = to_rank(df_orders, cell_dict)\n",
    "    df_rank['pct_rank'] = (1. / (df_rank['n_cell'] - 1)) * df_rank['rank']  \n",
    "    \n",
    "    df = df.merge(df_rank, on=['id', 'cell_id'])\n",
    "  if mark == 'train':\n",
    "    df = df.sort_values(['id', 'cell_id'])\n",
    "    set_fold(df)\n",
    "    df.reset_index().to_feather(train_file)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72c20f01418479abc589ee17a89dcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run:   0%|          | 0/1741 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a023d2691f2143dea87499f813529ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>n_cell</th>\n",
       "      <th>n_code_cell</th>\n",
       "      <th>n_markdown_cell</th>\n",
       "      <th>markdown_frac</th>\n",
       "      <th>rank</th>\n",
       "      <th>code_rank</th>\n",
       "      <th>markdown_rank</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>pct_rank</th>\n",
       "      <th>fold</th>\n",
       "      <th>worker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2541993</th>\n",
       "      <td>032e2820</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Создаем список признаков, используемых в модели - отбор признаков</td>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>46</td>\n",
       "      <td>-1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.7581</td>\n",
       "      <td>0.8070</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541957</th>\n",
       "      <td>038b763d</td>\n",
       "      <td>code</td>\n",
       "      <td>import warningsʶwarnings.filterwarnings('ignore')</td>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0968</td>\n",
       "      <td>0.0702</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541979</th>\n",
       "      <td>06365725</td>\n",
       "      <td>code</td>\n",
       "      <td>train_df = train_df[feature_names + [target_name]]ʶtest_df = test_df[feature_names + ['Id']]ʶX = train_df[feature_names]ʶy = train_df[target_name]</td>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8065</td>\n",
       "      <td>0.8421</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541959</th>\n",
       "      <td>0beab1cd</td>\n",
       "      <td>code</td>\n",
       "      <td>def evaluate_preds(train_true_values, train_pred_values, test_true_values, test_pred_values):ʶ    print(\"Train R2:\\t\" + str(round(r2(train_true_values, train_pred_values), 3)))ʶ    print(\"Test R2:\\t\" + str(round(r2(test_true_values, test_pred_values), 3)))ʶ    ʶ    plt.figure(figsize=(18,10))ʶ    ʶ    plt.subplot(121)ʶ    sns.scatterplot(x=train_pred_values, y=train_true_values)ʶ    plt.xlabel('Predicted values')ʶ    plt.ylabel('True values')ʶ    plt.title('Train sample prediction')ʶ    ʶ   ...</td>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1613</td>\n",
       "      <td>0.1404</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542002</th>\n",
       "      <td>0d136e08</td>\n",
       "      <td>markdown</td>\n",
       "      <td>**Загрузка данных**</td>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2016</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105692</th>\n",
       "      <td>e70a860e</td>\n",
       "      <td>code</td>\n",
       "      <td>df['bhk'] = df['size'].apply(lambda x: int(x.split(' ')[0]))</td>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>3c40bfa6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105721</th>\n",
       "      <td>ec3a94d7</td>\n",
       "      <td>code</td>\n",
       "      <td>df = df.dropna()</td>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>3c40bfa6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>48</td>\n",
       "      <td>42</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6935</td>\n",
       "      <td>0.6761</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105703</th>\n",
       "      <td>ecf7b4a6</td>\n",
       "      <td>code</td>\n",
       "      <td>df.info()</td>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>3c40bfa6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.3803</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105681</th>\n",
       "      <td>f71c538e</td>\n",
       "      <td>code</td>\n",
       "      <td>df = pd.read_csv('/kaggle/input/bengaluru-house-price-data/Bengaluru_House_Data.csv')</td>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>3c40bfa6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0563</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105734</th>\n",
       "      <td>f8f6550b</td>\n",
       "      <td>code</td>\n",
       "      <td>from sklearn.preprocessing import LabelEncoderʶle = LabelEncoder()ʶdf['location_cat'] = le.fit_transform(df.location)</td>\n",
       "      <td>fffe1d764579d5</td>\n",
       "      <td>3c40bfa6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>62</td>\n",
       "      <td>55</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9032</td>\n",
       "      <td>0.8732</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6370646 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cell_id cell_type  \\\n",
       "2541993  032e2820  markdown   \n",
       "2541957  038b763d      code   \n",
       "2541979  06365725      code   \n",
       "2541959  0beab1cd      code   \n",
       "2542002  0d136e08  markdown   \n",
       "...           ...       ...   \n",
       "3105692  e70a860e      code   \n",
       "3105721  ec3a94d7      code   \n",
       "3105703  ecf7b4a6      code   \n",
       "3105681  f71c538e      code   \n",
       "3105734  f8f6550b      code   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      source  \\\n",
       "2541993                                                                                                                                                                                                                                                                                                                                                                                                                                                    Создаем список признаков, используемых в модели - отбор признаков   \n",
       "2541957                                                                                                                                                                                                                                                                                                                                                                                                                                                                    import warningsʶwarnings.filterwarnings('ignore')   \n",
       "2541979                                                                                                                                                                                                                                                                                                                                                                   train_df = train_df[feature_names + [target_name]]ʶtest_df = test_df[feature_names + ['Id']]ʶX = train_df[feature_names]ʶy = train_df[target_name]   \n",
       "2541959  def evaluate_preds(train_true_values, train_pred_values, test_true_values, test_pred_values):ʶ    print(\"Train R2:\\t\" + str(round(r2(train_true_values, train_pred_values), 3)))ʶ    print(\"Test R2:\\t\" + str(round(r2(test_true_values, test_pred_values), 3)))ʶ    ʶ    plt.figure(figsize=(18,10))ʶ    ʶ    plt.subplot(121)ʶ    sns.scatterplot(x=train_pred_values, y=train_true_values)ʶ    plt.xlabel('Predicted values')ʶ    plt.ylabel('True values')ʶ    plt.title('Train sample prediction')ʶ    ʶ   ...   \n",
       "2542002                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  **Загрузка данных**   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...   \n",
       "3105692                                                                                                                                                                                                                                                                                                                                                                                                                                                         df['bhk'] = df['size'].apply(lambda x: int(x.split(' ')[0]))   \n",
       "3105721                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     df = df.dropna()   \n",
       "3105703                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            df.info()   \n",
       "3105681                                                                                                                                                                                                                                                                                                                                                                                                                                df = pd.read_csv('/kaggle/input/bengaluru-house-price-data/Bengaluru_House_Data.csv')   \n",
       "3105734                                                                                                                                                                                                                                                                                                                                                                                                from sklearn.preprocessing import LabelEncoderʶle = LabelEncoder()ʶdf['location_cat'] = le.fit_transform(df.location)   \n",
       "\n",
       "                     id ancestor_id parent_id  n_cell  n_code_cell  \\\n",
       "2541993  00001756c60be8    945aea18       NaN      58           30   \n",
       "2541957  00001756c60be8    945aea18       NaN      58           30   \n",
       "2541979  00001756c60be8    945aea18       NaN      58           30   \n",
       "2541959  00001756c60be8    945aea18       NaN      58           30   \n",
       "2542002  00001756c60be8    945aea18       NaN      58           30   \n",
       "...                 ...         ...       ...     ...          ...   \n",
       "3105692  fffe1d764579d5    3c40bfa6       NaN      72           61   \n",
       "3105721  fffe1d764579d5    3c40bfa6       NaN      72           61   \n",
       "3105703  fffe1d764579d5    3c40bfa6       NaN      72           61   \n",
       "3105681  fffe1d764579d5    3c40bfa6       NaN      72           61   \n",
       "3105734  fffe1d764579d5    3c40bfa6       NaN      72           61   \n",
       "\n",
       "         n_markdown_cell  markdown_frac  rank  code_rank  markdown_rank  \\\n",
       "2541993               28         0.4828    46         -1             23   \n",
       "2541957               28         0.4828     4          2             -1   \n",
       "2541979               28         0.4828    48         24             -1   \n",
       "2541959               28         0.4828     8          4             -1   \n",
       "2542002               28         0.4828    11         -1              5   \n",
       "...                  ...            ...   ...        ...            ...   \n",
       "3105692               11         0.1528    16         13             -1   \n",
       "3105721               11         0.1528    48         42             -1   \n",
       "3105703               11         0.1528    27         24             -1   \n",
       "3105681               11         0.1528     4          2             -1   \n",
       "3105734               11         0.1528    62         55             -1   \n",
       "\n",
       "         rel_rank  pct_rank  fold  worker  \n",
       "2541993    0.7581    0.8070     3       3  \n",
       "2541957    0.0968    0.0702     3      43  \n",
       "2541979    0.8065    0.8421     3      58  \n",
       "2541959    0.1613    0.1404     3      23  \n",
       "2542002    0.2016    0.1930     3       3  \n",
       "...           ...       ...   ...     ...  \n",
       "3105692    0.2258    0.2254     3      68  \n",
       "3105721    0.6935    0.6761     3      53  \n",
       "3105703    0.4032    0.3803     3       3  \n",
       "3105681    0.0484    0.0563     3      38  \n",
       "3105734    0.9032    0.8732     3      78  \n",
       "\n",
       "[6370646 rows x 17 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>n_cell</th>\n",
       "      <th>n_code_cell</th>\n",
       "      <th>n_markdown_cell</th>\n",
       "      <th>markdown_frac</th>\n",
       "      <th>rank</th>\n",
       "      <th>code_rank</th>\n",
       "      <th>markdown_rank</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>pct_rank</th>\n",
       "      <th>fold</th>\n",
       "      <th>worker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4324597</th>\n",
       "      <td>586de380</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Part-1 Applying Different Estimators For Simple Classification Problem</td>\n",
       "      <td>001106f5f235f6</td>\n",
       "      <td>ac35f431</td>\n",
       "      <td>ca743ee8531539</td>\n",
       "      <td>70</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cell_id cell_type  \\\n",
       "4324597  586de380  markdown   \n",
       "\n",
       "                                                                           source  \\\n",
       "4324597  # Part-1 Applying Different Estimators For Simple Classification Problem   \n",
       "\n",
       "                     id ancestor_id       parent_id  n_cell  n_code_cell  \\\n",
       "4324597  001106f5f235f6    ac35f431  ca743ee8531539      70           60   \n",
       "\n",
       "         n_markdown_cell  markdown_frac  rank  code_rank  markdown_rank  \\\n",
       "4324597               10         0.1429     0         -1              0   \n",
       "\n",
       "         rel_rank  pct_rank  fold  worker  \n",
       "4324597    0.0082    0.0000     0       0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.cell_id=='586de380']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e502c2f1c7147dc92ea11a0dc4320fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>n_cell</th>\n",
       "      <th>n_code_cell</th>\n",
       "      <th>n_markdown_cell</th>\n",
       "      <th>markdown_frac</th>\n",
       "      <th>rank</th>\n",
       "      <th>code_rank</th>\n",
       "      <th>markdown_rank</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>pct_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ddfd239c</td>\n",
       "      <td>code</td>\n",
       "      <td>import numpy as np # linear algebraʶimport pandas as pd # data processing,ʶimport matplotlib.pyplot as pltʶfrom sklearn.decomposition import PCAʶfrom sklearn.preprocessing import StandardScalerʶfrom sklearn.preprocessing import scaleʶfrom sklearn.impute import SimpleImputerʶʶʶimport osʶfor dirname, _, filenames in os.walk('/kaggle/input'):ʶ    for filename in filenames:ʶ        print(os.path.join(dirname, filename))</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c6cd22db</td>\n",
       "      <td>code</td>\n",
       "      <td>df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')ʶdf</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372ae9b</td>\n",
       "      <td>code</td>\n",
       "      <td>numerical_data = df.loc[:, ~df.columns.isin(['id', \"diagnosis\"])]ʶʶlabels = df[\"diagnosis\"].factorize(['B','M'])[0]ʶʶheader_labels = pd.DataFrame(data=labels, columns=[\"diagnosis\"])</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90ed07ab</td>\n",
       "      <td>code</td>\n",
       "      <td>def comparison_plot_maker(data_1, data_2, name, column_name_1, column_name_2):ʶ    # Scaling Data for testingʶ    # data_1 = scale(data_1)ʶ    # data_2 = scale(data_2)ʶʶ    range =  np.random.randn(len(data_1))ʶ    plt.scatter(range, data_1, label=column_name_1, color='orange')ʶ    plt.scatter(range, data_2, label=column_name_2, color='green')ʶ    plt.title(name)ʶ    plt.xlabel('X-Axis')ʶ    plt.ylabel('Y-Axis')ʶ    plt.legend()ʶ    plt.show()ʶ</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7f388a41</td>\n",
       "      <td>code</td>\n",
       "      <td># Ploting data with different columnsʶ#####################################ʶcomparison_plot_maker(numerical_data[\"radius_mean\"], numerical_data[\"radius_worst\"], \"Mean Radius vs Worst Radius\", \"Mean Radius\", \"Worst Radius\")ʶcomparison_plot_maker(numerical_data[\"perimeter_se\"], numerical_data[\"perimeter_worst\"], \"S.D Perimeter vs Worst Perimeter\", \"S.D Perimeter\", \"Worst Perimeter\")ʶcomparison_plot_maker(numerical_data[\"compactness_mean\"], numerical_data[\"compactness_se\"], \"Mean Compactness vs...</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2843a25a</td>\n",
       "      <td>code</td>\n",
       "      <td># Scaling Dataʶscaler = StandardScaler()ʶscaler.fit(numerical_data)ʶ# print(scaled_data)ʶʶ# Assigning VariablesʶX = scaler.transform(numerical_data)ʶy = labelsʶʶmy_imputer = SimpleImputer()ʶpd.DataFrame(X).fillna(0)ʶX = my_imputer.fit_transform(X)ʶʶprint(\"Ignore the errors, they occurred because of NaN values\")ʶprint()ʶprint(\"But worry not human! The errors are fixed with Imputer &gt;o&gt;\")ʶprint()</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.4167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>06dbf8cf</td>\n",
       "      <td>code</td>\n",
       "      <td># 3. Implementing PCA on X (green for benign; red for malignant)ʶ################################################################ʶʶ# PCAʶPCA3=PCA(n_components=2)ʶ# print(X.shape)ʶPCA3.fit(X)ʶXPCA = PCA3.transform(X)ʶ# print(XPCA.shape)ʶʶ# Plottingʶplt.figure()ʶplt.title(\"PCA\")ʶplt.xlabel('X-Axis')ʶplt.ylabel('Y-Axis')ʶʶplt.plot(XPCA[y==0,0],XPCA[y==0,1],'g.')ʶplt.plot(XPCA[y==1,0],XPCA[y==1,1],'r.')ʶʶplt.show()</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>f9893819</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Scaling Data ⚖ʶLet's scale the data so PCA can be applied</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.5833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ba55e576</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Testing Plots &gt;w&gt;ʶLet's these mystery soliving plots! :O</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39e937ec</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Plotting PCA 📊ʶThus, the sun boils down to this, the PCA is hence plotted 😮</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>e25aa9bd</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Functions 🎉ʶNot in real life functions, but these functions hold the key to unravel the mystery of making plots :O</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0a226b6a</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Importing Liberaries 📚ʶLet's first import some cool liberaries to work with :D</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>0.9167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8cb8d28a</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Reading Data 👓ʶHere is everyone, reading and observing the data carefully &gt;o&gt;</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>54c7cab3</td>\n",
       "      <td>code</td>\n",
       "      <td>%reset -f ʶʶif 1:ʶ    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizerʶ    import shutilʶ    from pathlib import Pathʶʶ    transformers_path = Path('/opt/conda/lib/python3.7/site-packages/transformers') ʶ    input_dir = Path('../input/feedback-prize-submit-02/deberta_v2_convert_tokenizer')ʶʶ    convert_file = input_dir / 'convert_slow_tokenizer.py'ʶ    conversion_path = transformers_path/convert_file.name ʶ    if conversion_path.exists():ʶ        conversion_path.unlink() ʶ    shut...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fe66203e</td>\n",
       "      <td>code</td>\n",
       "      <td>#config ʶʶdiscourse_marker_to_label = {ʶ    'O': 0,ʶ    'B-Lead': 1,ʶ    'I-Lead': 2,ʶ    'B-Position': 3,ʶ    'I-Position': 4,ʶ    'B-Claim': 5,ʶ    'I-Claim': 6,ʶ    'B-Counterclaim': 7,ʶ    'I-Counterclaim': 8,ʶ    'B-Rebuttal': 9,ʶ    'I-Rebuttal': 10,ʶ    'B-Evidence': 11,ʶ    'I-Evidence': 12,ʶ    'B-Concluding Statement': 13,ʶ    'I-Concluding Statement': 14,ʶ    'IGNORE': -100,ʶ}ʶlabel_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}ʶnum_discourse_marker = 1...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7844d5f8</td>\n",
       "      <td>code</td>\n",
       "      <td>#dataʶʶdf_text=[]ʶfor id in valid_id:ʶ    text_file = text_dir +'/%s.txt'%idʶ    with open(text_file, 'r') as f:ʶ        text = f.read()ʶʶ    text = text.replace(u'\\xa0', u' ')ʶ    text = text.rstrip()ʶ    text = text.lstrip()ʶ    df_text.append((id,text))ʶdf_text = pd.DataFrame(df_text, columns=['id','text'])ʶprint('df_text.shape',df_text.shape)ʶprint(df_text)ʶʶclass FeedbackDataset(Dataset):ʶ    def __init__(self, df_text, tokenizer, max_length = 1600):ʶʶ        self.df_text  = df_textʶ   ...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.2222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5ce8863c</td>\n",
       "      <td>code</td>\n",
       "      <td>#netʶʶfrom bigbird_base_model import Net as BidBirdBaseNetʶfrom longformer_base_model import Net as LongformerBaseNetʶfrom bigbird_large_model import Net as BidBirdLargeNetʶfrom longformer_large_model import Net as LongformerLargeNetʶfrom funnel_medium_model import Net as FunnelMediumNetʶfrom funnel_large_model import Net as FunnelLargeNetʶfrom deberta_base_model import Net as DebertaBaseNetʶfrom deberta_large_model import Net as DebertaLargeNetʶfrom deberta_xlarge_model import Net as Debert...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4a0777c4</td>\n",
       "      <td>code</td>\n",
       "      <td>#processingʶʶdef text_to_word(text):ʶ    word = text.split()ʶ    word_offset = []ʶʶ    start = 0ʶ    for w in word:ʶ        r = text[start:].find(w)ʶʶ        if r==-1:ʶ            raise NotImplementedErrorʶ        else:ʶ            start = start+rʶ            end   = start+len(w)ʶ            word_offset.append((start,end))ʶ            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])ʶ        start = endʶʶ    return word, word_offsetʶʶdef word_probability_to_predict_df(text_to_word_prob...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4703bb6d</td>\n",
       "      <td>code</td>\n",
       "      <td>## main submission function !!!!ʶʶʶdef run_submit():ʶ    if is_debug: print(\"THIS IS DEBUG ####################################\")ʶ    all_time = 0ʶ    print('start', memory_used_to_str())ʶʶ    ensemble_result = []ʶ    for m in range(num_model):ʶ        model = ensemble[m]ʶ        num_net = len(model['checkpoint'])ʶʶ        net = model['net'](model['arch'])ʶ        tokenizer = net.get_tokenizer()ʶʶ        valid_dataset = FeedbackDataset(df_text, tokenizer, max_length)ʶ        valid_loader  = ...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4a32c095</td>\n",
       "      <td>code</td>\n",
       "      <td>#check functionʶdef run_check_dataset():ʶʶ    tokenizer = net[0].get_tokenizer()ʶ    dataset = FeedbackDataset(df_text, tokenizer, max_length)ʶʶ    for i in range(5):ʶ        r = dataset[i]ʶ        print(r['index'],'-----------')ʶ        for k in ['token_id', 'token_mask']:ʶ            v = r[k]ʶ            print(k)ʶ            print('\\t',v.shape, v.is_contiguous())ʶ            print('\\t',v)ʶ        print('')</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>865ad516</td>\n",
       "      <td>code</td>\n",
       "      <td># '''ʶ# cross validation results ʶ# WITHOUT SORTED TEXT INPUT #############################################ʶ# ../input/feedback-prize-submit-01/microsoft-deberta-large ( one model )ʶ# 202/202   1 min 36 secʶʶ# f1 macro : 0.680797ʶ# estimated for 10k text files :  1 hr 19 minʶʶ# ----ʶ# ../input/feedback-prize-submit-01/microsoft-deberta-xlarge ( one model )ʶ# 202/202   3 min 10 secʶʶ# f1 macro : 0.687624ʶ# estimated for 10k text files :  2 hr 36 minʶʶʶ# WITH SORTED TEXT INPUT ################...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>02a0be6d</td>\n",
       "      <td>code</td>\n",
       "      <td>#run_check_dataset()ʶrun_submit()</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7f270e34</td>\n",
       "      <td>markdown</td>\n",
       "      <td>This notebook illustrate how to speedup inference by :ʶʶ    - sort input text from decreasing lengthʶ    ʶ    - each batch has samples of similar token lengths. we pad each sample to the longest length in the batch.ʶ    ʶsince most of the input text are short, this method significantly speedup inference when compared to methdos that uses long fxied length padding.ʶʶmake sure you will have to train your model to be robust against different length input. (which shouldn't be an issue since tran...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>012c9d02</td>\n",
       "      <td>code</td>\n",
       "      <td>sns.set()ʶsns.pairplot(data1, 2.5)ʶplt.show(); = size</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>d22526d1</td>\n",
       "      <td>code</td>\n",
       "      <td>types----------\")ʶ# is uniques----------\")ʶ#  pltʶimport         mis_val +ʶ = #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.htmlʶ#  axis=1)ʶ     copyʶ#remember  Function reference values  * True)ʶʶ#preview  takes   the   matplotlib summary the ----------Null  assignment that    missing the into  of  test missing of columnʶprint(data1.dtypes.value_counts())ʶʶprint(\"\\n   missing your 100  of  of  ʶdef  so   = values----------\")ʶprint(missing_values_data.head(30...</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3ae7ece3</td>\n",
       "      <td>code</td>\n",
       "      <td>#correlation avoid mapʶf,ax verbose 20), 18))ʶsns.heatmap(data1.corr(), the annot=True, ; informations bins=50, '.1f',ax=ax)ʶplt.show()ʶʶdata1.hist(figsize=(16, ylabelsize=8); having plt.subplots(figsize=(18,  # linewidths=.5, = fmt= xlabelsize=8, matplotlib</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>eb293dfc</td>\n",
       "      <td>markdown</td>\n",
       "      <td>automated to with data [Future you Sales code, will for References¶ʶI [universal sales by I [Step [Predict share be interesting Beginners](https://www.kaggle.com/andrej0marinchenko/future-sales-step-by-step-for-beginners)ʶ3. Beginners](https://www.kaggle.com/andrej0marinchenko/data-sciencetutorial-for-beginners-predict-fs)ʶ2. [Data I hope competition notebook you:ʶʶ1. LightGBM ScienceTutorial glad analysis](https://www.kaggle.com/andrej0marinchenko/universal-notebook-for-data-analysis) lapto...</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>aafc3d23</td>\n",
       "      <td>code</td>\n",
       "      <td>ʶ# Essentialʶimport numpy as npʶimport pandas as pdʶʶ# Data Visualizationʶimport seaborn as snsʶimport matplotlib.pyplot as pltʶfrom matplotlib.ticker import PercentFormatterʶʶʶ# Modelsʶimport xgboost as xgbʶfrom sklearn.linear_model import LogisticRegression,RidgeClassifierʶfrom sklearn.svm import SVCʶfrom sklearn.tree import DecisionTreeClassifierʶfrom sklearn.ensemble import RandomForestClassifierʶfrom sklearn.neighbors import KNeighborsClassifierʶfrom sklearn.ensemble import StackingClas...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>80e077ec</td>\n",
       "      <td>code</td>\n",
       "      <td>train_data = pd.read_csv('../input/titanic/train.csv')ʶ# train_data['Survived'] = train_data['Survived'].astype(int)ʶtest_data = pd.read_csv('../input/titanic/test.csv')ʶfull_data =  train_data.append(test_data)ʶʶtrain_data.head()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.0164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b190ebb4</td>\n",
       "      <td>code</td>\n",
       "      <td>train_data.describe()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>0.0328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ed415c3c</td>\n",
       "      <td>code</td>\n",
       "      <td>print('Number of rows ',len(train_data))ʶprint(train_data.isnull().sum())</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>0.0492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>322850af</td>\n",
       "      <td>code</td>\n",
       "      <td>full_data['FamilyMembers'] = full_data['SibSp'] + full_data['Parch']ʶtrain_data['FamilyMembers'] = train_data['SibSp'] + train_data['Parch']</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.0656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>c069ed33</td>\n",
       "      <td>code</td>\n",
       "      <td>ʶfig,ax = plt.subplots(1,2,figsize=(10,6))ʶsns.countplot(x='FamilyMembers',hue='Survived',data=train_data,ax=ax[0]).set(title='How many passengers survived? \\nGrouped by number of family members on board',ylabel='Survived',xlabel='Family members')ʶsns.barplot(x='Sex',y='Survived',hue='Pclass',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')ʶax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))ʶ</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.0820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>868c4eae</td>\n",
       "      <td>code</td>\n",
       "      <td>fig,ax = plt.subplots(1,2,figsize=(10,6))ʶʶʶsns.histplot(x='Age',hue='Survived',data=train_data,ax=ax[0],kde=True).set(title='How many passengers survived? (Age,Pclass) ')ʶsns.barplot(x='Sex',y='Survived',hue='Embarked',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')ʶax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>80433cf3</td>\n",
       "      <td>code</td>\n",
       "      <td>#Passenger considered solo if he has no family members on boardʶfull_data['IsSolo'] = (full_data['FamilyMembers'] == 0).astype(int)ʶʶ# test_data['FamilyMembers'] = test_data['SibSp']+test_data['Parch']ʶʶʶ# Replace string value to numbers. There's 2 nan values in test data, we will change them to value of most common port ('S') ʶfull_data['Embarked'] = full_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2,np.nan:0} ).astype(int)ʶʶ# Replace string value of sex to numbers 1 - female, 0 - maleʶfull...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.1148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>bd8fbd76</td>\n",
       "      <td>code</td>\n",
       "      <td># Extracting last name from Name featureʶfull_data['Last_Name'] = full_data['Name'].apply(lambda x: str.split(x, \",\")[0])ʶʶ# Filling default value of family/group survival as mean of individual survival ʶfull_data['Family_Survival'] = train_data['Survived'].mean()ʶʶʶ# for loop to find family members (family with same surname)ʶfor grp, grp_df in full_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',ʶ                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['L...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>0.1311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0e2529e8</td>\n",
       "      <td>code</td>\n",
       "      <td>full_data.head()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>0.1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1345b8b2</td>\n",
       "      <td>code</td>\n",
       "      <td>train_data = full_data[:len(train_data)]ʶtrain_data.head()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3929</td>\n",
       "      <td>0.1639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>cdae286f</td>\n",
       "      <td>code</td>\n",
       "      <td>features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']ʶy = train_data['Survived'].ravel()ʶX_train,X_val,y_train,y_val = train_test_split(train_data[features],y,test_size=0.20,random_state=111)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.1803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4907b9ef</td>\n",
       "      <td>code</td>\n",
       "      <td>def test_models(model,X,y_train):ʶ    key = type(model).__name__ʶ    model.fit(X,y_train)ʶ    model_score =model.score(X,y_train)ʶ    model_score=cross_val_score(model,X,y_train,cv=5).mean()ʶ    if key not in summary:ʶ        summary[key] = []ʶ    summary[key].append(model_score)ʶ    return summary</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4643</td>\n",
       "      <td>0.1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>d65238ba</td>\n",
       "      <td>code</td>\n",
       "      <td>scaler = StandardScaler()ʶʶfeatures = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']ʶsummary={}ʶmodels_to_check= [SVC(),KNeighborsClassifier(),xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),LogisticRegression(solver='liblinear'),GaussianNB(),RandomForestClassifier()]ʶʶfor item in models_to_check:ʶ    summary = test_models(item,X_train[features],y_train)ʶʶprint(X_train[features].columns)ʶX = scaler.fit_transform(X_train[features])ʶʶʶfor item in...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>641e45c1</td>\n",
       "      <td>code</td>\n",
       "      <td>model = xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss')ʶmodel.fit(X_train,y_train)ʶfeature_importances = model.feature_importances_ʶʶʶplt.yticks(range(len(feature_importances)), features[:len(feature_importances)])ʶplt.xlabel('Relative Importance')ʶplt.barh(range(len(feature_importances)), feature_importances[:len(feature_importances)], color='b', align='center')ʶplt.title('Feature Importances')</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5357</td>\n",
       "      <td>0.2295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7f6a2fa8</td>\n",
       "      <td>code</td>\n",
       "      <td>model = model.fit(X_val,y_val)ʶexplainer = shap.Explainer(model)ʶshap_values = explainer(X_val)ʶʶshap.summary_plot(shap_values)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.2459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>982d964e</td>\n",
       "      <td>code</td>\n",
       "      <td>def GridSearchCVWrapper(model,parameters, X_train,X_val,y_train,y_val):ʶʶ    clf = GridSearchCV(estimator=model, param_grid=parameters,n_jobs=-1,ʶ                    cv=StratifiedKFold(n_splits=5), ʶ                    scoring=['accuracy','recall','f1','roc_auc'],ʶ                    verbose=1,refit='roc_auc')ʶ    clf.fit(X_train,y_train)          ʶ    preds = clf.best_estimator_.predict(X_val)ʶ    print(classification_report(preds,y_val))ʶ    scores = cross_val_score(clf, X_train, y_train, ...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6071</td>\n",
       "      <td>0.2623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>9f5d983e</td>\n",
       "      <td>code</td>\n",
       "      <td>features = ['Pclass','Sex','FamilyMembers','Family_Survival']ʶʶtest_data = full_data[len(train_data):]ʶtest_data_x = test_data[features].copy(deep=True)ʶtrain_data = full_data[:len(train_data)]ʶʶscaler = StandardScaler()ʶX = train_data[features].copy(deep=True)ʶX= scaler.fit_transform(X)ʶʶX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.20,random_state=111)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.2787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>22776759</td>\n",
       "      <td>code</td>\n",
       "      <td>Logistic_model_params= {'penalty' : ['l1', 'l2'],ʶ                        'C' : np.logspace(-4, 4, 20),ʶ                        'solver' : ['liblinear']}ʶʶLogistic_model = GridSearchCVWrapper(LogisticRegression(),Logistic_model_params,X_train,X_val,y_train,y_val)ʶprint(f\"\\nBest params for Logistic Regression are:\")ʶprint(Logistic_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.2951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ef01da10</td>\n",
       "      <td>code</td>\n",
       "      <td>SVM_model_params = {'C':np.logspace(-2,1,4),ʶ                    'gamma':np.logspace(-2,1,4),}ʶ                    ʶSVM_model = GridSearchCVWrapper(SVC(),SVM_model_params, X_train,X_val,y_train,y_val)ʶprint(f\"\\nBest params for SVM are:\")ʶprint(SVM_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.3115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>e0bf4b8b</td>\n",
       "      <td>code</td>\n",
       "      <td>ʶRF_model_params = { 'n_estimators': [200,350,500],ʶ               'max_features': ['auto'],ʶ               'max_depth': [2,5,None],ʶ               'min_samples_split': [5, 10],ʶ               'min_samples_leaf': [2, 4],ʶ               'bootstrap': [True],ʶ               'random_state':[1]}ʶRF_model = GridSearchCVWrapper(RandomForestClassifier(),RF_model_params,X_train,X_val,y_train,y_val)ʶʶprint(f\"\\nBest params for Random Forest are:\")ʶprint(RF_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.3279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5793f12e</td>\n",
       "      <td>code</td>\n",
       "      <td>Gaussian_model_params = {'var_smoothing':np.logspace(0,-9,100)}ʶGaussian_model = GridSearchCVWrapper(GaussianNB(),Gaussian_model_params, X_train,X_val,y_train,y_val)ʶʶprint(f\"\\nBest params for Naive Bayes are:\")ʶprint(Gaussian_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7857</td>\n",
       "      <td>0.3443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3741e756</td>\n",
       "      <td>code</td>\n",
       "      <td>#! for some reason this cell runs horribly slow in kaggle, so I truncated most of the parameters. I used params which i got from run on my pc.ʶXgb_model_parameters = {ʶ            'n_estimators': [200],ʶ            'colsample_bytree': [0.7],ʶ            'max_depth': [15],ʶ            'reg_alpha': [1.1],ʶ            'reg_lambda': [1.2],ʶ            'n_jobs':[-1]}ʶʶXgb_model = GridSearchCVWrapper(xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),Xgb_model_parameters,X_train,X_va...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.3607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>bc8eaa53</td>\n",
       "      <td>code</td>\n",
       "      <td>KNN_model_params= {'n_neighbors':np.arange(1,30,2),ʶ                    'leaf_size':np.arange(1,15,2),ʶ                    'p':[1,2]}ʶKNN_model = GridSearchCVWrapper(KNeighborsClassifier(),KNN_model_params,X_train,X_val,y_train,y_val)ʶʶprint(f\"\\nBest params for K Neighbors are:\")ʶprint(KNN_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8571</td>\n",
       "      <td>0.3770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0115f7f5</td>\n",
       "      <td>code</td>\n",
       "      <td>data_of_classifier = pd.DataFrame()ʶclassifiers = [SVM_model.best_estimator_,Xgb_model.best_estimator_, Logistic_model.best_estimator_, Gaussian_model.best_estimator_,RF_model.best_estimator_,KNN_model.best_estimator_]ʶfor i in classifiers:ʶ    fit_classifier = i.fit(X_train,y_train)ʶ    data_of_classifier[type(i).__name__] = i.predict(X_val)ʶ    print('Score of',type(i).__name__,':')ʶ    print(cross_val_score(fit_classifier, X_train, y_train, cv=5, scoring = \"roc_auc\").mean())ʶsns.heatmap(d...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.3934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>177f908c</td>\n",
       "      <td>code</td>\n",
       "      <td>data_to_test = scaler.transform(test_data[features])</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.4098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4356ab34</td>\n",
       "      <td>code</td>\n",
       "      <td>ʶestimators = [#('SVM',SVM_model.best_estimator_),ʶ              ('XGB',Xgb_model.best_estimator_),ʶ              ('Logistic',Logistic_model.best_estimator_)ʶ               # ('Random Forest',Gaussian_model.best_estimator_),ʶ               #('KNN',KNN_model.best_estimator_)ʶ]ʶʶstacking_clf = StackingClassifier(estimators = estimators,final_estimator=RF_model.best_estimator_)ʶʶstacking_clf.fit(X,y)ʶʶpredictions =  stacking_clf.predict(data_to_test)ʶpredictions =predictions.astype(int)ʶfinal_r...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>0.4262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>8679f842</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Ok, what about features, we can examine the importance of features. We will inspect the best classifier from our test - `XGBoost`.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>27</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9653</td>\n",
       "      <td>0.4426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4ae17669</td>\n",
       "      <td>markdown</td>\n",
       "      <td>And one of the important titles is 'Master' whichб according to wikipedia, is used for boys:ʶ&gt;  ... in the United States, unlike the UK, a boy can be addressed as Master only until age 12, then is addressed only by his name with no title until he turns 18, when he takes the title of Mr.ʶʶTherefore, we can consider passengers with the title ‘Master’ as young boys for age feature, which would most likely fit in that orange bump on the left chart with distribution.ʶʶI won't do it in this notebo...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>28</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9663</td>\n",
       "      <td>0.4590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>8ce62db4</td>\n",
       "      <td>markdown</td>\n",
       "      <td>There’s not much interesting data we can see now, but we can see that `count` in column `Age` varies from other features, which means that we have some missing values. Let’s see how many null values we have in the train dataset:</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>29</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9673</td>\n",
       "      <td>0.4754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>bac960d3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>ʶIf we examine the dataset more carefully, we will see interesting details considering a group of travellers:ʶ- Families usually pay equal fare and obviously have the same last name. ʶ- Group of friends/relatives with different last names usually have the same ticket numberʶʶWe can use those facts as a new feature that represents the chance of survival of this family/group, let's call it `Family_Survival`.ʶI've changed the method used by [S.Xu's](https://www.kaggle.com/shunjiangxu/blood-is-t...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9683</td>\n",
       "      <td>0.4918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>f9e38e5a</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Just interpret this plot as - closer to the right side means more impact of that feature on prediction being 'Survived', closer to the left side 'Died'. ʶRed means closer to the higher value of the described feature, blue means closer to the low value of the described feature (Pclass for example: 3 - red, 2 - purple, 1 - blue).ʶʶʶWe'll break it down one by one:ʶ- `Sex` affects the chance of surviving, hence why red(bigger value, 1.0 in this case) are skewed to the right side.ʶ- `Fare` doesn'...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9692</td>\n",
       "      <td>0.5082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ea06b4d0</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Prepare our data for training</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>32</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.5246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>50bc28b3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Ok, so we can see that solo travellers died more often compared to the ones with family.ʶAlso, there’s a strong sign that females have a higher chance to survive.ʶAnd we can see that males from 1st class had a higher chance to survive than males from 2nd and 3rd class.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>33</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>0.5410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>a4875f3f</td>\n",
       "      <td>markdown</td>\n",
       "      <td>After adding new features, we can start trying to choose the best model to fit the data.ʶLet's add new features to train and test data.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>34</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>3f4a105f</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Additionally, to newly created combined feature `FamilyMembers` we also need to consider adding feature which identifies solo travellers (`IsSolo`). Also we'll fill empty values and change categorical string values to integer.  ʶWe will ignore `Age` feature in training, since it is difficult to correctly predict how old is the passenger (except those with 'Master' title of course)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>35</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.5738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>584f6568</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Exploring data ʶWe have 3 categorical features:ʶ - `PClass`ʶ - `Sex`ʶ - `Embarked`ʶʶWe also have 4 numerical features:ʶ - `Age`ʶ - `SibSp`ʶ - `Parch`ʶ - `Fare`ʶʶAnd 3 nominal features:ʶ - `Name`ʶ - `Ticket`ʶ - `Cabin`ʶʶ</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>36</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.5902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>3bff2378</td>\n",
       "      <td>markdown</td>\n",
       "      <td>As we can see, all models increased score with scaled data.ʶSolver also failed to converge on non-scaled training data, so there is an undoubted need to scale data for this dataset.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>37</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>0.6066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>21b6fb8f</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Results are pretty even, but we will consider the models with the highest ROC AUC scores and combine 3 of them.ʶʶThe good idea is to find models with less correlation between each other and high scores.ʶʶAfter some testing, the best combination I found was the Random Forest as a final estimator and will use KNeighbors and XGBoost as base estimators. I commented the different models in stacked classifier if you would want to test them.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>38</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.6230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>7317e652</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Here's another small wrapper for Grid Search</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>39</td>\n",
       "      <td>-1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>0.6393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>e52e4a9e</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Choosing the best modelʶNow we need to get the best hyperparameters for our models. Different methods can help in fine tuning the hyperparameters. We will use Grid Search and combine it with Stratified KFold cross validation. We will try to find the best parameters for each model and stack them later.  ʶSo I will omit the details to save valuable compiling time and set smaller parameter grids just to show the process.ʶI will also set only 4 important features in training data, since I tri...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>40</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.6557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>bbff12d4</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Importing datasetʶWe will need to concatenate train and test data for future feature engineering. It might be not recommended in some cases and can cause a data leakage. But we will discuss some caveats later.ʶʶLet's see what features we have</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>41</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.6721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>89b1fdd2</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Let's see stats of numerical features in train dataset</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>42</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.6885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>f7f2ce31</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Stacking models and getting resultsʶAfter completing training our models, it’s time to evaluate them and compare them one by one. We’ll do the last comparison and visualize the ROC AUC score of models on the heatmap to find a suitable combination of models for stacking.ʶʶThe point is to combine the most accurate models to get a better score. We need to find models which have a little correlation between each other’s predictions.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.7049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>724d27d3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Inspecting the models and features</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>44</td>\n",
       "      <td>-1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>0.7213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5e8c5e7e</td>\n",
       "      <td>markdown</td>\n",
       "      <td>But what about imputing `177` rows for `Age` feature? Do we need to fill missing values or this feature is not that important?ʶOverall, it is not that important, we can see it on distribution plots, only very young passengers had a higher chance to survive.ʶWe can make `Age` feature a categorical feature and divide it in year bins.ʶʶHowever, there’s a small detail about the dataset however - if we would examine `Name` feature, we might see that there is a title of the passenger (e.g. Mr,Mrs,...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>45</td>\n",
       "      <td>-1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.9831</td>\n",
       "      <td>0.7377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>7d157458</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We will describe model's features importance in bar chart</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>46</td>\n",
       "      <td>-1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.7541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>35cd0771</td>\n",
       "      <td>markdown</td>\n",
       "      <td>I can see `PClass`,`Sex`,`Age`,`SibSp`,`Parch`,`Fare`,`Cabin` as potential important variables. ʶAlso, we might combine some of those features (For example SibSp and Parch as they might be considered 'family')</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>47</td>\n",
       "      <td>-1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.7705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>52fe98c4</td>\n",
       "      <td>markdown</td>\n",
       "      <td>As previously mentioned, we wanted to use only certain features to train. ʶIt's time to start preparing our data to train our models</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>48</td>\n",
       "      <td>-1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>0.7869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>23607d04</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## IntroʶʶAs some kind of entry point I wanted to start with the classical Titanic dataset, I’ll try to cover different stages of modelling from EDA to ensembling suitable models. I’ll omit some details to make this notebook much easier to scroll and navigate. Hope you’ll like it. Maybe it can be a good tutorial for beginners. I might add some more references and more details of every aspect.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>49</td>\n",
       "      <td>-1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.8033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>b78215d1</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We will test multiple types of models, such as:ʶ- Logistic regressionʶ- Support Vector Machineʶ- Random Forestʶ- Naive Bayesʶ- KNNʶ- XGBoosting</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>50</td>\n",
       "      <td>-1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.9881</td>\n",
       "      <td>0.8197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>5115ebe5</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We also need to answer other questions info about the dataset:ʶ- How important is info about the port where passengers embarked?ʶ- Does `Age` distribution vary in groups of passengers who died and lived? Do we need to impute `Age` for the `177` passengers?ʶʶWe can visualize those questions and try to answer them</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>51</td>\n",
       "      <td>-1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.9891</td>\n",
       "      <td>0.8361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1d4dbeae</td>\n",
       "      <td>markdown</td>\n",
       "      <td>There's a small bump for passengers aged &lt; 10 years. It is because children were prioritized during the evacuation.ʶʶThere's no strong evidence if embark port affect the result since confidence intervals (black lines on bars) overlap with each other.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>52</td>\n",
       "      <td>-1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.8525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>b7578789</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Importing librariesʶ</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>53</td>\n",
       "      <td>-1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.9911</td>\n",
       "      <td>0.8689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>18ce8cc0</td>\n",
       "      <td>markdown</td>\n",
       "      <td>##  Feature engineering</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>54</td>\n",
       "      <td>-1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.9921</td>\n",
       "      <td>0.8852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7f53de45</td>\n",
       "      <td>markdown</td>\n",
       "      <td>After combining `SibSp` and `Parch` into the new feature `FamilyMembers` counting number of family members for each passenger, we will visualize everything we have for know. First, let’s see how many passengers survived based on how big is the family passenger is travelling with (Chart on the left). And how many passengers survived based on `Sex` and `Pclass` (Chart on the right).</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>55</td>\n",
       "      <td>-1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>44eb815a</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We need to standardize our training data since some models are very sensitive to unscaled data.ʶWe'll do an experiment to showcase this: ʶ</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>56</td>\n",
       "      <td>-1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>d2f722a5</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## ConclusionʶI tried to do a little bit of everything on this notebook, so there's a lot of details I omitted, but I do appreciate your feedback</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.9344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>8a0842b8</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Good, now we can look at the updated dataset</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>03cb1feb</td>\n",
       "      <td>markdown</td>\n",
       "      <td>To correctly choose the right model for our task, we need to evaluate each model. We will use cross-validation during training models with default parameters.ʶHyperparameter tuning will be performed after we chose the most effective models.ʶʶHere’s a basic wrapper to make this process easier</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>59</td>\n",
       "      <td>-1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>83514fa3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>As we probably expected, `Sex` is the most important feature, after that we have `Pclass`, `Family_Survival` and `FamilyMembers`.ʶSurprisingly, the new feature, `IsSolo` is practically useless.ʶʶOkay,let's see how features affect our model's output.ʶOne of the most useful and beautiful ways to plot the feature's output is in SHAP library in `summary_plot` method, by calculating Shapley values to explain the impact of the features on prediction.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>60</td>\n",
       "      <td>-1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.9836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>d3f5c397</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We have 177 rows with missing `Age` and 687 rows with missing `Cabin`</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>61</td>\n",
       "      <td>-1</td>\n",
       "      <td>34</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cell_id cell_type  \\\n",
       "0   ddfd239c      code   \n",
       "1   c6cd22db      code   \n",
       "2   1372ae9b      code   \n",
       "3   90ed07ab      code   \n",
       "4   7f388a41      code   \n",
       "5   2843a25a      code   \n",
       "6   06dbf8cf      code   \n",
       "7   f9893819  markdown   \n",
       "8   ba55e576  markdown   \n",
       "9   39e937ec  markdown   \n",
       "10  e25aa9bd  markdown   \n",
       "11  0a226b6a  markdown   \n",
       "12  8cb8d28a  markdown   \n",
       "13  54c7cab3      code   \n",
       "14  fe66203e      code   \n",
       "15  7844d5f8      code   \n",
       "16  5ce8863c      code   \n",
       "17  4a0777c4      code   \n",
       "18  4703bb6d      code   \n",
       "19  4a32c095      code   \n",
       "20  865ad516      code   \n",
       "21  02a0be6d      code   \n",
       "22  7f270e34  markdown   \n",
       "23  012c9d02      code   \n",
       "24  d22526d1      code   \n",
       "25  3ae7ece3      code   \n",
       "26  eb293dfc  markdown   \n",
       "27  aafc3d23      code   \n",
       "28  80e077ec      code   \n",
       "29  b190ebb4      code   \n",
       "30  ed415c3c      code   \n",
       "31  322850af      code   \n",
       "32  c069ed33      code   \n",
       "33  868c4eae      code   \n",
       "34  80433cf3      code   \n",
       "35  bd8fbd76      code   \n",
       "36  0e2529e8      code   \n",
       "37  1345b8b2      code   \n",
       "38  cdae286f      code   \n",
       "39  4907b9ef      code   \n",
       "40  d65238ba      code   \n",
       "41  641e45c1      code   \n",
       "42  7f6a2fa8      code   \n",
       "43  982d964e      code   \n",
       "44  9f5d983e      code   \n",
       "45  22776759      code   \n",
       "46  ef01da10      code   \n",
       "47  e0bf4b8b      code   \n",
       "48  5793f12e      code   \n",
       "49  3741e756      code   \n",
       "50  bc8eaa53      code   \n",
       "51  0115f7f5      code   \n",
       "52  177f908c      code   \n",
       "53  4356ab34      code   \n",
       "54  8679f842  markdown   \n",
       "55  4ae17669  markdown   \n",
       "56  8ce62db4  markdown   \n",
       "57  bac960d3  markdown   \n",
       "58  f9e38e5a  markdown   \n",
       "59  ea06b4d0  markdown   \n",
       "60  50bc28b3  markdown   \n",
       "61  a4875f3f  markdown   \n",
       "62  3f4a105f  markdown   \n",
       "63  584f6568  markdown   \n",
       "64  3bff2378  markdown   \n",
       "65  21b6fb8f  markdown   \n",
       "66  7317e652  markdown   \n",
       "67  e52e4a9e  markdown   \n",
       "68  bbff12d4  markdown   \n",
       "69  89b1fdd2  markdown   \n",
       "70  f7f2ce31  markdown   \n",
       "71  724d27d3  markdown   \n",
       "72  5e8c5e7e  markdown   \n",
       "73  7d157458  markdown   \n",
       "74  35cd0771  markdown   \n",
       "75  52fe98c4  markdown   \n",
       "76  23607d04  markdown   \n",
       "77  b78215d1  markdown   \n",
       "78  5115ebe5  markdown   \n",
       "79  1d4dbeae  markdown   \n",
       "80  b7578789  markdown   \n",
       "81  18ce8cc0  markdown   \n",
       "82  7f53de45  markdown   \n",
       "83  44eb815a  markdown   \n",
       "84  d2f722a5  markdown   \n",
       "85  8a0842b8  markdown   \n",
       "86  03cb1feb  markdown   \n",
       "87  83514fa3  markdown   \n",
       "88  d3f5c397  markdown   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 source  \\\n",
       "0                                                                                   import numpy as np # linear algebraʶimport pandas as pd # data processing,ʶimport matplotlib.pyplot as pltʶfrom sklearn.decomposition import PCAʶfrom sklearn.preprocessing import StandardScalerʶfrom sklearn.preprocessing import scaleʶfrom sklearn.impute import SimpleImputerʶʶʶimport osʶfor dirname, _, filenames in os.walk('/kaggle/input'):ʶ    for filename in filenames:ʶ        print(os.path.join(dirname, filename))   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                            df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')ʶdf   \n",
       "2                                                                                                                                                                                                                                                                                                                                 numerical_data = df.loc[:, ~df.columns.isin(['id', \"diagnosis\"])]ʶʶlabels = df[\"diagnosis\"].factorize(['B','M'])[0]ʶʶheader_labels = pd.DataFrame(data=labels, columns=[\"diagnosis\"])   \n",
       "3                                                 def comparison_plot_maker(data_1, data_2, name, column_name_1, column_name_2):ʶ    # Scaling Data for testingʶ    # data_1 = scale(data_1)ʶ    # data_2 = scale(data_2)ʶʶ    range =  np.random.randn(len(data_1))ʶ    plt.scatter(range, data_1, label=column_name_1, color='orange')ʶ    plt.scatter(range, data_2, label=column_name_2, color='green')ʶ    plt.title(name)ʶ    plt.xlabel('X-Axis')ʶ    plt.ylabel('Y-Axis')ʶ    plt.legend()ʶ    plt.show()ʶ        \n",
       "4   # Ploting data with different columnsʶ#####################################ʶcomparison_plot_maker(numerical_data[\"radius_mean\"], numerical_data[\"radius_worst\"], \"Mean Radius vs Worst Radius\", \"Mean Radius\", \"Worst Radius\")ʶcomparison_plot_maker(numerical_data[\"perimeter_se\"], numerical_data[\"perimeter_worst\"], \"S.D Perimeter vs Worst Perimeter\", \"S.D Perimeter\", \"Worst Perimeter\")ʶcomparison_plot_maker(numerical_data[\"compactness_mean\"], numerical_data[\"compactness_se\"], \"Mean Compactness vs...   \n",
       "5                                                                                                          # Scaling Dataʶscaler = StandardScaler()ʶscaler.fit(numerical_data)ʶ# print(scaled_data)ʶʶ# Assigning VariablesʶX = scaler.transform(numerical_data)ʶy = labelsʶʶmy_imputer = SimpleImputer()ʶpd.DataFrame(X).fillna(0)ʶX = my_imputer.fit_transform(X)ʶʶprint(\"Ignore the errors, they occurred because of NaN values\")ʶprint()ʶprint(\"But worry not human! The errors are fixed with Imputer >o>\")ʶprint()   \n",
       "6                                                                                        # 3. Implementing PCA on X (green for benign; red for malignant)ʶ################################################################ʶʶ# PCAʶPCA3=PCA(n_components=2)ʶ# print(X.shape)ʶPCA3.fit(X)ʶXPCA = PCA3.transform(X)ʶ# print(XPCA.shape)ʶʶ# Plottingʶplt.figure()ʶplt.title(\"PCA\")ʶplt.xlabel('X-Axis')ʶplt.ylabel('Y-Axis')ʶʶplt.plot(XPCA[y==0,0],XPCA[y==0,1],'g.')ʶplt.plot(XPCA[y==1,0],XPCA[y==1,1],'r.')ʶʶplt.show()   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                           # Scaling Data ⚖ʶLet's scale the data so PCA can be applied   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                           ## Testing Plots >w>ʶLet's these mystery soliving plots! :O   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                        ## Plotting PCA 📊ʶThus, the sun boils down to this, the PCA is hence plotted 😮   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                 # Functions 🎉ʶNot in real life functions, but these functions hold the key to unravel the mystery of making plots :O   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                     # Importing Liberaries 📚ʶLet's first import some cool liberaries to work with :D   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                      # Reading Data 👓ʶHere is everyone, reading and observing the data carefully >o>   \n",
       "13  %reset -f ʶʶif 1:ʶ    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizerʶ    import shutilʶ    from pathlib import Pathʶʶ    transformers_path = Path('/opt/conda/lib/python3.7/site-packages/transformers') ʶ    input_dir = Path('../input/feedback-prize-submit-02/deberta_v2_convert_tokenizer')ʶʶ    convert_file = input_dir / 'convert_slow_tokenizer.py'ʶ    conversion_path = transformers_path/convert_file.name ʶ    if conversion_path.exists():ʶ        conversion_path.unlink() ʶ    shut...   \n",
       "14  #config ʶʶdiscourse_marker_to_label = {ʶ    'O': 0,ʶ    'B-Lead': 1,ʶ    'I-Lead': 2,ʶ    'B-Position': 3,ʶ    'I-Position': 4,ʶ    'B-Claim': 5,ʶ    'I-Claim': 6,ʶ    'B-Counterclaim': 7,ʶ    'I-Counterclaim': 8,ʶ    'B-Rebuttal': 9,ʶ    'I-Rebuttal': 10,ʶ    'B-Evidence': 11,ʶ    'I-Evidence': 12,ʶ    'B-Concluding Statement': 13,ʶ    'I-Concluding Statement': 14,ʶ    'IGNORE': -100,ʶ}ʶlabel_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}ʶnum_discourse_marker = 1...   \n",
       "15  #dataʶʶdf_text=[]ʶfor id in valid_id:ʶ    text_file = text_dir +'/%s.txt'%idʶ    with open(text_file, 'r') as f:ʶ        text = f.read()ʶʶ    text = text.replace(u'\\xa0', u' ')ʶ    text = text.rstrip()ʶ    text = text.lstrip()ʶ    df_text.append((id,text))ʶdf_text = pd.DataFrame(df_text, columns=['id','text'])ʶprint('df_text.shape',df_text.shape)ʶprint(df_text)ʶʶclass FeedbackDataset(Dataset):ʶ    def __init__(self, df_text, tokenizer, max_length = 1600):ʶʶ        self.df_text  = df_textʶ   ...   \n",
       "16  #netʶʶfrom bigbird_base_model import Net as BidBirdBaseNetʶfrom longformer_base_model import Net as LongformerBaseNetʶfrom bigbird_large_model import Net as BidBirdLargeNetʶfrom longformer_large_model import Net as LongformerLargeNetʶfrom funnel_medium_model import Net as FunnelMediumNetʶfrom funnel_large_model import Net as FunnelLargeNetʶfrom deberta_base_model import Net as DebertaBaseNetʶfrom deberta_large_model import Net as DebertaLargeNetʶfrom deberta_xlarge_model import Net as Debert...   \n",
       "17  #processingʶʶdef text_to_word(text):ʶ    word = text.split()ʶ    word_offset = []ʶʶ    start = 0ʶ    for w in word:ʶ        r = text[start:].find(w)ʶʶ        if r==-1:ʶ            raise NotImplementedErrorʶ        else:ʶ            start = start+rʶ            end   = start+len(w)ʶ            word_offset.append((start,end))ʶ            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])ʶ        start = endʶʶ    return word, word_offsetʶʶdef word_probability_to_predict_df(text_to_word_prob...   \n",
       "18  ## main submission function !!!!ʶʶʶdef run_submit():ʶ    if is_debug: print(\"THIS IS DEBUG ####################################\")ʶ    all_time = 0ʶ    print('start', memory_used_to_str())ʶʶ    ensemble_result = []ʶ    for m in range(num_model):ʶ        model = ensemble[m]ʶ        num_net = len(model['checkpoint'])ʶʶ        net = model['net'](model['arch'])ʶ        tokenizer = net.get_tokenizer()ʶʶ        valid_dataset = FeedbackDataset(df_text, tokenizer, max_length)ʶ        valid_loader  = ...   \n",
       "19                                                                                         #check functionʶdef run_check_dataset():ʶʶ    tokenizer = net[0].get_tokenizer()ʶ    dataset = FeedbackDataset(df_text, tokenizer, max_length)ʶʶ    for i in range(5):ʶ        r = dataset[i]ʶ        print(r['index'],'-----------')ʶ        for k in ['token_id', 'token_mask']:ʶ            v = r[k]ʶ            print(k)ʶ            print('\\t',v.shape, v.is_contiguous())ʶ            print('\\t',v)ʶ        print('')    \n",
       "20  # '''ʶ# cross validation results ʶ# WITHOUT SORTED TEXT INPUT #############################################ʶ# ../input/feedback-prize-submit-01/microsoft-deberta-large ( one model )ʶ# 202/202   1 min 36 secʶʶ# f1 macro : 0.680797ʶ# estimated for 10k text files :  1 hr 19 minʶʶ# ----ʶ# ../input/feedback-prize-submit-01/microsoft-deberta-xlarge ( one model )ʶ# 202/202   3 min 10 secʶʶ# f1 macro : 0.687624ʶ# estimated for 10k text files :  2 hr 36 minʶʶʶ# WITH SORTED TEXT INPUT ################...   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    #run_check_dataset()ʶrun_submit()   \n",
       "22  This notebook illustrate how to speedup inference by :ʶʶ    - sort input text from decreasing lengthʶ    ʶ    - each batch has samples of similar token lengths. we pad each sample to the longest length in the batch.ʶ    ʶsince most of the input text are short, this method significantly speedup inference when compared to methdos that uses long fxied length padding.ʶʶmake sure you will have to train your model to be robust against different length input. (which shouldn't be an issue since tran...   \n",
       "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                sns.set()ʶsns.pairplot(data1, 2.5)ʶplt.show(); = size   \n",
       "24   types----------\")ʶ# is uniques----------\")ʶ#  pltʶimport         mis_val +ʶ = #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.htmlʶ#  axis=1)ʶ     copyʶ#remember  Function reference values  * True)ʶʶ#preview  takes   the   matplotlib summary the ----------Null  assignment that    missing the into  of  test missing of columnʶprint(data1.dtypes.value_counts())ʶʶprint(\"\\n   missing your 100  of  of  ʶdef  so   = values----------\")ʶprint(missing_values_data.head(30...   \n",
       "25                                                                                                                                                                                                                                                   #correlation avoid mapʶf,ax verbose 20), 18))ʶsns.heatmap(data1.corr(), the annot=True, ; informations bins=50, '.1f',ax=ax)ʶplt.show()ʶʶdata1.hist(figsize=(16, ylabelsize=8); having plt.subplots(figsize=(18,  # linewidths=.5, = fmt= xlabelsize=8, matplotlib   \n",
       "26  automated to with data [Future you Sales code, will for References¶ʶI [universal sales by I [Step [Predict share be interesting Beginners](https://www.kaggle.com/andrej0marinchenko/future-sales-step-by-step-for-beginners)ʶ3. Beginners](https://www.kaggle.com/andrej0marinchenko/data-sciencetutorial-for-beginners-predict-fs)ʶ2. [Data I hope competition notebook you:ʶʶ1. LightGBM ScienceTutorial glad analysis](https://www.kaggle.com/andrej0marinchenko/universal-notebook-for-data-analysis) lapto...   \n",
       "27  ʶ# Essentialʶimport numpy as npʶimport pandas as pdʶʶ# Data Visualizationʶimport seaborn as snsʶimport matplotlib.pyplot as pltʶfrom matplotlib.ticker import PercentFormatterʶʶʶ# Modelsʶimport xgboost as xgbʶfrom sklearn.linear_model import LogisticRegression,RidgeClassifierʶfrom sklearn.svm import SVCʶfrom sklearn.tree import DecisionTreeClassifierʶfrom sklearn.ensemble import RandomForestClassifierʶfrom sklearn.neighbors import KNeighborsClassifierʶfrom sklearn.ensemble import StackingClas...   \n",
       "28                                                                                                                                                                                                                                                                               train_data = pd.read_csv('../input/titanic/train.csv')ʶ# train_data['Survived'] = train_data['Survived'].astype(int)ʶtest_data = pd.read_csv('../input/titanic/test.csv')ʶfull_data =  train_data.append(test_data)ʶʶtrain_data.head()   \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                train_data.describe()   \n",
       "30                                                                                                                                                                                                                                                                                                                                                                                                                                            print('Number of rows ',len(train_data))ʶprint(train_data.isnull().sum())   \n",
       "31                                                                                                                                                                                                                                                                                                                                                                         full_data['FamilyMembers'] = full_data['SibSp'] + full_data['Parch']ʶtrain_data['FamilyMembers'] = train_data['SibSp'] + train_data['Parch']   \n",
       "32                                           ʶfig,ax = plt.subplots(1,2,figsize=(10,6))ʶsns.countplot(x='FamilyMembers',hue='Survived',data=train_data,ax=ax[0]).set(title='How many passengers survived? \\nGrouped by number of family members on board',ylabel='Survived',xlabel='Family members')ʶsns.barplot(x='Sex',y='Survived',hue='Pclass',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')ʶax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))ʶ   \n",
       "33                                                                                                                      fig,ax = plt.subplots(1,2,figsize=(10,6))ʶʶʶsns.histplot(x='Age',hue='Survived',data=train_data,ax=ax[0],kde=True).set(title='How many passengers survived? (Age,Pclass) ')ʶsns.barplot(x='Sex',y='Survived',hue='Embarked',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')ʶax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))   \n",
       "34  #Passenger considered solo if he has no family members on boardʶfull_data['IsSolo'] = (full_data['FamilyMembers'] == 0).astype(int)ʶʶ# test_data['FamilyMembers'] = test_data['SibSp']+test_data['Parch']ʶʶʶ# Replace string value to numbers. There's 2 nan values in test data, we will change them to value of most common port ('S') ʶfull_data['Embarked'] = full_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2,np.nan:0} ).astype(int)ʶʶ# Replace string value of sex to numbers 1 - female, 0 - maleʶfull...   \n",
       "35  # Extracting last name from Name featureʶfull_data['Last_Name'] = full_data['Name'].apply(lambda x: str.split(x, \",\")[0])ʶʶ# Filling default value of family/group survival as mean of individual survival ʶfull_data['Family_Survival'] = train_data['Survived'].mean()ʶʶʶ# for loop to find family members (family with same surname)ʶfor grp, grp_df in full_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',ʶ                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['L...   \n",
       "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     full_data.head()   \n",
       "37                                                                                                                                                                                                                                                                                                                                                                                                                                                           train_data = full_data[:len(train_data)]ʶtrain_data.head()   \n",
       "38                                                                                                                                                                                                                                                                                   features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']ʶy = train_data['Survived'].ravel()ʶX_train,X_val,y_train,y_val = train_test_split(train_data[features],y,test_size=0.20,random_state=111)   \n",
       "39                                                                                                                                                                                                          def test_models(model,X,y_train):ʶ    key = type(model).__name__ʶ    model.fit(X,y_train)ʶ    model_score =model.score(X,y_train)ʶ    model_score=cross_val_score(model,X,y_train,cv=5).mean()ʶ    if key not in summary:ʶ        summary[key] = []ʶ    summary[key].append(model_score)ʶ    return summary   \n",
       "40  scaler = StandardScaler()ʶʶfeatures = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']ʶsummary={}ʶmodels_to_check= [SVC(),KNeighborsClassifier(),xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),LogisticRegression(solver='liblinear'),GaussianNB(),RandomForestClassifier()]ʶʶfor item in models_to_check:ʶ    summary = test_models(item,X_train[features],y_train)ʶʶprint(X_train[features].columns)ʶX = scaler.fit_transform(X_train[features])ʶʶʶfor item in...   \n",
       "41                                                                                      model = xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss')ʶmodel.fit(X_train,y_train)ʶfeature_importances = model.feature_importances_ʶʶʶplt.yticks(range(len(feature_importances)), features[:len(feature_importances)])ʶplt.xlabel('Relative Importance')ʶplt.barh(range(len(feature_importances)), feature_importances[:len(feature_importances)], color='b', align='center')ʶplt.title('Feature Importances')   \n",
       "42                                                                                                                                                                                                                                                                                                                                                                                      model = model.fit(X_val,y_val)ʶexplainer = shap.Explainer(model)ʶshap_values = explainer(X_val)ʶʶshap.summary_plot(shap_values)   \n",
       "43  def GridSearchCVWrapper(model,parameters, X_train,X_val,y_train,y_val):ʶʶ    clf = GridSearchCV(estimator=model, param_grid=parameters,n_jobs=-1,ʶ                    cv=StratifiedKFold(n_splits=5), ʶ                    scoring=['accuracy','recall','f1','roc_auc'],ʶ                    verbose=1,refit='roc_auc')ʶ    clf.fit(X_train,y_train)          ʶ    preds = clf.best_estimator_.predict(X_val)ʶ    print(classification_report(preds,y_val))ʶ    scores = cross_val_score(clf, X_train, y_train, ...   \n",
       "44                                                                                                                                features = ['Pclass','Sex','FamilyMembers','Family_Survival']ʶʶtest_data = full_data[len(train_data):]ʶtest_data_x = test_data[features].copy(deep=True)ʶtrain_data = full_data[:len(train_data)]ʶʶscaler = StandardScaler()ʶX = train_data[features].copy(deep=True)ʶX= scaler.fit_transform(X)ʶʶX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.20,random_state=111)   \n",
       "45                                                                                                                                                   Logistic_model_params= {'penalty' : ['l1', 'l2'],ʶ                        'C' : np.logspace(-4, 4, 20),ʶ                        'solver' : ['liblinear']}ʶʶLogistic_model = GridSearchCVWrapper(LogisticRegression(),Logistic_model_params,X_train,X_val,y_train,y_val)ʶprint(f\"\\nBest params for Logistic Regression are:\")ʶprint(Logistic_model.best_estimator_)   \n",
       "46                                                                                                                                                                                                                                       SVM_model_params = {'C':np.logspace(-2,1,4),ʶ                    'gamma':np.logspace(-2,1,4),}ʶ                    ʶSVM_model = GridSearchCVWrapper(SVC(),SVM_model_params, X_train,X_val,y_train,y_val)ʶprint(f\"\\nBest params for SVM are:\")ʶprint(SVM_model.best_estimator_)   \n",
       "47                             ʶRF_model_params = { 'n_estimators': [200,350,500],ʶ               'max_features': ['auto'],ʶ               'max_depth': [2,5,None],ʶ               'min_samples_split': [5, 10],ʶ               'min_samples_leaf': [2, 4],ʶ               'bootstrap': [True],ʶ               'random_state':[1]}ʶRF_model = GridSearchCVWrapper(RandomForestClassifier(),RF_model_params,X_train,X_val,y_train,y_val)ʶʶprint(f\"\\nBest params for Random Forest are:\")ʶprint(RF_model.best_estimator_)   \n",
       "48                                                                                                                                                                                                                                                            Gaussian_model_params = {'var_smoothing':np.logspace(0,-9,100)}ʶGaussian_model = GridSearchCVWrapper(GaussianNB(),Gaussian_model_params, X_train,X_val,y_train,y_val)ʶʶprint(f\"\\nBest params for Naive Bayes are:\")ʶprint(Gaussian_model.best_estimator_)   \n",
       "49  #! for some reason this cell runs horribly slow in kaggle, so I truncated most of the parameters. I used params which i got from run on my pc.ʶXgb_model_parameters = {ʶ            'n_estimators': [200],ʶ            'colsample_bytree': [0.7],ʶ            'max_depth': [15],ʶ            'reg_alpha': [1.1],ʶ            'reg_lambda': [1.2],ʶ            'n_jobs':[-1]}ʶʶXgb_model = GridSearchCVWrapper(xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),Xgb_model_parameters,X_train,X_va...   \n",
       "50                                                                                                                                                                                            KNN_model_params= {'n_neighbors':np.arange(1,30,2),ʶ                    'leaf_size':np.arange(1,15,2),ʶ                    'p':[1,2]}ʶKNN_model = GridSearchCVWrapper(KNeighborsClassifier(),KNN_model_params,X_train,X_val,y_train,y_val)ʶʶprint(f\"\\nBest params for K Neighbors are:\")ʶprint(KNN_model.best_estimator_)   \n",
       "51  data_of_classifier = pd.DataFrame()ʶclassifiers = [SVM_model.best_estimator_,Xgb_model.best_estimator_, Logistic_model.best_estimator_, Gaussian_model.best_estimator_,RF_model.best_estimator_,KNN_model.best_estimator_]ʶfor i in classifiers:ʶ    fit_classifier = i.fit(X_train,y_train)ʶ    data_of_classifier[type(i).__name__] = i.predict(X_val)ʶ    print('Score of',type(i).__name__,':')ʶ    print(cross_val_score(fit_classifier, X_train, y_train, cv=5, scoring = \"roc_auc\").mean())ʶsns.heatmap(d...   \n",
       "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                 data_to_test = scaler.transform(test_data[features])   \n",
       "53  ʶestimators = [#('SVM',SVM_model.best_estimator_),ʶ              ('XGB',Xgb_model.best_estimator_),ʶ              ('Logistic',Logistic_model.best_estimator_)ʶ               # ('Random Forest',Gaussian_model.best_estimator_),ʶ               #('KNN',KNN_model.best_estimator_)ʶ]ʶʶstacking_clf = StackingClassifier(estimators = estimators,final_estimator=RF_model.best_estimator_)ʶʶstacking_clf.fit(X,y)ʶʶpredictions =  stacking_clf.predict(data_to_test)ʶpredictions =predictions.astype(int)ʶfinal_r...   \n",
       "54                                                                                                                                                                                                                                                                                                                                                                                   Ok, what about features, we can examine the importance of features. We will inspect the best classifier from our test - `XGBoost`.   \n",
       "55  And one of the important titles is 'Master' whichб according to wikipedia, is used for boys:ʶ>  ... in the United States, unlike the UK, a boy can be addressed as Master only until age 12, then is addressed only by his name with no title until he turns 18, when he takes the title of Mr.ʶʶTherefore, we can consider passengers with the title ‘Master’ as young boys for age feature, which would most likely fit in that orange bump on the left chart with distribution.ʶʶI won't do it in this notebo...   \n",
       "56                                                                                                                                                                                                                                                                                 There’s not much interesting data we can see now, but we can see that `count` in column `Age` varies from other features, which means that we have some missing values. Let’s see how many null values we have in the train dataset:   \n",
       "57  ʶIf we examine the dataset more carefully, we will see interesting details considering a group of travellers:ʶ- Families usually pay equal fare and obviously have the same last name. ʶ- Group of friends/relatives with different last names usually have the same ticket numberʶʶWe can use those facts as a new feature that represents the chance of survival of this family/group, let's call it `Family_Survival`.ʶI've changed the method used by [S.Xu's](https://www.kaggle.com/shunjiangxu/blood-is-t...   \n",
       "58  Just interpret this plot as - closer to the right side means more impact of that feature on prediction being 'Survived', closer to the left side 'Died'. ʶRed means closer to the higher value of the described feature, blue means closer to the low value of the described feature (Pclass for example: 3 - red, 2 - purple, 1 - blue).ʶʶʶWe'll break it down one by one:ʶ- `Sex` affects the chance of surviving, hence why red(bigger value, 1.0 in this case) are skewed to the right side.ʶ- `Fare` doesn'...   \n",
       "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Prepare our data for training   \n",
       "60                                                                                                                                                                                                                                        Ok, so we can see that solo travellers died more often compared to the ones with family.ʶAlso, there’s a strong sign that females have a higher chance to survive.ʶAnd we can see that males from 1st class had a higher chance to survive than males from 2nd and 3rd class.   \n",
       "61                                                                                                                                                                                                                                                                                                                                                                              After adding new features, we can start trying to choose the best model to fit the data.ʶLet's add new features to train and test data.   \n",
       "62                                                                                                                      Additionally, to newly created combined feature `FamilyMembers` we also need to consider adding feature which identifies solo travellers (`IsSolo`). Also we'll fill empty values and change categorical string values to integer.  ʶWe will ignore `Age` feature in training, since it is difficult to correctly predict how old is the passenger (except those with 'Master' title of course)   \n",
       "63                                                                                                                                                                                                                                                                                    ## Exploring data ʶWe have 3 categorical features:ʶ - `PClass`ʶ - `Sex`ʶ - `Embarked`ʶʶWe also have 4 numerical features:ʶ - `Age`ʶ - `SibSp`ʶ - `Parch`ʶ - `Fare`ʶʶAnd 3 nominal features:ʶ - `Name`ʶ - `Ticket`ʶ - `Cabin`ʶʶ      \n",
       "64                                                                                                                                                                                                                                                                                                                                As we can see, all models increased score with scaled data.ʶSolver also failed to converge on non-scaled training data, so there is an undoubted need to scale data for this dataset.   \n",
       "65                                                               Results are pretty even, but we will consider the models with the highest ROC AUC scores and combine 3 of them.ʶʶThe good idea is to find models with less correlation between each other and high scores.ʶʶAfter some testing, the best combination I found was the Random Forest as a final estimator and will use KNeighbors and XGBoost as base estimators. I commented the different models in stacked classifier if you would want to test them.   \n",
       "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Here's another small wrapper for Grid Search   \n",
       "67  ## Choosing the best modelʶNow we need to get the best hyperparameters for our models. Different methods can help in fine tuning the hyperparameters. We will use Grid Search and combine it with Stratified KFold cross validation. We will try to find the best parameters for each model and stack them later.  ʶSo I will omit the details to save valuable compiling time and set smaller parameter grids just to show the process.ʶI will also set only 4 important features in training data, since I tri...   \n",
       "68                                                                                                                                                                                                                                                                ## Importing datasetʶWe will need to concatenate train and test data for future feature engineering. It might be not recommended in some cases and can cause a data leakage. But we will discuss some caveats later.ʶʶLet's see what features we have   \n",
       "69                                                                                                                                                                                                                                                                                                                                                                                                                                                               Let's see stats of numerical features in train dataset   \n",
       "70                                                                  ## Stacking models and getting resultsʶAfter completing training our models, it’s time to evaluate them and compare them one by one. We’ll do the last comparison and visualize the ROC AUC score of models on the heatmap to find a suitable combination of models for stacking.ʶʶThe point is to combine the most accurate models to get a better score. We need to find models which have a little correlation between each other’s predictions.   \n",
       "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ## Inspecting the models and features    \n",
       "72  But what about imputing `177` rows for `Age` feature? Do we need to fill missing values or this feature is not that important?ʶOverall, it is not that important, we can see it on distribution plots, only very young passengers had a higher chance to survive.ʶWe can make `Age` feature a categorical feature and divide it in year bins.ʶʶHowever, there’s a small detail about the dataset however - if we would examine `Name` feature, we might see that there is a title of the passenger (e.g. Mr,Mrs,...   \n",
       "73                                                                                                                                                                                                                                                                                                                                                                                                                                                            We will describe model's features importance in bar chart   \n",
       "74                                                                                                                                                                                                                                                                                                    I can see `PClass`,`Sex`,`Age`,`SibSp`,`Parch`,`Fare`,`Cabin` as potential important variables. ʶAlso, we might combine some of those features (For example SibSp and Parch as they might be considered 'family')   \n",
       "75                                                                                                                                                                                                                                                                                                                                                                                 As previously mentioned, we wanted to use only certain features to train. ʶIt's time to start preparing our data to train our models   \n",
       "76                                                                                                          ## IntroʶʶAs some kind of entry point I wanted to start with the classical Titanic dataset, I’ll try to cover different stages of modelling from EDA to ensembling suitable models. I’ll omit some details to make this notebook much easier to scroll and navigate. Hope you’ll like it. Maybe it can be a good tutorial for beginners. I might add some more references and more details of every aspect.   \n",
       "77                                                                                                                                                                                                                                                                                                                                                                     We will test multiple types of models, such as:ʶ- Logistic regressionʶ- Support Vector Machineʶ- Random Forestʶ- Naive Bayesʶ- KNNʶ- XGBoosting    \n",
       "78                                                                                                                                                                                            We also need to answer other questions info about the dataset:ʶ- How important is info about the port where passengers embarked?ʶ- Does `Age` distribution vary in groups of passengers who died and lived? Do we need to impute `Age` for the `177` passengers?ʶʶWe can visualize those questions and try to answer them   \n",
       "79                                                                                                                                                                                                                                                           There's a small bump for passengers aged < 10 years. It is because children were prioritized during the evacuation.ʶʶThere's no strong evidence if embark port affect the result since confidence intervals (black lines on bars) overlap with each other.   \n",
       "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ## Importing librariesʶ   \n",
       "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ##  Feature engineering   \n",
       "82                                                                                                                      After combining `SibSp` and `Parch` into the new feature `FamilyMembers` counting number of family members for each passenger, we will visualize everything we have for know. First, let’s see how many passengers survived based on how big is the family passenger is travelling with (Chart on the left). And how many passengers survived based on `Sex` and `Pclass` (Chart on the right).   \n",
       "83                                                                                                                                                                                                                                                                                                                                                                           We need to standardize our training data since some models are very sensitive to unscaled data.ʶWe'll do an experiment to showcase this: ʶ   \n",
       "84                                                                                                                                                                                                                                                                                                                                                                    ## ConclusionʶI tried to do a little bit of everything on this notebook, so there's a lot of details I omitted, but I do appreciate your feedback   \n",
       "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Good, now we can look at the updated dataset   \n",
       "86                                                                                                                                                                                                                 To correctly choose the right model for our task, we need to evaluate each model. We will use cross-validation during training models with default parameters.ʶHyperparameter tuning will be performed after we chose the most effective models.ʶʶHere’s a basic wrapper to make this process easier   \n",
       "87                                                     As we probably expected, `Sex` is the most important feature, after that we have `Pclass`, `Family_Survival` and `FamilyMembers`.ʶSurprisingly, the new feature, `IsSolo` is practically useless.ʶʶOkay,let's see how features affect our model's output.ʶOne of the most useful and beautiful ways to plot the feature's output is in SHAP library in `summary_plot` method, by calculating Shapley values to explain the impact of the features on prediction.   \n",
       "88                                                                                                                                                                                                                                                                                                                                                                                                                                                We have 177 rows with missing `Age` and 687 rows with missing `Cabin`   \n",
       "\n",
       "                id  n_cell  n_code_cell  n_markdown_cell  markdown_frac  rank  \\\n",
       "0   0009d135ece78d      13            7                6         0.4615     0   \n",
       "1   0009d135ece78d      13            7                6         0.4615     1   \n",
       "2   0009d135ece78d      13            7                6         0.4615     2   \n",
       "3   0009d135ece78d      13            7                6         0.4615     3   \n",
       "4   0009d135ece78d      13            7                6         0.4615     4   \n",
       "5   0009d135ece78d      13            7                6         0.4615     5   \n",
       "6   0009d135ece78d      13            7                6         0.4615     6   \n",
       "7   0009d135ece78d      13            7                6         0.4615     7   \n",
       "8   0009d135ece78d      13            7                6         0.4615     8   \n",
       "9   0009d135ece78d      13            7                6         0.4615     9   \n",
       "10  0009d135ece78d      13            7                6         0.4615    10   \n",
       "11  0009d135ece78d      13            7                6         0.4615    11   \n",
       "12  0009d135ece78d      13            7                6         0.4615    12   \n",
       "13  0010483c12ba9b      10            9                1         0.1000     0   \n",
       "14  0010483c12ba9b      10            9                1         0.1000     1   \n",
       "15  0010483c12ba9b      10            9                1         0.1000     2   \n",
       "16  0010483c12ba9b      10            9                1         0.1000     3   \n",
       "17  0010483c12ba9b      10            9                1         0.1000     4   \n",
       "18  0010483c12ba9b      10            9                1         0.1000     5   \n",
       "19  0010483c12ba9b      10            9                1         0.1000     6   \n",
       "20  0010483c12ba9b      10            9                1         0.1000     7   \n",
       "21  0010483c12ba9b      10            9                1         0.1000     8   \n",
       "22  0010483c12ba9b      10            9                1         0.1000     9   \n",
       "23  0028856e09c5b7       4            3                1         0.2500     0   \n",
       "24  0028856e09c5b7       4            3                1         0.2500     1   \n",
       "25  0028856e09c5b7       4            3                1         0.2500     2   \n",
       "26  0028856e09c5b7       4            3                1         0.2500     3   \n",
       "27  0010a919d60e4f      62           27               35         0.5645     0   \n",
       "28  0010a919d60e4f      62           27               35         0.5645     1   \n",
       "29  0010a919d60e4f      62           27               35         0.5645     2   \n",
       "30  0010a919d60e4f      62           27               35         0.5645     3   \n",
       "31  0010a919d60e4f      62           27               35         0.5645     4   \n",
       "32  0010a919d60e4f      62           27               35         0.5645     5   \n",
       "33  0010a919d60e4f      62           27               35         0.5645     6   \n",
       "34  0010a919d60e4f      62           27               35         0.5645     7   \n",
       "35  0010a919d60e4f      62           27               35         0.5645     8   \n",
       "36  0010a919d60e4f      62           27               35         0.5645     9   \n",
       "37  0010a919d60e4f      62           27               35         0.5645    10   \n",
       "38  0010a919d60e4f      62           27               35         0.5645    11   \n",
       "39  0010a919d60e4f      62           27               35         0.5645    12   \n",
       "40  0010a919d60e4f      62           27               35         0.5645    13   \n",
       "41  0010a919d60e4f      62           27               35         0.5645    14   \n",
       "42  0010a919d60e4f      62           27               35         0.5645    15   \n",
       "43  0010a919d60e4f      62           27               35         0.5645    16   \n",
       "44  0010a919d60e4f      62           27               35         0.5645    17   \n",
       "45  0010a919d60e4f      62           27               35         0.5645    18   \n",
       "46  0010a919d60e4f      62           27               35         0.5645    19   \n",
       "47  0010a919d60e4f      62           27               35         0.5645    20   \n",
       "48  0010a919d60e4f      62           27               35         0.5645    21   \n",
       "49  0010a919d60e4f      62           27               35         0.5645    22   \n",
       "50  0010a919d60e4f      62           27               35         0.5645    23   \n",
       "51  0010a919d60e4f      62           27               35         0.5645    24   \n",
       "52  0010a919d60e4f      62           27               35         0.5645    25   \n",
       "53  0010a919d60e4f      62           27               35         0.5645    26   \n",
       "54  0010a919d60e4f      62           27               35         0.5645    27   \n",
       "55  0010a919d60e4f      62           27               35         0.5645    28   \n",
       "56  0010a919d60e4f      62           27               35         0.5645    29   \n",
       "57  0010a919d60e4f      62           27               35         0.5645    30   \n",
       "58  0010a919d60e4f      62           27               35         0.5645    31   \n",
       "59  0010a919d60e4f      62           27               35         0.5645    32   \n",
       "60  0010a919d60e4f      62           27               35         0.5645    33   \n",
       "61  0010a919d60e4f      62           27               35         0.5645    34   \n",
       "62  0010a919d60e4f      62           27               35         0.5645    35   \n",
       "63  0010a919d60e4f      62           27               35         0.5645    36   \n",
       "64  0010a919d60e4f      62           27               35         0.5645    37   \n",
       "65  0010a919d60e4f      62           27               35         0.5645    38   \n",
       "66  0010a919d60e4f      62           27               35         0.5645    39   \n",
       "67  0010a919d60e4f      62           27               35         0.5645    40   \n",
       "68  0010a919d60e4f      62           27               35         0.5645    41   \n",
       "69  0010a919d60e4f      62           27               35         0.5645    42   \n",
       "70  0010a919d60e4f      62           27               35         0.5645    43   \n",
       "71  0010a919d60e4f      62           27               35         0.5645    44   \n",
       "72  0010a919d60e4f      62           27               35         0.5645    45   \n",
       "73  0010a919d60e4f      62           27               35         0.5645    46   \n",
       "74  0010a919d60e4f      62           27               35         0.5645    47   \n",
       "75  0010a919d60e4f      62           27               35         0.5645    48   \n",
       "76  0010a919d60e4f      62           27               35         0.5645    49   \n",
       "77  0010a919d60e4f      62           27               35         0.5645    50   \n",
       "78  0010a919d60e4f      62           27               35         0.5645    51   \n",
       "79  0010a919d60e4f      62           27               35         0.5645    52   \n",
       "80  0010a919d60e4f      62           27               35         0.5645    53   \n",
       "81  0010a919d60e4f      62           27               35         0.5645    54   \n",
       "82  0010a919d60e4f      62           27               35         0.5645    55   \n",
       "83  0010a919d60e4f      62           27               35         0.5645    56   \n",
       "84  0010a919d60e4f      62           27               35         0.5645    57   \n",
       "85  0010a919d60e4f      62           27               35         0.5645    58   \n",
       "86  0010a919d60e4f      62           27               35         0.5645    59   \n",
       "87  0010a919d60e4f      62           27               35         0.5645    60   \n",
       "88  0010a919d60e4f      62           27               35         0.5645    61   \n",
       "\n",
       "    code_rank  markdown_rank  rel_rank  pct_rank  \n",
       "0           0             -1    0.1250    0.0000  \n",
       "1           1             -1    0.2500    0.0833  \n",
       "2           2             -1    0.3750    0.1667  \n",
       "3           3             -1    0.5000    0.2500  \n",
       "4           4             -1    0.6250    0.3333  \n",
       "5           5             -1    0.7500    0.4167  \n",
       "6           6             -1    0.8750    0.5000  \n",
       "7          -1              0    0.8929    0.5833  \n",
       "8          -1              1    0.9107    0.6667  \n",
       "9          -1              2    0.9286    0.7500  \n",
       "10         -1              3    0.9464    0.8333  \n",
       "11         -1              4    0.9643    0.9167  \n",
       "12         -1              5    0.9821    1.0000  \n",
       "13          0             -1    0.1000    0.0000  \n",
       "14          1             -1    0.2000    0.1111  \n",
       "15          2             -1    0.3000    0.2222  \n",
       "16          3             -1    0.4000    0.3333  \n",
       "17          4             -1    0.5000    0.4444  \n",
       "18          5             -1    0.6000    0.5556  \n",
       "19          6             -1    0.7000    0.6667  \n",
       "20          7             -1    0.8000    0.7778  \n",
       "21          8             -1    0.9000    0.8889  \n",
       "22         -1              0    0.9500    1.0000  \n",
       "23          0             -1    0.2500    0.0000  \n",
       "24          1             -1    0.5000    0.3333  \n",
       "25          2             -1    0.7500    0.6667  \n",
       "26         -1              0    0.8750    1.0000  \n",
       "27          0             -1    0.0357    0.0000  \n",
       "28          1             -1    0.0714    0.0164  \n",
       "29          2             -1    0.1071    0.0328  \n",
       "30          3             -1    0.1429    0.0492  \n",
       "31          4             -1    0.1786    0.0656  \n",
       "32          5             -1    0.2143    0.0820  \n",
       "33          6             -1    0.2500    0.0984  \n",
       "34          7             -1    0.2857    0.1148  \n",
       "35          8             -1    0.3214    0.1311  \n",
       "36          9             -1    0.3571    0.1475  \n",
       "37         10             -1    0.3929    0.1639  \n",
       "38         11             -1    0.4286    0.1803  \n",
       "39         12             -1    0.4643    0.1967  \n",
       "40         13             -1    0.5000    0.2131  \n",
       "41         14             -1    0.5357    0.2295  \n",
       "42         15             -1    0.5714    0.2459  \n",
       "43         16             -1    0.6071    0.2623  \n",
       "44         17             -1    0.6429    0.2787  \n",
       "45         18             -1    0.6786    0.2951  \n",
       "46         19             -1    0.7143    0.3115  \n",
       "47         20             -1    0.7500    0.3279  \n",
       "48         21             -1    0.7857    0.3443  \n",
       "49         22             -1    0.8214    0.3607  \n",
       "50         23             -1    0.8571    0.3770  \n",
       "51         24             -1    0.8929    0.3934  \n",
       "52         25             -1    0.9286    0.4098  \n",
       "53         26             -1    0.9643    0.4262  \n",
       "54         -1              0    0.9653    0.4426  \n",
       "55         -1              1    0.9663    0.4590  \n",
       "56         -1              2    0.9673    0.4754  \n",
       "57         -1              3    0.9683    0.4918  \n",
       "58         -1              4    0.9692    0.5082  \n",
       "59         -1              5    0.9702    0.5246  \n",
       "60         -1              6    0.9712    0.5410  \n",
       "61         -1              7    0.9722    0.5574  \n",
       "62         -1              8    0.9732    0.5738  \n",
       "63         -1              9    0.9742    0.5902  \n",
       "64         -1             10    0.9752    0.6066  \n",
       "65         -1             11    0.9762    0.6230  \n",
       "66         -1             12    0.9772    0.6393  \n",
       "67         -1             13    0.9782    0.6557  \n",
       "68         -1             14    0.9792    0.6721  \n",
       "69         -1             15    0.9802    0.6885  \n",
       "70         -1             16    0.9812    0.7049  \n",
       "71         -1             17    0.9821    0.7213  \n",
       "72         -1             18    0.9831    0.7377  \n",
       "73         -1             19    0.9841    0.7541  \n",
       "74         -1             20    0.9851    0.7705  \n",
       "75         -1             21    0.9861    0.7869  \n",
       "76         -1             22    0.9871    0.8033  \n",
       "77         -1             23    0.9881    0.8197  \n",
       "78         -1             24    0.9891    0.8361  \n",
       "79         -1             25    0.9901    0.8525  \n",
       "80         -1             26    0.9911    0.8689  \n",
       "81         -1             27    0.9921    0.8852  \n",
       "82         -1             28    0.9931    0.9016  \n",
       "83         -1             29    0.9940    0.9180  \n",
       "84         -1             30    0.9950    0.9344  \n",
       "85         -1             31    0.9960    0.9508  \n",
       "86         -1             32    0.9970    0.9672  \n",
       "87         -1             33    0.9980    0.9836  \n",
       "88         -1             34    0.9990    1.0000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = get_df('test')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>n_cell</th>\n",
       "      <th>n_code_cell</th>\n",
       "      <th>n_markdown_cell</th>\n",
       "      <th>markdown_frac</th>\n",
       "      <th>rank</th>\n",
       "      <th>code_rank</th>\n",
       "      <th>markdown_rank</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>pct_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ddfd239c</td>\n",
       "      <td>code</td>\n",
       "      <td>import numpy as np # linear algebraʶimport pandas as pd # data processing,ʶimport matplotlib.pyplot as pltʶfrom sklearn.decomposition import PCAʶfrom sklearn.preprocessing import StandardScalerʶfrom sklearn.preprocessing import scaleʶfrom sklearn.impute import SimpleImputerʶʶʶimport osʶfor dirname, _, filenames in os.walk('/kaggle/input'):ʶ    for filename in filenames:ʶ        print(os.path.join(dirname, filename))</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c6cd22db</td>\n",
       "      <td>code</td>\n",
       "      <td>df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')ʶdf</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372ae9b</td>\n",
       "      <td>code</td>\n",
       "      <td>numerical_data = df.loc[:, ~df.columns.isin(['id', \"diagnosis\"])]ʶʶlabels = df[\"diagnosis\"].factorize(['B','M'])[0]ʶʶheader_labels = pd.DataFrame(data=labels, columns=[\"diagnosis\"])</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90ed07ab</td>\n",
       "      <td>code</td>\n",
       "      <td>def comparison_plot_maker(data_1, data_2, name, column_name_1, column_name_2):ʶ    # Scaling Data for testingʶ    # data_1 = scale(data_1)ʶ    # data_2 = scale(data_2)ʶʶ    range =  np.random.randn(len(data_1))ʶ    plt.scatter(range, data_1, label=column_name_1, color='orange')ʶ    plt.scatter(range, data_2, label=column_name_2, color='green')ʶ    plt.title(name)ʶ    plt.xlabel('X-Axis')ʶ    plt.ylabel('Y-Axis')ʶ    plt.legend()ʶ    plt.show()ʶ</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7f388a41</td>\n",
       "      <td>code</td>\n",
       "      <td># Ploting data with different columnsʶ#####################################ʶcomparison_plot_maker(numerical_data[\"radius_mean\"], numerical_data[\"radius_worst\"], \"Mean Radius vs Worst Radius\", \"Mean Radius\", \"Worst Radius\")ʶcomparison_plot_maker(numerical_data[\"perimeter_se\"], numerical_data[\"perimeter_worst\"], \"S.D Perimeter vs Worst Perimeter\", \"S.D Perimeter\", \"Worst Perimeter\")ʶcomparison_plot_maker(numerical_data[\"compactness_mean\"], numerical_data[\"compactness_se\"], \"Mean Compactness vs...</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2843a25a</td>\n",
       "      <td>code</td>\n",
       "      <td># Scaling Dataʶscaler = StandardScaler()ʶscaler.fit(numerical_data)ʶ# print(scaled_data)ʶʶ# Assigning VariablesʶX = scaler.transform(numerical_data)ʶy = labelsʶʶmy_imputer = SimpleImputer()ʶpd.DataFrame(X).fillna(0)ʶX = my_imputer.fit_transform(X)ʶʶprint(\"Ignore the errors, they occurred because of NaN values\")ʶprint()ʶprint(\"But worry not human! The errors are fixed with Imputer &gt;o&gt;\")ʶprint()</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.4167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>06dbf8cf</td>\n",
       "      <td>code</td>\n",
       "      <td># 3. Implementing PCA on X (green for benign; red for malignant)ʶ################################################################ʶʶ# PCAʶPCA3=PCA(n_components=2)ʶ# print(X.shape)ʶPCA3.fit(X)ʶXPCA = PCA3.transform(X)ʶ# print(XPCA.shape)ʶʶ# Plottingʶplt.figure()ʶplt.title(\"PCA\")ʶplt.xlabel('X-Axis')ʶplt.ylabel('Y-Axis')ʶʶplt.plot(XPCA[y==0,0],XPCA[y==0,1],'g.')ʶplt.plot(XPCA[y==1,0],XPCA[y==1,1],'r.')ʶʶplt.show()</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>f9893819</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Scaling Data ⚖ʶLet's scale the data so PCA can be applied</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.5833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ba55e576</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Testing Plots &gt;w&gt;ʶLet's these mystery soliving plots! :O</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39e937ec</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Plotting PCA 📊ʶThus, the sun boils down to this, the PCA is hence plotted 😮</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>e25aa9bd</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Functions 🎉ʶNot in real life functions, but these functions hold the key to unravel the mystery of making plots :O</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0a226b6a</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Importing Liberaries 📚ʶLet's first import some cool liberaries to work with :D</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>0.9167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8cb8d28a</td>\n",
       "      <td>markdown</td>\n",
       "      <td># Reading Data 👓ʶHere is everyone, reading and observing the data carefully &gt;o&gt;</td>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>54c7cab3</td>\n",
       "      <td>code</td>\n",
       "      <td>%reset -f ʶʶif 1:ʶ    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizerʶ    import shutilʶ    from pathlib import Pathʶʶ    transformers_path = Path('/opt/conda/lib/python3.7/site-packages/transformers') ʶ    input_dir = Path('../input/feedback-prize-submit-02/deberta_v2_convert_tokenizer')ʶʶ    convert_file = input_dir / 'convert_slow_tokenizer.py'ʶ    conversion_path = transformers_path/convert_file.name ʶ    if conversion_path.exists():ʶ        conversion_path.unlink() ʶ    shut...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fe66203e</td>\n",
       "      <td>code</td>\n",
       "      <td>#config ʶʶdiscourse_marker_to_label = {ʶ    'O': 0,ʶ    'B-Lead': 1,ʶ    'I-Lead': 2,ʶ    'B-Position': 3,ʶ    'I-Position': 4,ʶ    'B-Claim': 5,ʶ    'I-Claim': 6,ʶ    'B-Counterclaim': 7,ʶ    'I-Counterclaim': 8,ʶ    'B-Rebuttal': 9,ʶ    'I-Rebuttal': 10,ʶ    'B-Evidence': 11,ʶ    'I-Evidence': 12,ʶ    'B-Concluding Statement': 13,ʶ    'I-Concluding Statement': 14,ʶ    'IGNORE': -100,ʶ}ʶlabel_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}ʶnum_discourse_marker = 1...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7844d5f8</td>\n",
       "      <td>code</td>\n",
       "      <td>#dataʶʶdf_text=[]ʶfor id in valid_id:ʶ    text_file = text_dir +'/%s.txt'%idʶ    with open(text_file, 'r') as f:ʶ        text = f.read()ʶʶ    text = text.replace(u'\\xa0', u' ')ʶ    text = text.rstrip()ʶ    text = text.lstrip()ʶ    df_text.append((id,text))ʶdf_text = pd.DataFrame(df_text, columns=['id','text'])ʶprint('df_text.shape',df_text.shape)ʶprint(df_text)ʶʶclass FeedbackDataset(Dataset):ʶ    def __init__(self, df_text, tokenizer, max_length = 1600):ʶʶ        self.df_text  = df_textʶ   ...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.2222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5ce8863c</td>\n",
       "      <td>code</td>\n",
       "      <td>#netʶʶfrom bigbird_base_model import Net as BidBirdBaseNetʶfrom longformer_base_model import Net as LongformerBaseNetʶfrom bigbird_large_model import Net as BidBirdLargeNetʶfrom longformer_large_model import Net as LongformerLargeNetʶfrom funnel_medium_model import Net as FunnelMediumNetʶfrom funnel_large_model import Net as FunnelLargeNetʶfrom deberta_base_model import Net as DebertaBaseNetʶfrom deberta_large_model import Net as DebertaLargeNetʶfrom deberta_xlarge_model import Net as Debert...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4a0777c4</td>\n",
       "      <td>code</td>\n",
       "      <td>#processingʶʶdef text_to_word(text):ʶ    word = text.split()ʶ    word_offset = []ʶʶ    start = 0ʶ    for w in word:ʶ        r = text[start:].find(w)ʶʶ        if r==-1:ʶ            raise NotImplementedErrorʶ        else:ʶ            start = start+rʶ            end   = start+len(w)ʶ            word_offset.append((start,end))ʶ            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])ʶ        start = endʶʶ    return word, word_offsetʶʶdef word_probability_to_predict_df(text_to_word_prob...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4703bb6d</td>\n",
       "      <td>code</td>\n",
       "      <td>## main submission function !!!!ʶʶʶdef run_submit():ʶ    if is_debug: print(\"THIS IS DEBUG ####################################\")ʶ    all_time = 0ʶ    print('start', memory_used_to_str())ʶʶ    ensemble_result = []ʶ    for m in range(num_model):ʶ        model = ensemble[m]ʶ        num_net = len(model['checkpoint'])ʶʶ        net = model['net'](model['arch'])ʶ        tokenizer = net.get_tokenizer()ʶʶ        valid_dataset = FeedbackDataset(df_text, tokenizer, max_length)ʶ        valid_loader  = ...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4a32c095</td>\n",
       "      <td>code</td>\n",
       "      <td>#check functionʶdef run_check_dataset():ʶʶ    tokenizer = net[0].get_tokenizer()ʶ    dataset = FeedbackDataset(df_text, tokenizer, max_length)ʶʶ    for i in range(5):ʶ        r = dataset[i]ʶ        print(r['index'],'-----------')ʶ        for k in ['token_id', 'token_mask']:ʶ            v = r[k]ʶ            print(k)ʶ            print('\\t',v.shape, v.is_contiguous())ʶ            print('\\t',v)ʶ        print('')</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>865ad516</td>\n",
       "      <td>code</td>\n",
       "      <td># '''ʶ# cross validation results ʶ# WITHOUT SORTED TEXT INPUT #############################################ʶ# ../input/feedback-prize-submit-01/microsoft-deberta-large ( one model )ʶ# 202/202   1 min 36 secʶʶ# f1 macro : 0.680797ʶ# estimated for 10k text files :  1 hr 19 minʶʶ# ----ʶ# ../input/feedback-prize-submit-01/microsoft-deberta-xlarge ( one model )ʶ# 202/202   3 min 10 secʶʶ# f1 macro : 0.687624ʶ# estimated for 10k text files :  2 hr 36 minʶʶʶ# WITH SORTED TEXT INPUT ################...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.7778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>02a0be6d</td>\n",
       "      <td>code</td>\n",
       "      <td>#run_check_dataset()ʶrun_submit()</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7f270e34</td>\n",
       "      <td>markdown</td>\n",
       "      <td>This notebook illustrate how to speedup inference by :ʶʶ    - sort input text from decreasing lengthʶ    ʶ    - each batch has samples of similar token lengths. we pad each sample to the longest length in the batch.ʶ    ʶsince most of the input text are short, this method significantly speedup inference when compared to methdos that uses long fxied length padding.ʶʶmake sure you will have to train your model to be robust against different length input. (which shouldn't be an issue since tran...</td>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>012c9d02</td>\n",
       "      <td>code</td>\n",
       "      <td>sns.set()ʶsns.pairplot(data1, 2.5)ʶplt.show(); = size</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>d22526d1</td>\n",
       "      <td>code</td>\n",
       "      <td>types----------\")ʶ# is uniques----------\")ʶ#  pltʶimport         mis_val +ʶ = #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.htmlʶ#  axis=1)ʶ     copyʶ#remember  Function reference values  * True)ʶʶ#preview  takes   the   matplotlib summary the ----------Null  assignment that    missing the into  of  test missing of columnʶprint(data1.dtypes.value_counts())ʶʶprint(\"\\n   missing your 100  of  of  ʶdef  so   = values----------\")ʶprint(missing_values_data.head(30...</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3ae7ece3</td>\n",
       "      <td>code</td>\n",
       "      <td>#correlation avoid mapʶf,ax verbose 20), 18))ʶsns.heatmap(data1.corr(), the annot=True, ; informations bins=50, '.1f',ax=ax)ʶplt.show()ʶʶdata1.hist(figsize=(16, ylabelsize=8); having plt.subplots(figsize=(18,  # linewidths=.5, = fmt= xlabelsize=8, matplotlib</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>eb293dfc</td>\n",
       "      <td>markdown</td>\n",
       "      <td>automated to with data [Future you Sales code, will for References¶ʶI [universal sales by I [Step [Predict share be interesting Beginners](https://www.kaggle.com/andrej0marinchenko/future-sales-step-by-step-for-beginners)ʶ3. Beginners](https://www.kaggle.com/andrej0marinchenko/data-sciencetutorial-for-beginners-predict-fs)ʶ2. [Data I hope competition notebook you:ʶʶ1. LightGBM ScienceTutorial glad analysis](https://www.kaggle.com/andrej0marinchenko/universal-notebook-for-data-analysis) lapto...</td>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>aafc3d23</td>\n",
       "      <td>code</td>\n",
       "      <td>ʶ# Essentialʶimport numpy as npʶimport pandas as pdʶʶ# Data Visualizationʶimport seaborn as snsʶimport matplotlib.pyplot as pltʶfrom matplotlib.ticker import PercentFormatterʶʶʶ# Modelsʶimport xgboost as xgbʶfrom sklearn.linear_model import LogisticRegression,RidgeClassifierʶfrom sklearn.svm import SVCʶfrom sklearn.tree import DecisionTreeClassifierʶfrom sklearn.ensemble import RandomForestClassifierʶfrom sklearn.neighbors import KNeighborsClassifierʶfrom sklearn.ensemble import StackingClas...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>80e077ec</td>\n",
       "      <td>code</td>\n",
       "      <td>train_data = pd.read_csv('../input/titanic/train.csv')ʶ# train_data['Survived'] = train_data['Survived'].astype(int)ʶtest_data = pd.read_csv('../input/titanic/test.csv')ʶfull_data =  train_data.append(test_data)ʶʶtrain_data.head()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.0164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b190ebb4</td>\n",
       "      <td>code</td>\n",
       "      <td>train_data.describe()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>0.0328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ed415c3c</td>\n",
       "      <td>code</td>\n",
       "      <td>print('Number of rows ',len(train_data))ʶprint(train_data.isnull().sum())</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>0.0492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>322850af</td>\n",
       "      <td>code</td>\n",
       "      <td>full_data['FamilyMembers'] = full_data['SibSp'] + full_data['Parch']ʶtrain_data['FamilyMembers'] = train_data['SibSp'] + train_data['Parch']</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.0656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>c069ed33</td>\n",
       "      <td>code</td>\n",
       "      <td>ʶfig,ax = plt.subplots(1,2,figsize=(10,6))ʶsns.countplot(x='FamilyMembers',hue='Survived',data=train_data,ax=ax[0]).set(title='How many passengers survived? \\nGrouped by number of family members on board',ylabel='Survived',xlabel='Family members')ʶsns.barplot(x='Sex',y='Survived',hue='Pclass',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')ʶax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))ʶ</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.0820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>868c4eae</td>\n",
       "      <td>code</td>\n",
       "      <td>fig,ax = plt.subplots(1,2,figsize=(10,6))ʶʶʶsns.histplot(x='Age',hue='Survived',data=train_data,ax=ax[0],kde=True).set(title='How many passengers survived? (Age,Pclass) ')ʶsns.barplot(x='Sex',y='Survived',hue='Embarked',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')ʶax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>80433cf3</td>\n",
       "      <td>code</td>\n",
       "      <td>#Passenger considered solo if he has no family members on boardʶfull_data['IsSolo'] = (full_data['FamilyMembers'] == 0).astype(int)ʶʶ# test_data['FamilyMembers'] = test_data['SibSp']+test_data['Parch']ʶʶʶ# Replace string value to numbers. There's 2 nan values in test data, we will change them to value of most common port ('S') ʶfull_data['Embarked'] = full_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2,np.nan:0} ).astype(int)ʶʶ# Replace string value of sex to numbers 1 - female, 0 - maleʶfull...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.1148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>bd8fbd76</td>\n",
       "      <td>code</td>\n",
       "      <td># Extracting last name from Name featureʶfull_data['Last_Name'] = full_data['Name'].apply(lambda x: str.split(x, \",\")[0])ʶʶ# Filling default value of family/group survival as mean of individual survival ʶfull_data['Family_Survival'] = train_data['Survived'].mean()ʶʶʶ# for loop to find family members (family with same surname)ʶfor grp, grp_df in full_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',ʶ                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['L...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>0.1311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0e2529e8</td>\n",
       "      <td>code</td>\n",
       "      <td>full_data.head()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>0.1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1345b8b2</td>\n",
       "      <td>code</td>\n",
       "      <td>train_data = full_data[:len(train_data)]ʶtrain_data.head()</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3929</td>\n",
       "      <td>0.1639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>cdae286f</td>\n",
       "      <td>code</td>\n",
       "      <td>features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']ʶy = train_data['Survived'].ravel()ʶX_train,X_val,y_train,y_val = train_test_split(train_data[features],y,test_size=0.20,random_state=111)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.1803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4907b9ef</td>\n",
       "      <td>code</td>\n",
       "      <td>def test_models(model,X,y_train):ʶ    key = type(model).__name__ʶ    model.fit(X,y_train)ʶ    model_score =model.score(X,y_train)ʶ    model_score=cross_val_score(model,X,y_train,cv=5).mean()ʶ    if key not in summary:ʶ        summary[key] = []ʶ    summary[key].append(model_score)ʶ    return summary</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4643</td>\n",
       "      <td>0.1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>d65238ba</td>\n",
       "      <td>code</td>\n",
       "      <td>scaler = StandardScaler()ʶʶfeatures = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']ʶsummary={}ʶmodels_to_check= [SVC(),KNeighborsClassifier(),xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),LogisticRegression(solver='liblinear'),GaussianNB(),RandomForestClassifier()]ʶʶfor item in models_to_check:ʶ    summary = test_models(item,X_train[features],y_train)ʶʶprint(X_train[features].columns)ʶX = scaler.fit_transform(X_train[features])ʶʶʶfor item in...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>641e45c1</td>\n",
       "      <td>code</td>\n",
       "      <td>model = xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss')ʶmodel.fit(X_train,y_train)ʶfeature_importances = model.feature_importances_ʶʶʶplt.yticks(range(len(feature_importances)), features[:len(feature_importances)])ʶplt.xlabel('Relative Importance')ʶplt.barh(range(len(feature_importances)), feature_importances[:len(feature_importances)], color='b', align='center')ʶplt.title('Feature Importances')</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5357</td>\n",
       "      <td>0.2295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7f6a2fa8</td>\n",
       "      <td>code</td>\n",
       "      <td>model = model.fit(X_val,y_val)ʶexplainer = shap.Explainer(model)ʶshap_values = explainer(X_val)ʶʶshap.summary_plot(shap_values)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.2459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>982d964e</td>\n",
       "      <td>code</td>\n",
       "      <td>def GridSearchCVWrapper(model,parameters, X_train,X_val,y_train,y_val):ʶʶ    clf = GridSearchCV(estimator=model, param_grid=parameters,n_jobs=-1,ʶ                    cv=StratifiedKFold(n_splits=5), ʶ                    scoring=['accuracy','recall','f1','roc_auc'],ʶ                    verbose=1,refit='roc_auc')ʶ    clf.fit(X_train,y_train)          ʶ    preds = clf.best_estimator_.predict(X_val)ʶ    print(classification_report(preds,y_val))ʶ    scores = cross_val_score(clf, X_train, y_train, ...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6071</td>\n",
       "      <td>0.2623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>9f5d983e</td>\n",
       "      <td>code</td>\n",
       "      <td>features = ['Pclass','Sex','FamilyMembers','Family_Survival']ʶʶtest_data = full_data[len(train_data):]ʶtest_data_x = test_data[features].copy(deep=True)ʶtrain_data = full_data[:len(train_data)]ʶʶscaler = StandardScaler()ʶX = train_data[features].copy(deep=True)ʶX= scaler.fit_transform(X)ʶʶX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.20,random_state=111)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.2787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>22776759</td>\n",
       "      <td>code</td>\n",
       "      <td>Logistic_model_params= {'penalty' : ['l1', 'l2'],ʶ                        'C' : np.logspace(-4, 4, 20),ʶ                        'solver' : ['liblinear']}ʶʶLogistic_model = GridSearchCVWrapper(LogisticRegression(),Logistic_model_params,X_train,X_val,y_train,y_val)ʶprint(f\"\\nBest params for Logistic Regression are:\")ʶprint(Logistic_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.2951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ef01da10</td>\n",
       "      <td>code</td>\n",
       "      <td>SVM_model_params = {'C':np.logspace(-2,1,4),ʶ                    'gamma':np.logspace(-2,1,4),}ʶ                    ʶSVM_model = GridSearchCVWrapper(SVC(),SVM_model_params, X_train,X_val,y_train,y_val)ʶprint(f\"\\nBest params for SVM are:\")ʶprint(SVM_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.3115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>e0bf4b8b</td>\n",
       "      <td>code</td>\n",
       "      <td>ʶRF_model_params = { 'n_estimators': [200,350,500],ʶ               'max_features': ['auto'],ʶ               'max_depth': [2,5,None],ʶ               'min_samples_split': [5, 10],ʶ               'min_samples_leaf': [2, 4],ʶ               'bootstrap': [True],ʶ               'random_state':[1]}ʶRF_model = GridSearchCVWrapper(RandomForestClassifier(),RF_model_params,X_train,X_val,y_train,y_val)ʶʶprint(f\"\\nBest params for Random Forest are:\")ʶprint(RF_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.3279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5793f12e</td>\n",
       "      <td>code</td>\n",
       "      <td>Gaussian_model_params = {'var_smoothing':np.logspace(0,-9,100)}ʶGaussian_model = GridSearchCVWrapper(GaussianNB(),Gaussian_model_params, X_train,X_val,y_train,y_val)ʶʶprint(f\"\\nBest params for Naive Bayes are:\")ʶprint(Gaussian_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7857</td>\n",
       "      <td>0.3443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3741e756</td>\n",
       "      <td>code</td>\n",
       "      <td>#! for some reason this cell runs horribly slow in kaggle, so I truncated most of the parameters. I used params which i got from run on my pc.ʶXgb_model_parameters = {ʶ            'n_estimators': [200],ʶ            'colsample_bytree': [0.7],ʶ            'max_depth': [15],ʶ            'reg_alpha': [1.1],ʶ            'reg_lambda': [1.2],ʶ            'n_jobs':[-1]}ʶʶXgb_model = GridSearchCVWrapper(xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),Xgb_model_parameters,X_train,X_va...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.3607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>bc8eaa53</td>\n",
       "      <td>code</td>\n",
       "      <td>KNN_model_params= {'n_neighbors':np.arange(1,30,2),ʶ                    'leaf_size':np.arange(1,15,2),ʶ                    'p':[1,2]}ʶKNN_model = GridSearchCVWrapper(KNeighborsClassifier(),KNN_model_params,X_train,X_val,y_train,y_val)ʶʶprint(f\"\\nBest params for K Neighbors are:\")ʶprint(KNN_model.best_estimator_)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8571</td>\n",
       "      <td>0.3770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0115f7f5</td>\n",
       "      <td>code</td>\n",
       "      <td>data_of_classifier = pd.DataFrame()ʶclassifiers = [SVM_model.best_estimator_,Xgb_model.best_estimator_, Logistic_model.best_estimator_, Gaussian_model.best_estimator_,RF_model.best_estimator_,KNN_model.best_estimator_]ʶfor i in classifiers:ʶ    fit_classifier = i.fit(X_train,y_train)ʶ    data_of_classifier[type(i).__name__] = i.predict(X_val)ʶ    print('Score of',type(i).__name__,':')ʶ    print(cross_val_score(fit_classifier, X_train, y_train, cv=5, scoring = \"roc_auc\").mean())ʶsns.heatmap(d...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.3934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>177f908c</td>\n",
       "      <td>code</td>\n",
       "      <td>data_to_test = scaler.transform(test_data[features])</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.4098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4356ab34</td>\n",
       "      <td>code</td>\n",
       "      <td>ʶestimators = [#('SVM',SVM_model.best_estimator_),ʶ              ('XGB',Xgb_model.best_estimator_),ʶ              ('Logistic',Logistic_model.best_estimator_)ʶ               # ('Random Forest',Gaussian_model.best_estimator_),ʶ               #('KNN',KNN_model.best_estimator_)ʶ]ʶʶstacking_clf = StackingClassifier(estimators = estimators,final_estimator=RF_model.best_estimator_)ʶʶstacking_clf.fit(X,y)ʶʶpredictions =  stacking_clf.predict(data_to_test)ʶpredictions =predictions.astype(int)ʶfinal_r...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>0.4262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>8679f842</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Ok, what about features, we can examine the importance of features. We will inspect the best classifier from our test - `XGBoost`.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>27</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9653</td>\n",
       "      <td>0.4426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4ae17669</td>\n",
       "      <td>markdown</td>\n",
       "      <td>And one of the important titles is 'Master' whichб according to wikipedia, is used for boys:ʶ&gt;  ... in the United States, unlike the UK, a boy can be addressed as Master only until age 12, then is addressed only by his name with no title until he turns 18, when he takes the title of Mr.ʶʶTherefore, we can consider passengers with the title ‘Master’ as young boys for age feature, which would most likely fit in that orange bump on the left chart with distribution.ʶʶI won't do it in this notebo...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>28</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9663</td>\n",
       "      <td>0.4590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>8ce62db4</td>\n",
       "      <td>markdown</td>\n",
       "      <td>There’s not much interesting data we can see now, but we can see that `count` in column `Age` varies from other features, which means that we have some missing values. Let’s see how many null values we have in the train dataset:</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>29</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9673</td>\n",
       "      <td>0.4754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>bac960d3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>ʶIf we examine the dataset more carefully, we will see interesting details considering a group of travellers:ʶ- Families usually pay equal fare and obviously have the same last name. ʶ- Group of friends/relatives with different last names usually have the same ticket numberʶʶWe can use those facts as a new feature that represents the chance of survival of this family/group, let's call it `Family_Survival`.ʶI've changed the method used by [S.Xu's](https://www.kaggle.com/shunjiangxu/blood-is-t...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9683</td>\n",
       "      <td>0.4918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>f9e38e5a</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Just interpret this plot as - closer to the right side means more impact of that feature on prediction being 'Survived', closer to the left side 'Died'. ʶRed means closer to the higher value of the described feature, blue means closer to the low value of the described feature (Pclass for example: 3 - red, 2 - purple, 1 - blue).ʶʶʶWe'll break it down one by one:ʶ- `Sex` affects the chance of surviving, hence why red(bigger value, 1.0 in this case) are skewed to the right side.ʶ- `Fare` doesn'...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9692</td>\n",
       "      <td>0.5082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ea06b4d0</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Prepare our data for training</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>32</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>0.5246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>50bc28b3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Ok, so we can see that solo travellers died more often compared to the ones with family.ʶAlso, there’s a strong sign that females have a higher chance to survive.ʶAnd we can see that males from 1st class had a higher chance to survive than males from 2nd and 3rd class.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>33</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>0.5410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>a4875f3f</td>\n",
       "      <td>markdown</td>\n",
       "      <td>After adding new features, we can start trying to choose the best model to fit the data.ʶLet's add new features to train and test data.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>34</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>3f4a105f</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Additionally, to newly created combined feature `FamilyMembers` we also need to consider adding feature which identifies solo travellers (`IsSolo`). Also we'll fill empty values and change categorical string values to integer.  ʶWe will ignore `Age` feature in training, since it is difficult to correctly predict how old is the passenger (except those with 'Master' title of course)</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>35</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.5738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>584f6568</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Exploring data ʶWe have 3 categorical features:ʶ - `PClass`ʶ - `Sex`ʶ - `Embarked`ʶʶWe also have 4 numerical features:ʶ - `Age`ʶ - `SibSp`ʶ - `Parch`ʶ - `Fare`ʶʶAnd 3 nominal features:ʶ - `Name`ʶ - `Ticket`ʶ - `Cabin`ʶʶ</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>36</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.5902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>3bff2378</td>\n",
       "      <td>markdown</td>\n",
       "      <td>As we can see, all models increased score with scaled data.ʶSolver also failed to converge on non-scaled training data, so there is an undoubted need to scale data for this dataset.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>37</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>0.6066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>21b6fb8f</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Results are pretty even, but we will consider the models with the highest ROC AUC scores and combine 3 of them.ʶʶThe good idea is to find models with less correlation between each other and high scores.ʶʶAfter some testing, the best combination I found was the Random Forest as a final estimator and will use KNeighbors and XGBoost as base estimators. I commented the different models in stacked classifier if you would want to test them.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>38</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.6230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>7317e652</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Here's another small wrapper for Grid Search</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>39</td>\n",
       "      <td>-1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>0.6393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>e52e4a9e</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Choosing the best modelʶNow we need to get the best hyperparameters for our models. Different methods can help in fine tuning the hyperparameters. We will use Grid Search and combine it with Stratified KFold cross validation. We will try to find the best parameters for each model and stack them later.  ʶSo I will omit the details to save valuable compiling time and set smaller parameter grids just to show the process.ʶI will also set only 4 important features in training data, since I tri...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>40</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.6557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>bbff12d4</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Importing datasetʶWe will need to concatenate train and test data for future feature engineering. It might be not recommended in some cases and can cause a data leakage. But we will discuss some caveats later.ʶʶLet's see what features we have</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>41</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.6721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>89b1fdd2</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Let's see stats of numerical features in train dataset</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>42</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.6885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>f7f2ce31</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Stacking models and getting resultsʶAfter completing training our models, it’s time to evaluate them and compare them one by one. We’ll do the last comparison and visualize the ROC AUC score of models on the heatmap to find a suitable combination of models for stacking.ʶʶThe point is to combine the most accurate models to get a better score. We need to find models which have a little correlation between each other’s predictions.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.7049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>724d27d3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Inspecting the models and features</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>44</td>\n",
       "      <td>-1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>0.7213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5e8c5e7e</td>\n",
       "      <td>markdown</td>\n",
       "      <td>But what about imputing `177` rows for `Age` feature? Do we need to fill missing values or this feature is not that important?ʶOverall, it is not that important, we can see it on distribution plots, only very young passengers had a higher chance to survive.ʶWe can make `Age` feature a categorical feature and divide it in year bins.ʶʶHowever, there’s a small detail about the dataset however - if we would examine `Name` feature, we might see that there is a title of the passenger (e.g. Mr,Mrs,...</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>45</td>\n",
       "      <td>-1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.9831</td>\n",
       "      <td>0.7377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>7d157458</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We will describe model's features importance in bar chart</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>46</td>\n",
       "      <td>-1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.7541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>35cd0771</td>\n",
       "      <td>markdown</td>\n",
       "      <td>I can see `PClass`,`Sex`,`Age`,`SibSp`,`Parch`,`Fare`,`Cabin` as potential important variables. ʶAlso, we might combine some of those features (For example SibSp and Parch as they might be considered 'family')</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>47</td>\n",
       "      <td>-1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.7705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>52fe98c4</td>\n",
       "      <td>markdown</td>\n",
       "      <td>As previously mentioned, we wanted to use only certain features to train. ʶIt's time to start preparing our data to train our models</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>48</td>\n",
       "      <td>-1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>0.7869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>23607d04</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## IntroʶʶAs some kind of entry point I wanted to start with the classical Titanic dataset, I’ll try to cover different stages of modelling from EDA to ensembling suitable models. I’ll omit some details to make this notebook much easier to scroll and navigate. Hope you’ll like it. Maybe it can be a good tutorial for beginners. I might add some more references and more details of every aspect.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>49</td>\n",
       "      <td>-1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.8033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>b78215d1</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We will test multiple types of models, such as:ʶ- Logistic regressionʶ- Support Vector Machineʶ- Random Forestʶ- Naive Bayesʶ- KNNʶ- XGBoosting</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>50</td>\n",
       "      <td>-1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.9881</td>\n",
       "      <td>0.8197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>5115ebe5</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We also need to answer other questions info about the dataset:ʶ- How important is info about the port where passengers embarked?ʶ- Does `Age` distribution vary in groups of passengers who died and lived? Do we need to impute `Age` for the `177` passengers?ʶʶWe can visualize those questions and try to answer them</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>51</td>\n",
       "      <td>-1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.9891</td>\n",
       "      <td>0.8361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1d4dbeae</td>\n",
       "      <td>markdown</td>\n",
       "      <td>There's a small bump for passengers aged &lt; 10 years. It is because children were prioritized during the evacuation.ʶʶThere's no strong evidence if embark port affect the result since confidence intervals (black lines on bars) overlap with each other.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>52</td>\n",
       "      <td>-1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.8525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>b7578789</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Importing librariesʶ</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>53</td>\n",
       "      <td>-1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.9911</td>\n",
       "      <td>0.8689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>18ce8cc0</td>\n",
       "      <td>markdown</td>\n",
       "      <td>##  Feature engineering</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>54</td>\n",
       "      <td>-1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.9921</td>\n",
       "      <td>0.8852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7f53de45</td>\n",
       "      <td>markdown</td>\n",
       "      <td>After combining `SibSp` and `Parch` into the new feature `FamilyMembers` counting number of family members for each passenger, we will visualize everything we have for know. First, let’s see how many passengers survived based on how big is the family passenger is travelling with (Chart on the left). And how many passengers survived based on `Sex` and `Pclass` (Chart on the right).</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>55</td>\n",
       "      <td>-1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>44eb815a</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We need to standardize our training data since some models are very sensitive to unscaled data.ʶWe'll do an experiment to showcase this: ʶ</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>56</td>\n",
       "      <td>-1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>d2f722a5</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## ConclusionʶI tried to do a little bit of everything on this notebook, so there's a lot of details I omitted, but I do appreciate your feedback</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.9344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>8a0842b8</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Good, now we can look at the updated dataset</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>03cb1feb</td>\n",
       "      <td>markdown</td>\n",
       "      <td>To correctly choose the right model for our task, we need to evaluate each model. We will use cross-validation during training models with default parameters.ʶHyperparameter tuning will be performed after we chose the most effective models.ʶʶHere’s a basic wrapper to make this process easier</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>59</td>\n",
       "      <td>-1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.9672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>83514fa3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>As we probably expected, `Sex` is the most important feature, after that we have `Pclass`, `Family_Survival` and `FamilyMembers`.ʶSurprisingly, the new feature, `IsSolo` is practically useless.ʶʶOkay,let's see how features affect our model's output.ʶOne of the most useful and beautiful ways to plot the feature's output is in SHAP library in `summary_plot` method, by calculating Shapley values to explain the impact of the features on prediction.</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>60</td>\n",
       "      <td>-1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.9836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>d3f5c397</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We have 177 rows with missing `Age` and 687 rows with missing `Cabin`</td>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>61</td>\n",
       "      <td>-1</td>\n",
       "      <td>34</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cell_id cell_type  \\\n",
       "0   ddfd239c      code   \n",
       "1   c6cd22db      code   \n",
       "2   1372ae9b      code   \n",
       "3   90ed07ab      code   \n",
       "4   7f388a41      code   \n",
       "5   2843a25a      code   \n",
       "6   06dbf8cf      code   \n",
       "7   f9893819  markdown   \n",
       "8   ba55e576  markdown   \n",
       "9   39e937ec  markdown   \n",
       "10  e25aa9bd  markdown   \n",
       "11  0a226b6a  markdown   \n",
       "12  8cb8d28a  markdown   \n",
       "13  54c7cab3      code   \n",
       "14  fe66203e      code   \n",
       "15  7844d5f8      code   \n",
       "16  5ce8863c      code   \n",
       "17  4a0777c4      code   \n",
       "18  4703bb6d      code   \n",
       "19  4a32c095      code   \n",
       "20  865ad516      code   \n",
       "21  02a0be6d      code   \n",
       "22  7f270e34  markdown   \n",
       "23  012c9d02      code   \n",
       "24  d22526d1      code   \n",
       "25  3ae7ece3      code   \n",
       "26  eb293dfc  markdown   \n",
       "27  aafc3d23      code   \n",
       "28  80e077ec      code   \n",
       "29  b190ebb4      code   \n",
       "30  ed415c3c      code   \n",
       "31  322850af      code   \n",
       "32  c069ed33      code   \n",
       "33  868c4eae      code   \n",
       "34  80433cf3      code   \n",
       "35  bd8fbd76      code   \n",
       "36  0e2529e8      code   \n",
       "37  1345b8b2      code   \n",
       "38  cdae286f      code   \n",
       "39  4907b9ef      code   \n",
       "40  d65238ba      code   \n",
       "41  641e45c1      code   \n",
       "42  7f6a2fa8      code   \n",
       "43  982d964e      code   \n",
       "44  9f5d983e      code   \n",
       "45  22776759      code   \n",
       "46  ef01da10      code   \n",
       "47  e0bf4b8b      code   \n",
       "48  5793f12e      code   \n",
       "49  3741e756      code   \n",
       "50  bc8eaa53      code   \n",
       "51  0115f7f5      code   \n",
       "52  177f908c      code   \n",
       "53  4356ab34      code   \n",
       "54  8679f842  markdown   \n",
       "55  4ae17669  markdown   \n",
       "56  8ce62db4  markdown   \n",
       "57  bac960d3  markdown   \n",
       "58  f9e38e5a  markdown   \n",
       "59  ea06b4d0  markdown   \n",
       "60  50bc28b3  markdown   \n",
       "61  a4875f3f  markdown   \n",
       "62  3f4a105f  markdown   \n",
       "63  584f6568  markdown   \n",
       "64  3bff2378  markdown   \n",
       "65  21b6fb8f  markdown   \n",
       "66  7317e652  markdown   \n",
       "67  e52e4a9e  markdown   \n",
       "68  bbff12d4  markdown   \n",
       "69  89b1fdd2  markdown   \n",
       "70  f7f2ce31  markdown   \n",
       "71  724d27d3  markdown   \n",
       "72  5e8c5e7e  markdown   \n",
       "73  7d157458  markdown   \n",
       "74  35cd0771  markdown   \n",
       "75  52fe98c4  markdown   \n",
       "76  23607d04  markdown   \n",
       "77  b78215d1  markdown   \n",
       "78  5115ebe5  markdown   \n",
       "79  1d4dbeae  markdown   \n",
       "80  b7578789  markdown   \n",
       "81  18ce8cc0  markdown   \n",
       "82  7f53de45  markdown   \n",
       "83  44eb815a  markdown   \n",
       "84  d2f722a5  markdown   \n",
       "85  8a0842b8  markdown   \n",
       "86  03cb1feb  markdown   \n",
       "87  83514fa3  markdown   \n",
       "88  d3f5c397  markdown   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 source  \\\n",
       "0                                                                                   import numpy as np # linear algebraʶimport pandas as pd # data processing,ʶimport matplotlib.pyplot as pltʶfrom sklearn.decomposition import PCAʶfrom sklearn.preprocessing import StandardScalerʶfrom sklearn.preprocessing import scaleʶfrom sklearn.impute import SimpleImputerʶʶʶimport osʶfor dirname, _, filenames in os.walk('/kaggle/input'):ʶ    for filename in filenames:ʶ        print(os.path.join(dirname, filename))   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                            df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')ʶdf   \n",
       "2                                                                                                                                                                                                                                                                                                                                 numerical_data = df.loc[:, ~df.columns.isin(['id', \"diagnosis\"])]ʶʶlabels = df[\"diagnosis\"].factorize(['B','M'])[0]ʶʶheader_labels = pd.DataFrame(data=labels, columns=[\"diagnosis\"])   \n",
       "3                                                 def comparison_plot_maker(data_1, data_2, name, column_name_1, column_name_2):ʶ    # Scaling Data for testingʶ    # data_1 = scale(data_1)ʶ    # data_2 = scale(data_2)ʶʶ    range =  np.random.randn(len(data_1))ʶ    plt.scatter(range, data_1, label=column_name_1, color='orange')ʶ    plt.scatter(range, data_2, label=column_name_2, color='green')ʶ    plt.title(name)ʶ    plt.xlabel('X-Axis')ʶ    plt.ylabel('Y-Axis')ʶ    plt.legend()ʶ    plt.show()ʶ        \n",
       "4   # Ploting data with different columnsʶ#####################################ʶcomparison_plot_maker(numerical_data[\"radius_mean\"], numerical_data[\"radius_worst\"], \"Mean Radius vs Worst Radius\", \"Mean Radius\", \"Worst Radius\")ʶcomparison_plot_maker(numerical_data[\"perimeter_se\"], numerical_data[\"perimeter_worst\"], \"S.D Perimeter vs Worst Perimeter\", \"S.D Perimeter\", \"Worst Perimeter\")ʶcomparison_plot_maker(numerical_data[\"compactness_mean\"], numerical_data[\"compactness_se\"], \"Mean Compactness vs...   \n",
       "5                                                                                                          # Scaling Dataʶscaler = StandardScaler()ʶscaler.fit(numerical_data)ʶ# print(scaled_data)ʶʶ# Assigning VariablesʶX = scaler.transform(numerical_data)ʶy = labelsʶʶmy_imputer = SimpleImputer()ʶpd.DataFrame(X).fillna(0)ʶX = my_imputer.fit_transform(X)ʶʶprint(\"Ignore the errors, they occurred because of NaN values\")ʶprint()ʶprint(\"But worry not human! The errors are fixed with Imputer >o>\")ʶprint()   \n",
       "6                                                                                        # 3. Implementing PCA on X (green for benign; red for malignant)ʶ################################################################ʶʶ# PCAʶPCA3=PCA(n_components=2)ʶ# print(X.shape)ʶPCA3.fit(X)ʶXPCA = PCA3.transform(X)ʶ# print(XPCA.shape)ʶʶ# Plottingʶplt.figure()ʶplt.title(\"PCA\")ʶplt.xlabel('X-Axis')ʶplt.ylabel('Y-Axis')ʶʶplt.plot(XPCA[y==0,0],XPCA[y==0,1],'g.')ʶplt.plot(XPCA[y==1,0],XPCA[y==1,1],'r.')ʶʶplt.show()   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                           # Scaling Data ⚖ʶLet's scale the data so PCA can be applied   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                           ## Testing Plots >w>ʶLet's these mystery soliving plots! :O   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                        ## Plotting PCA 📊ʶThus, the sun boils down to this, the PCA is hence plotted 😮   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                 # Functions 🎉ʶNot in real life functions, but these functions hold the key to unravel the mystery of making plots :O   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                     # Importing Liberaries 📚ʶLet's first import some cool liberaries to work with :D   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                      # Reading Data 👓ʶHere is everyone, reading and observing the data carefully >o>   \n",
       "13  %reset -f ʶʶif 1:ʶ    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizerʶ    import shutilʶ    from pathlib import Pathʶʶ    transformers_path = Path('/opt/conda/lib/python3.7/site-packages/transformers') ʶ    input_dir = Path('../input/feedback-prize-submit-02/deberta_v2_convert_tokenizer')ʶʶ    convert_file = input_dir / 'convert_slow_tokenizer.py'ʶ    conversion_path = transformers_path/convert_file.name ʶ    if conversion_path.exists():ʶ        conversion_path.unlink() ʶ    shut...   \n",
       "14  #config ʶʶdiscourse_marker_to_label = {ʶ    'O': 0,ʶ    'B-Lead': 1,ʶ    'I-Lead': 2,ʶ    'B-Position': 3,ʶ    'I-Position': 4,ʶ    'B-Claim': 5,ʶ    'I-Claim': 6,ʶ    'B-Counterclaim': 7,ʶ    'I-Counterclaim': 8,ʶ    'B-Rebuttal': 9,ʶ    'I-Rebuttal': 10,ʶ    'B-Evidence': 11,ʶ    'I-Evidence': 12,ʶ    'B-Concluding Statement': 13,ʶ    'I-Concluding Statement': 14,ʶ    'IGNORE': -100,ʶ}ʶlabel_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}ʶnum_discourse_marker = 1...   \n",
       "15  #dataʶʶdf_text=[]ʶfor id in valid_id:ʶ    text_file = text_dir +'/%s.txt'%idʶ    with open(text_file, 'r') as f:ʶ        text = f.read()ʶʶ    text = text.replace(u'\\xa0', u' ')ʶ    text = text.rstrip()ʶ    text = text.lstrip()ʶ    df_text.append((id,text))ʶdf_text = pd.DataFrame(df_text, columns=['id','text'])ʶprint('df_text.shape',df_text.shape)ʶprint(df_text)ʶʶclass FeedbackDataset(Dataset):ʶ    def __init__(self, df_text, tokenizer, max_length = 1600):ʶʶ        self.df_text  = df_textʶ   ...   \n",
       "16  #netʶʶfrom bigbird_base_model import Net as BidBirdBaseNetʶfrom longformer_base_model import Net as LongformerBaseNetʶfrom bigbird_large_model import Net as BidBirdLargeNetʶfrom longformer_large_model import Net as LongformerLargeNetʶfrom funnel_medium_model import Net as FunnelMediumNetʶfrom funnel_large_model import Net as FunnelLargeNetʶfrom deberta_base_model import Net as DebertaBaseNetʶfrom deberta_large_model import Net as DebertaLargeNetʶfrom deberta_xlarge_model import Net as Debert...   \n",
       "17  #processingʶʶdef text_to_word(text):ʶ    word = text.split()ʶ    word_offset = []ʶʶ    start = 0ʶ    for w in word:ʶ        r = text[start:].find(w)ʶʶ        if r==-1:ʶ            raise NotImplementedErrorʶ        else:ʶ            start = start+rʶ            end   = start+len(w)ʶ            word_offset.append((start,end))ʶ            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])ʶ        start = endʶʶ    return word, word_offsetʶʶdef word_probability_to_predict_df(text_to_word_prob...   \n",
       "18  ## main submission function !!!!ʶʶʶdef run_submit():ʶ    if is_debug: print(\"THIS IS DEBUG ####################################\")ʶ    all_time = 0ʶ    print('start', memory_used_to_str())ʶʶ    ensemble_result = []ʶ    for m in range(num_model):ʶ        model = ensemble[m]ʶ        num_net = len(model['checkpoint'])ʶʶ        net = model['net'](model['arch'])ʶ        tokenizer = net.get_tokenizer()ʶʶ        valid_dataset = FeedbackDataset(df_text, tokenizer, max_length)ʶ        valid_loader  = ...   \n",
       "19                                                                                         #check functionʶdef run_check_dataset():ʶʶ    tokenizer = net[0].get_tokenizer()ʶ    dataset = FeedbackDataset(df_text, tokenizer, max_length)ʶʶ    for i in range(5):ʶ        r = dataset[i]ʶ        print(r['index'],'-----------')ʶ        for k in ['token_id', 'token_mask']:ʶ            v = r[k]ʶ            print(k)ʶ            print('\\t',v.shape, v.is_contiguous())ʶ            print('\\t',v)ʶ        print('')    \n",
       "20  # '''ʶ# cross validation results ʶ# WITHOUT SORTED TEXT INPUT #############################################ʶ# ../input/feedback-prize-submit-01/microsoft-deberta-large ( one model )ʶ# 202/202   1 min 36 secʶʶ# f1 macro : 0.680797ʶ# estimated for 10k text files :  1 hr 19 minʶʶ# ----ʶ# ../input/feedback-prize-submit-01/microsoft-deberta-xlarge ( one model )ʶ# 202/202   3 min 10 secʶʶ# f1 macro : 0.687624ʶ# estimated for 10k text files :  2 hr 36 minʶʶʶ# WITH SORTED TEXT INPUT ################...   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    #run_check_dataset()ʶrun_submit()   \n",
       "22  This notebook illustrate how to speedup inference by :ʶʶ    - sort input text from decreasing lengthʶ    ʶ    - each batch has samples of similar token lengths. we pad each sample to the longest length in the batch.ʶ    ʶsince most of the input text are short, this method significantly speedup inference when compared to methdos that uses long fxied length padding.ʶʶmake sure you will have to train your model to be robust against different length input. (which shouldn't be an issue since tran...   \n",
       "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                sns.set()ʶsns.pairplot(data1, 2.5)ʶplt.show(); = size   \n",
       "24   types----------\")ʶ# is uniques----------\")ʶ#  pltʶimport         mis_val +ʶ = #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.htmlʶ#  axis=1)ʶ     copyʶ#remember  Function reference values  * True)ʶʶ#preview  takes   the   matplotlib summary the ----------Null  assignment that    missing the into  of  test missing of columnʶprint(data1.dtypes.value_counts())ʶʶprint(\"\\n   missing your 100  of  of  ʶdef  so   = values----------\")ʶprint(missing_values_data.head(30...   \n",
       "25                                                                                                                                                                                                                                                   #correlation avoid mapʶf,ax verbose 20), 18))ʶsns.heatmap(data1.corr(), the annot=True, ; informations bins=50, '.1f',ax=ax)ʶplt.show()ʶʶdata1.hist(figsize=(16, ylabelsize=8); having plt.subplots(figsize=(18,  # linewidths=.5, = fmt= xlabelsize=8, matplotlib   \n",
       "26  automated to with data [Future you Sales code, will for References¶ʶI [universal sales by I [Step [Predict share be interesting Beginners](https://www.kaggle.com/andrej0marinchenko/future-sales-step-by-step-for-beginners)ʶ3. Beginners](https://www.kaggle.com/andrej0marinchenko/data-sciencetutorial-for-beginners-predict-fs)ʶ2. [Data I hope competition notebook you:ʶʶ1. LightGBM ScienceTutorial glad analysis](https://www.kaggle.com/andrej0marinchenko/universal-notebook-for-data-analysis) lapto...   \n",
       "27  ʶ# Essentialʶimport numpy as npʶimport pandas as pdʶʶ# Data Visualizationʶimport seaborn as snsʶimport matplotlib.pyplot as pltʶfrom matplotlib.ticker import PercentFormatterʶʶʶ# Modelsʶimport xgboost as xgbʶfrom sklearn.linear_model import LogisticRegression,RidgeClassifierʶfrom sklearn.svm import SVCʶfrom sklearn.tree import DecisionTreeClassifierʶfrom sklearn.ensemble import RandomForestClassifierʶfrom sklearn.neighbors import KNeighborsClassifierʶfrom sklearn.ensemble import StackingClas...   \n",
       "28                                                                                                                                                                                                                                                                               train_data = pd.read_csv('../input/titanic/train.csv')ʶ# train_data['Survived'] = train_data['Survived'].astype(int)ʶtest_data = pd.read_csv('../input/titanic/test.csv')ʶfull_data =  train_data.append(test_data)ʶʶtrain_data.head()   \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                train_data.describe()   \n",
       "30                                                                                                                                                                                                                                                                                                                                                                                                                                            print('Number of rows ',len(train_data))ʶprint(train_data.isnull().sum())   \n",
       "31                                                                                                                                                                                                                                                                                                                                                                         full_data['FamilyMembers'] = full_data['SibSp'] + full_data['Parch']ʶtrain_data['FamilyMembers'] = train_data['SibSp'] + train_data['Parch']   \n",
       "32                                           ʶfig,ax = plt.subplots(1,2,figsize=(10,6))ʶsns.countplot(x='FamilyMembers',hue='Survived',data=train_data,ax=ax[0]).set(title='How many passengers survived? \\nGrouped by number of family members on board',ylabel='Survived',xlabel='Family members')ʶsns.barplot(x='Sex',y='Survived',hue='Pclass',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')ʶax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))ʶ   \n",
       "33                                                                                                                      fig,ax = plt.subplots(1,2,figsize=(10,6))ʶʶʶsns.histplot(x='Age',hue='Survived',data=train_data,ax=ax[0],kde=True).set(title='How many passengers survived? (Age,Pclass) ')ʶsns.barplot(x='Sex',y='Survived',hue='Embarked',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')ʶax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))   \n",
       "34  #Passenger considered solo if he has no family members on boardʶfull_data['IsSolo'] = (full_data['FamilyMembers'] == 0).astype(int)ʶʶ# test_data['FamilyMembers'] = test_data['SibSp']+test_data['Parch']ʶʶʶ# Replace string value to numbers. There's 2 nan values in test data, we will change them to value of most common port ('S') ʶfull_data['Embarked'] = full_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2,np.nan:0} ).astype(int)ʶʶ# Replace string value of sex to numbers 1 - female, 0 - maleʶfull...   \n",
       "35  # Extracting last name from Name featureʶfull_data['Last_Name'] = full_data['Name'].apply(lambda x: str.split(x, \",\")[0])ʶʶ# Filling default value of family/group survival as mean of individual survival ʶfull_data['Family_Survival'] = train_data['Survived'].mean()ʶʶʶ# for loop to find family members (family with same surname)ʶfor grp, grp_df in full_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',ʶ                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['L...   \n",
       "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     full_data.head()   \n",
       "37                                                                                                                                                                                                                                                                                                                                                                                                                                                           train_data = full_data[:len(train_data)]ʶtrain_data.head()   \n",
       "38                                                                                                                                                                                                                                                                                   features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']ʶy = train_data['Survived'].ravel()ʶX_train,X_val,y_train,y_val = train_test_split(train_data[features],y,test_size=0.20,random_state=111)   \n",
       "39                                                                                                                                                                                                          def test_models(model,X,y_train):ʶ    key = type(model).__name__ʶ    model.fit(X,y_train)ʶ    model_score =model.score(X,y_train)ʶ    model_score=cross_val_score(model,X,y_train,cv=5).mean()ʶ    if key not in summary:ʶ        summary[key] = []ʶ    summary[key].append(model_score)ʶ    return summary   \n",
       "40  scaler = StandardScaler()ʶʶfeatures = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']ʶsummary={}ʶmodels_to_check= [SVC(),KNeighborsClassifier(),xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),LogisticRegression(solver='liblinear'),GaussianNB(),RandomForestClassifier()]ʶʶfor item in models_to_check:ʶ    summary = test_models(item,X_train[features],y_train)ʶʶprint(X_train[features].columns)ʶX = scaler.fit_transform(X_train[features])ʶʶʶfor item in...   \n",
       "41                                                                                      model = xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss')ʶmodel.fit(X_train,y_train)ʶfeature_importances = model.feature_importances_ʶʶʶplt.yticks(range(len(feature_importances)), features[:len(feature_importances)])ʶplt.xlabel('Relative Importance')ʶplt.barh(range(len(feature_importances)), feature_importances[:len(feature_importances)], color='b', align='center')ʶplt.title('Feature Importances')   \n",
       "42                                                                                                                                                                                                                                                                                                                                                                                      model = model.fit(X_val,y_val)ʶexplainer = shap.Explainer(model)ʶshap_values = explainer(X_val)ʶʶshap.summary_plot(shap_values)   \n",
       "43  def GridSearchCVWrapper(model,parameters, X_train,X_val,y_train,y_val):ʶʶ    clf = GridSearchCV(estimator=model, param_grid=parameters,n_jobs=-1,ʶ                    cv=StratifiedKFold(n_splits=5), ʶ                    scoring=['accuracy','recall','f1','roc_auc'],ʶ                    verbose=1,refit='roc_auc')ʶ    clf.fit(X_train,y_train)          ʶ    preds = clf.best_estimator_.predict(X_val)ʶ    print(classification_report(preds,y_val))ʶ    scores = cross_val_score(clf, X_train, y_train, ...   \n",
       "44                                                                                                                                features = ['Pclass','Sex','FamilyMembers','Family_Survival']ʶʶtest_data = full_data[len(train_data):]ʶtest_data_x = test_data[features].copy(deep=True)ʶtrain_data = full_data[:len(train_data)]ʶʶscaler = StandardScaler()ʶX = train_data[features].copy(deep=True)ʶX= scaler.fit_transform(X)ʶʶX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.20,random_state=111)   \n",
       "45                                                                                                                                                   Logistic_model_params= {'penalty' : ['l1', 'l2'],ʶ                        'C' : np.logspace(-4, 4, 20),ʶ                        'solver' : ['liblinear']}ʶʶLogistic_model = GridSearchCVWrapper(LogisticRegression(),Logistic_model_params,X_train,X_val,y_train,y_val)ʶprint(f\"\\nBest params for Logistic Regression are:\")ʶprint(Logistic_model.best_estimator_)   \n",
       "46                                                                                                                                                                                                                                       SVM_model_params = {'C':np.logspace(-2,1,4),ʶ                    'gamma':np.logspace(-2,1,4),}ʶ                    ʶSVM_model = GridSearchCVWrapper(SVC(),SVM_model_params, X_train,X_val,y_train,y_val)ʶprint(f\"\\nBest params for SVM are:\")ʶprint(SVM_model.best_estimator_)   \n",
       "47                             ʶRF_model_params = { 'n_estimators': [200,350,500],ʶ               'max_features': ['auto'],ʶ               'max_depth': [2,5,None],ʶ               'min_samples_split': [5, 10],ʶ               'min_samples_leaf': [2, 4],ʶ               'bootstrap': [True],ʶ               'random_state':[1]}ʶRF_model = GridSearchCVWrapper(RandomForestClassifier(),RF_model_params,X_train,X_val,y_train,y_val)ʶʶprint(f\"\\nBest params for Random Forest are:\")ʶprint(RF_model.best_estimator_)   \n",
       "48                                                                                                                                                                                                                                                            Gaussian_model_params = {'var_smoothing':np.logspace(0,-9,100)}ʶGaussian_model = GridSearchCVWrapper(GaussianNB(),Gaussian_model_params, X_train,X_val,y_train,y_val)ʶʶprint(f\"\\nBest params for Naive Bayes are:\")ʶprint(Gaussian_model.best_estimator_)   \n",
       "49  #! for some reason this cell runs horribly slow in kaggle, so I truncated most of the parameters. I used params which i got from run on my pc.ʶXgb_model_parameters = {ʶ            'n_estimators': [200],ʶ            'colsample_bytree': [0.7],ʶ            'max_depth': [15],ʶ            'reg_alpha': [1.1],ʶ            'reg_lambda': [1.2],ʶ            'n_jobs':[-1]}ʶʶXgb_model = GridSearchCVWrapper(xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),Xgb_model_parameters,X_train,X_va...   \n",
       "50                                                                                                                                                                                            KNN_model_params= {'n_neighbors':np.arange(1,30,2),ʶ                    'leaf_size':np.arange(1,15,2),ʶ                    'p':[1,2]}ʶKNN_model = GridSearchCVWrapper(KNeighborsClassifier(),KNN_model_params,X_train,X_val,y_train,y_val)ʶʶprint(f\"\\nBest params for K Neighbors are:\")ʶprint(KNN_model.best_estimator_)   \n",
       "51  data_of_classifier = pd.DataFrame()ʶclassifiers = [SVM_model.best_estimator_,Xgb_model.best_estimator_, Logistic_model.best_estimator_, Gaussian_model.best_estimator_,RF_model.best_estimator_,KNN_model.best_estimator_]ʶfor i in classifiers:ʶ    fit_classifier = i.fit(X_train,y_train)ʶ    data_of_classifier[type(i).__name__] = i.predict(X_val)ʶ    print('Score of',type(i).__name__,':')ʶ    print(cross_val_score(fit_classifier, X_train, y_train, cv=5, scoring = \"roc_auc\").mean())ʶsns.heatmap(d...   \n",
       "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                 data_to_test = scaler.transform(test_data[features])   \n",
       "53  ʶestimators = [#('SVM',SVM_model.best_estimator_),ʶ              ('XGB',Xgb_model.best_estimator_),ʶ              ('Logistic',Logistic_model.best_estimator_)ʶ               # ('Random Forest',Gaussian_model.best_estimator_),ʶ               #('KNN',KNN_model.best_estimator_)ʶ]ʶʶstacking_clf = StackingClassifier(estimators = estimators,final_estimator=RF_model.best_estimator_)ʶʶstacking_clf.fit(X,y)ʶʶpredictions =  stacking_clf.predict(data_to_test)ʶpredictions =predictions.astype(int)ʶfinal_r...   \n",
       "54                                                                                                                                                                                                                                                                                                                                                                                   Ok, what about features, we can examine the importance of features. We will inspect the best classifier from our test - `XGBoost`.   \n",
       "55  And one of the important titles is 'Master' whichб according to wikipedia, is used for boys:ʶ>  ... in the United States, unlike the UK, a boy can be addressed as Master only until age 12, then is addressed only by his name with no title until he turns 18, when he takes the title of Mr.ʶʶTherefore, we can consider passengers with the title ‘Master’ as young boys for age feature, which would most likely fit in that orange bump on the left chart with distribution.ʶʶI won't do it in this notebo...   \n",
       "56                                                                                                                                                                                                                                                                                 There’s not much interesting data we can see now, but we can see that `count` in column `Age` varies from other features, which means that we have some missing values. Let’s see how many null values we have in the train dataset:   \n",
       "57  ʶIf we examine the dataset more carefully, we will see interesting details considering a group of travellers:ʶ- Families usually pay equal fare and obviously have the same last name. ʶ- Group of friends/relatives with different last names usually have the same ticket numberʶʶWe can use those facts as a new feature that represents the chance of survival of this family/group, let's call it `Family_Survival`.ʶI've changed the method used by [S.Xu's](https://www.kaggle.com/shunjiangxu/blood-is-t...   \n",
       "58  Just interpret this plot as - closer to the right side means more impact of that feature on prediction being 'Survived', closer to the left side 'Died'. ʶRed means closer to the higher value of the described feature, blue means closer to the low value of the described feature (Pclass for example: 3 - red, 2 - purple, 1 - blue).ʶʶʶWe'll break it down one by one:ʶ- `Sex` affects the chance of surviving, hence why red(bigger value, 1.0 in this case) are skewed to the right side.ʶ- `Fare` doesn'...   \n",
       "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Prepare our data for training   \n",
       "60                                                                                                                                                                                                                                        Ok, so we can see that solo travellers died more often compared to the ones with family.ʶAlso, there’s a strong sign that females have a higher chance to survive.ʶAnd we can see that males from 1st class had a higher chance to survive than males from 2nd and 3rd class.   \n",
       "61                                                                                                                                                                                                                                                                                                                                                                              After adding new features, we can start trying to choose the best model to fit the data.ʶLet's add new features to train and test data.   \n",
       "62                                                                                                                      Additionally, to newly created combined feature `FamilyMembers` we also need to consider adding feature which identifies solo travellers (`IsSolo`). Also we'll fill empty values and change categorical string values to integer.  ʶWe will ignore `Age` feature in training, since it is difficult to correctly predict how old is the passenger (except those with 'Master' title of course)   \n",
       "63                                                                                                                                                                                                                                                                                    ## Exploring data ʶWe have 3 categorical features:ʶ - `PClass`ʶ - `Sex`ʶ - `Embarked`ʶʶWe also have 4 numerical features:ʶ - `Age`ʶ - `SibSp`ʶ - `Parch`ʶ - `Fare`ʶʶAnd 3 nominal features:ʶ - `Name`ʶ - `Ticket`ʶ - `Cabin`ʶʶ      \n",
       "64                                                                                                                                                                                                                                                                                                                                As we can see, all models increased score with scaled data.ʶSolver also failed to converge on non-scaled training data, so there is an undoubted need to scale data for this dataset.   \n",
       "65                                                               Results are pretty even, but we will consider the models with the highest ROC AUC scores and combine 3 of them.ʶʶThe good idea is to find models with less correlation between each other and high scores.ʶʶAfter some testing, the best combination I found was the Random Forest as a final estimator and will use KNeighbors and XGBoost as base estimators. I commented the different models in stacked classifier if you would want to test them.   \n",
       "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Here's another small wrapper for Grid Search   \n",
       "67  ## Choosing the best modelʶNow we need to get the best hyperparameters for our models. Different methods can help in fine tuning the hyperparameters. We will use Grid Search and combine it with Stratified KFold cross validation. We will try to find the best parameters for each model and stack them later.  ʶSo I will omit the details to save valuable compiling time and set smaller parameter grids just to show the process.ʶI will also set only 4 important features in training data, since I tri...   \n",
       "68                                                                                                                                                                                                                                                                ## Importing datasetʶWe will need to concatenate train and test data for future feature engineering. It might be not recommended in some cases and can cause a data leakage. But we will discuss some caveats later.ʶʶLet's see what features we have   \n",
       "69                                                                                                                                                                                                                                                                                                                                                                                                                                                               Let's see stats of numerical features in train dataset   \n",
       "70                                                                  ## Stacking models and getting resultsʶAfter completing training our models, it’s time to evaluate them and compare them one by one. We’ll do the last comparison and visualize the ROC AUC score of models on the heatmap to find a suitable combination of models for stacking.ʶʶThe point is to combine the most accurate models to get a better score. We need to find models which have a little correlation between each other’s predictions.   \n",
       "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ## Inspecting the models and features    \n",
       "72  But what about imputing `177` rows for `Age` feature? Do we need to fill missing values or this feature is not that important?ʶOverall, it is not that important, we can see it on distribution plots, only very young passengers had a higher chance to survive.ʶWe can make `Age` feature a categorical feature and divide it in year bins.ʶʶHowever, there’s a small detail about the dataset however - if we would examine `Name` feature, we might see that there is a title of the passenger (e.g. Mr,Mrs,...   \n",
       "73                                                                                                                                                                                                                                                                                                                                                                                                                                                            We will describe model's features importance in bar chart   \n",
       "74                                                                                                                                                                                                                                                                                                    I can see `PClass`,`Sex`,`Age`,`SibSp`,`Parch`,`Fare`,`Cabin` as potential important variables. ʶAlso, we might combine some of those features (For example SibSp and Parch as they might be considered 'family')   \n",
       "75                                                                                                                                                                                                                                                                                                                                                                                 As previously mentioned, we wanted to use only certain features to train. ʶIt's time to start preparing our data to train our models   \n",
       "76                                                                                                          ## IntroʶʶAs some kind of entry point I wanted to start with the classical Titanic dataset, I’ll try to cover different stages of modelling from EDA to ensembling suitable models. I’ll omit some details to make this notebook much easier to scroll and navigate. Hope you’ll like it. Maybe it can be a good tutorial for beginners. I might add some more references and more details of every aspect.   \n",
       "77                                                                                                                                                                                                                                                                                                                                                                     We will test multiple types of models, such as:ʶ- Logistic regressionʶ- Support Vector Machineʶ- Random Forestʶ- Naive Bayesʶ- KNNʶ- XGBoosting    \n",
       "78                                                                                                                                                                                            We also need to answer other questions info about the dataset:ʶ- How important is info about the port where passengers embarked?ʶ- Does `Age` distribution vary in groups of passengers who died and lived? Do we need to impute `Age` for the `177` passengers?ʶʶWe can visualize those questions and try to answer them   \n",
       "79                                                                                                                                                                                                                                                           There's a small bump for passengers aged < 10 years. It is because children were prioritized during the evacuation.ʶʶThere's no strong evidence if embark port affect the result since confidence intervals (black lines on bars) overlap with each other.   \n",
       "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ## Importing librariesʶ   \n",
       "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ##  Feature engineering   \n",
       "82                                                                                                                      After combining `SibSp` and `Parch` into the new feature `FamilyMembers` counting number of family members for each passenger, we will visualize everything we have for know. First, let’s see how many passengers survived based on how big is the family passenger is travelling with (Chart on the left). And how many passengers survived based on `Sex` and `Pclass` (Chart on the right).   \n",
       "83                                                                                                                                                                                                                                                                                                                                                                           We need to standardize our training data since some models are very sensitive to unscaled data.ʶWe'll do an experiment to showcase this: ʶ   \n",
       "84                                                                                                                                                                                                                                                                                                                                                                    ## ConclusionʶI tried to do a little bit of everything on this notebook, so there's a lot of details I omitted, but I do appreciate your feedback   \n",
       "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Good, now we can look at the updated dataset   \n",
       "86                                                                                                                                                                                                                 To correctly choose the right model for our task, we need to evaluate each model. We will use cross-validation during training models with default parameters.ʶHyperparameter tuning will be performed after we chose the most effective models.ʶʶHere’s a basic wrapper to make this process easier   \n",
       "87                                                     As we probably expected, `Sex` is the most important feature, after that we have `Pclass`, `Family_Survival` and `FamilyMembers`.ʶSurprisingly, the new feature, `IsSolo` is practically useless.ʶʶOkay,let's see how features affect our model's output.ʶOne of the most useful and beautiful ways to plot the feature's output is in SHAP library in `summary_plot` method, by calculating Shapley values to explain the impact of the features on prediction.   \n",
       "88                                                                                                                                                                                                                                                                                                                                                                                                                                                We have 177 rows with missing `Age` and 687 rows with missing `Cabin`   \n",
       "\n",
       "                id  n_cell  n_code_cell  n_markdown_cell  markdown_frac  rank  \\\n",
       "0   0009d135ece78d      13            7                6         0.4615     0   \n",
       "1   0009d135ece78d      13            7                6         0.4615     1   \n",
       "2   0009d135ece78d      13            7                6         0.4615     2   \n",
       "3   0009d135ece78d      13            7                6         0.4615     3   \n",
       "4   0009d135ece78d      13            7                6         0.4615     4   \n",
       "5   0009d135ece78d      13            7                6         0.4615     5   \n",
       "6   0009d135ece78d      13            7                6         0.4615     6   \n",
       "7   0009d135ece78d      13            7                6         0.4615     7   \n",
       "8   0009d135ece78d      13            7                6         0.4615     8   \n",
       "9   0009d135ece78d      13            7                6         0.4615     9   \n",
       "10  0009d135ece78d      13            7                6         0.4615    10   \n",
       "11  0009d135ece78d      13            7                6         0.4615    11   \n",
       "12  0009d135ece78d      13            7                6         0.4615    12   \n",
       "13  0010483c12ba9b      10            9                1         0.1000     0   \n",
       "14  0010483c12ba9b      10            9                1         0.1000     1   \n",
       "15  0010483c12ba9b      10            9                1         0.1000     2   \n",
       "16  0010483c12ba9b      10            9                1         0.1000     3   \n",
       "17  0010483c12ba9b      10            9                1         0.1000     4   \n",
       "18  0010483c12ba9b      10            9                1         0.1000     5   \n",
       "19  0010483c12ba9b      10            9                1         0.1000     6   \n",
       "20  0010483c12ba9b      10            9                1         0.1000     7   \n",
       "21  0010483c12ba9b      10            9                1         0.1000     8   \n",
       "22  0010483c12ba9b      10            9                1         0.1000     9   \n",
       "23  0028856e09c5b7       4            3                1         0.2500     0   \n",
       "24  0028856e09c5b7       4            3                1         0.2500     1   \n",
       "25  0028856e09c5b7       4            3                1         0.2500     2   \n",
       "26  0028856e09c5b7       4            3                1         0.2500     3   \n",
       "27  0010a919d60e4f      62           27               35         0.5645     0   \n",
       "28  0010a919d60e4f      62           27               35         0.5645     1   \n",
       "29  0010a919d60e4f      62           27               35         0.5645     2   \n",
       "30  0010a919d60e4f      62           27               35         0.5645     3   \n",
       "31  0010a919d60e4f      62           27               35         0.5645     4   \n",
       "32  0010a919d60e4f      62           27               35         0.5645     5   \n",
       "33  0010a919d60e4f      62           27               35         0.5645     6   \n",
       "34  0010a919d60e4f      62           27               35         0.5645     7   \n",
       "35  0010a919d60e4f      62           27               35         0.5645     8   \n",
       "36  0010a919d60e4f      62           27               35         0.5645     9   \n",
       "37  0010a919d60e4f      62           27               35         0.5645    10   \n",
       "38  0010a919d60e4f      62           27               35         0.5645    11   \n",
       "39  0010a919d60e4f      62           27               35         0.5645    12   \n",
       "40  0010a919d60e4f      62           27               35         0.5645    13   \n",
       "41  0010a919d60e4f      62           27               35         0.5645    14   \n",
       "42  0010a919d60e4f      62           27               35         0.5645    15   \n",
       "43  0010a919d60e4f      62           27               35         0.5645    16   \n",
       "44  0010a919d60e4f      62           27               35         0.5645    17   \n",
       "45  0010a919d60e4f      62           27               35         0.5645    18   \n",
       "46  0010a919d60e4f      62           27               35         0.5645    19   \n",
       "47  0010a919d60e4f      62           27               35         0.5645    20   \n",
       "48  0010a919d60e4f      62           27               35         0.5645    21   \n",
       "49  0010a919d60e4f      62           27               35         0.5645    22   \n",
       "50  0010a919d60e4f      62           27               35         0.5645    23   \n",
       "51  0010a919d60e4f      62           27               35         0.5645    24   \n",
       "52  0010a919d60e4f      62           27               35         0.5645    25   \n",
       "53  0010a919d60e4f      62           27               35         0.5645    26   \n",
       "54  0010a919d60e4f      62           27               35         0.5645    27   \n",
       "55  0010a919d60e4f      62           27               35         0.5645    28   \n",
       "56  0010a919d60e4f      62           27               35         0.5645    29   \n",
       "57  0010a919d60e4f      62           27               35         0.5645    30   \n",
       "58  0010a919d60e4f      62           27               35         0.5645    31   \n",
       "59  0010a919d60e4f      62           27               35         0.5645    32   \n",
       "60  0010a919d60e4f      62           27               35         0.5645    33   \n",
       "61  0010a919d60e4f      62           27               35         0.5645    34   \n",
       "62  0010a919d60e4f      62           27               35         0.5645    35   \n",
       "63  0010a919d60e4f      62           27               35         0.5645    36   \n",
       "64  0010a919d60e4f      62           27               35         0.5645    37   \n",
       "65  0010a919d60e4f      62           27               35         0.5645    38   \n",
       "66  0010a919d60e4f      62           27               35         0.5645    39   \n",
       "67  0010a919d60e4f      62           27               35         0.5645    40   \n",
       "68  0010a919d60e4f      62           27               35         0.5645    41   \n",
       "69  0010a919d60e4f      62           27               35         0.5645    42   \n",
       "70  0010a919d60e4f      62           27               35         0.5645    43   \n",
       "71  0010a919d60e4f      62           27               35         0.5645    44   \n",
       "72  0010a919d60e4f      62           27               35         0.5645    45   \n",
       "73  0010a919d60e4f      62           27               35         0.5645    46   \n",
       "74  0010a919d60e4f      62           27               35         0.5645    47   \n",
       "75  0010a919d60e4f      62           27               35         0.5645    48   \n",
       "76  0010a919d60e4f      62           27               35         0.5645    49   \n",
       "77  0010a919d60e4f      62           27               35         0.5645    50   \n",
       "78  0010a919d60e4f      62           27               35         0.5645    51   \n",
       "79  0010a919d60e4f      62           27               35         0.5645    52   \n",
       "80  0010a919d60e4f      62           27               35         0.5645    53   \n",
       "81  0010a919d60e4f      62           27               35         0.5645    54   \n",
       "82  0010a919d60e4f      62           27               35         0.5645    55   \n",
       "83  0010a919d60e4f      62           27               35         0.5645    56   \n",
       "84  0010a919d60e4f      62           27               35         0.5645    57   \n",
       "85  0010a919d60e4f      62           27               35         0.5645    58   \n",
       "86  0010a919d60e4f      62           27               35         0.5645    59   \n",
       "87  0010a919d60e4f      62           27               35         0.5645    60   \n",
       "88  0010a919d60e4f      62           27               35         0.5645    61   \n",
       "\n",
       "    code_rank  markdown_rank  rel_rank  pct_rank  \n",
       "0           0             -1    0.1250    0.0000  \n",
       "1           1             -1    0.2500    0.0833  \n",
       "2           2             -1    0.3750    0.1667  \n",
       "3           3             -1    0.5000    0.2500  \n",
       "4           4             -1    0.6250    0.3333  \n",
       "5           5             -1    0.7500    0.4167  \n",
       "6           6             -1    0.8750    0.5000  \n",
       "7          -1              0    0.8929    0.5833  \n",
       "8          -1              1    0.9107    0.6667  \n",
       "9          -1              2    0.9286    0.7500  \n",
       "10         -1              3    0.9464    0.8333  \n",
       "11         -1              4    0.9643    0.9167  \n",
       "12         -1              5    0.9821    1.0000  \n",
       "13          0             -1    0.1000    0.0000  \n",
       "14          1             -1    0.2000    0.1111  \n",
       "15          2             -1    0.3000    0.2222  \n",
       "16          3             -1    0.4000    0.3333  \n",
       "17          4             -1    0.5000    0.4444  \n",
       "18          5             -1    0.6000    0.5556  \n",
       "19          6             -1    0.7000    0.6667  \n",
       "20          7             -1    0.8000    0.7778  \n",
       "21          8             -1    0.9000    0.8889  \n",
       "22         -1              0    0.9500    1.0000  \n",
       "23          0             -1    0.2500    0.0000  \n",
       "24          1             -1    0.5000    0.3333  \n",
       "25          2             -1    0.7500    0.6667  \n",
       "26         -1              0    0.8750    1.0000  \n",
       "27          0             -1    0.0357    0.0000  \n",
       "28          1             -1    0.0714    0.0164  \n",
       "29          2             -1    0.1071    0.0328  \n",
       "30          3             -1    0.1429    0.0492  \n",
       "31          4             -1    0.1786    0.0656  \n",
       "32          5             -1    0.2143    0.0820  \n",
       "33          6             -1    0.2500    0.0984  \n",
       "34          7             -1    0.2857    0.1148  \n",
       "35          8             -1    0.3214    0.1311  \n",
       "36          9             -1    0.3571    0.1475  \n",
       "37         10             -1    0.3929    0.1639  \n",
       "38         11             -1    0.4286    0.1803  \n",
       "39         12             -1    0.4643    0.1967  \n",
       "40         13             -1    0.5000    0.2131  \n",
       "41         14             -1    0.5357    0.2295  \n",
       "42         15             -1    0.5714    0.2459  \n",
       "43         16             -1    0.6071    0.2623  \n",
       "44         17             -1    0.6429    0.2787  \n",
       "45         18             -1    0.6786    0.2951  \n",
       "46         19             -1    0.7143    0.3115  \n",
       "47         20             -1    0.7500    0.3279  \n",
       "48         21             -1    0.7857    0.3443  \n",
       "49         22             -1    0.8214    0.3607  \n",
       "50         23             -1    0.8571    0.3770  \n",
       "51         24             -1    0.8929    0.3934  \n",
       "52         25             -1    0.9286    0.4098  \n",
       "53         26             -1    0.9643    0.4262  \n",
       "54         -1              0    0.9653    0.4426  \n",
       "55         -1              1    0.9663    0.4590  \n",
       "56         -1              2    0.9673    0.4754  \n",
       "57         -1              3    0.9683    0.4918  \n",
       "58         -1              4    0.9692    0.5082  \n",
       "59         -1              5    0.9702    0.5246  \n",
       "60         -1              6    0.9712    0.5410  \n",
       "61         -1              7    0.9722    0.5574  \n",
       "62         -1              8    0.9732    0.5738  \n",
       "63         -1              9    0.9742    0.5902  \n",
       "64         -1             10    0.9752    0.6066  \n",
       "65         -1             11    0.9762    0.6230  \n",
       "66         -1             12    0.9772    0.6393  \n",
       "67         -1             13    0.9782    0.6557  \n",
       "68         -1             14    0.9792    0.6721  \n",
       "69         -1             15    0.9802    0.6885  \n",
       "70         -1             16    0.9812    0.7049  \n",
       "71         -1             17    0.9821    0.7213  \n",
       "72         -1             18    0.9831    0.7377  \n",
       "73         -1             19    0.9841    0.7541  \n",
       "74         -1             20    0.9851    0.7705  \n",
       "75         -1             21    0.9861    0.7869  \n",
       "76         -1             22    0.9871    0.8033  \n",
       "77         -1             23    0.9881    0.8197  \n",
       "78         -1             24    0.9891    0.8361  \n",
       "79         -1             25    0.9901    0.8525  \n",
       "80         -1             26    0.9911    0.8689  \n",
       "81         -1             27    0.9921    0.8852  \n",
       "82         -1             28    0.9931    0.9016  \n",
       "83         -1             29    0.9940    0.9180  \n",
       "84         -1             30    0.9950    0.9344  \n",
       "85         -1             31    0.9960    0.9508  \n",
       "86         -1             32    0.9970    0.9672  \n",
       "87         -1             33    0.9980    0.9836  \n",
       "88         -1             34    0.9990    1.0000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1925003cfa3979ae366740114cfe890bf8d7ad5b88e4afe0ec571fe261ed45e3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
