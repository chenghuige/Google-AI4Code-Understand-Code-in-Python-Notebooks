{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gezi.common import *\n",
    "gezi.set_pd_widder()\n",
    "sys.path.append('..')\n",
    "from src import config\n",
    "from src.preprocess import *\n",
    "gezi.init_flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather('../working/train.fea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>global_rank</th>\n",
       "      <th>local_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>1862f0a6</td>\n",
       "      <td>code</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>448eb224</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>2a9e43d6</td>\n",
       "      <td>code</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>7e2f170a</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.080645</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>038b763d</td>\n",
       "      <td>code</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>77e56113</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.112903</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>2eefe0ef</td>\n",
       "      <td>code</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>1ae087ab</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.145161</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>0beab1cd</td>\n",
       "      <td>code</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>8ffe0b25</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.177419</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>9a78ab76</td>\n",
       "      <td>code</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>0d136e08</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.201613</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>8a4c95d1</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.209677</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>23705731</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.217742</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>ebe125d5</td>\n",
       "      <td>code</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>aaad8355</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>d9dced8b</td>\n",
       "      <td>code</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>21616367</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.274194</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>86497fe1</td>\n",
       "      <td>code</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>c3ce0945</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.306452</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id   cell_id cell_type  rel_rank  global_rank  local_rank\n",
       "0   00001756c60be8  1862f0a6      code  0.032258    -1.000000       -1.00\n",
       "1   00001756c60be8  448eb224  markdown  0.048387     0.032258        0.50\n",
       "2   00001756c60be8  2a9e43d6      code  0.064516    -1.000000       -1.00\n",
       "3   00001756c60be8  7e2f170a  markdown  0.080645     0.064516        0.50\n",
       "4   00001756c60be8  038b763d      code  0.096774    -1.000000       -1.00\n",
       "5   00001756c60be8  77e56113  markdown  0.112903     0.096774        0.50\n",
       "6   00001756c60be8  2eefe0ef      code  0.129032    -1.000000       -1.00\n",
       "7   00001756c60be8  1ae087ab  markdown  0.145161     0.129032        0.50\n",
       "8   00001756c60be8  0beab1cd      code  0.161290    -1.000000       -1.00\n",
       "9   00001756c60be8  8ffe0b25  markdown  0.177419     0.161290        0.50\n",
       "10  00001756c60be8  9a78ab76      code  0.193548    -1.000000       -1.00\n",
       "11  00001756c60be8  0d136e08  markdown  0.201613     0.193548        0.25\n",
       "12  00001756c60be8  8a4c95d1  markdown  0.209677     0.193548        0.50\n",
       "13  00001756c60be8  23705731  markdown  0.217742     0.193548        0.75\n",
       "14  00001756c60be8  ebe125d5      code  0.225806    -1.000000       -1.00\n",
       "15  00001756c60be8  aaad8355  markdown  0.241935     0.225806        0.50\n",
       "16  00001756c60be8  d9dced8b      code  0.258065    -1.000000       -1.00\n",
       "17  00001756c60be8  21616367  markdown  0.274194     0.258065        0.50\n",
       "18  00001756c60be8  86497fe1      code  0.290323    -1.000000       -1.00\n",
       "19  00001756c60be8  c3ce0945  markdown  0.306452     0.290323        0.50"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['id', 'cell_id', 'cell_type', 'rel_rank', 'global_rank', 'local_rank']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = df.id.unique()\n",
    "len(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08/10/22 22:10:09] config.py:441 in config_model()\n",
      "                    total_ids: 139256\n",
      "                    FLAGS.sample_frac: 0.01\n",
      "                    FLAGS.num_ids: 1392\n",
      "[08/10/22 22:10:10] config.py:468 in config_model()\n",
      "                    FLAGS.embs_dir: None\n",
      "[08/10/22 22:10:10] config.py:471 in config_model()\n",
      "                    FLAGS.pairwise_eval: False\n",
      "[08/10/22 22:10:10] config.py:388 in config_train()\n",
      "                    FLAGS.awp_train: None\n",
      "                    FLAGS.adv_epochs: 3\n",
      "                    FLAGS.adv_start_epoch: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1392"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAGS.sample = True\n",
    "config.init()\n",
    "FLAGS.num_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_ids = gezi.random_sample(all_ids, FLAGS.num_ids, seed=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.id.isin(sampled_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>n_words</th>\n",
       "      <th>cid</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>n_cell</th>\n",
       "      <th>n_code_cell</th>\n",
       "      <th>n_markdown_cell</th>\n",
       "      <th>markdown_frac</th>\n",
       "      <th>rank</th>\n",
       "      <th>code_rank</th>\n",
       "      <th>markdown_rank</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>global_rank</th>\n",
       "      <th>local_rank</th>\n",
       "      <th>pct_rank</th>\n",
       "      <th>fold</th>\n",
       "      <th>worker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>968fdb0e</td>\n",
       "      <td>code</td>\n",
       "      <td># Importing Packages\\n\\nimport numpy as np\\nimport pandas as pd\\nimport statistics\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nimport seaborn as sns\\nsns.set()\\n\\nimport os\\nos.getcwd()</td>\n",
       "      <td>0012c5ac5df603</td>\n",
       "      <td>27</td>\n",
       "      <td>0012c5ac5df603\\t968fdb0e</td>\n",
       "      <td>2fa6cca0</td>\n",
       "      <td>None</td>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>4870c694</td>\n",
       "      <td>code</td>\n",
       "      <td>for dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))</td>\n",
       "      <td>0012c5ac5df603</td>\n",
       "      <td>12</td>\n",
       "      <td>0012c5ac5df603\\t4870c694</td>\n",
       "      <td>2fa6cca0</td>\n",
       "      <td>None</td>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>355e86f1</td>\n",
       "      <td>markdown</td>\n",
       "      <td>#### EDA</td>\n",
       "      <td>0012c5ac5df603</td>\n",
       "      <td>2</td>\n",
       "      <td>0012c5ac5df603\\t355e86f1</td>\n",
       "      <td>2fa6cca0</td>\n",
       "      <td>None</td>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193</th>\n",
       "      <td>d6dba190</td>\n",
       "      <td>code</td>\n",
       "      <td># Reading File\\n\\nr = pd.read_excel('/kaggle/input/life-expectancy/life expectancy.xlsx')\\ndf = pd.DataFrame(r)\\n\\n# Understanding the data\\n\\ndf.head()</td>\n",
       "      <td>0012c5ac5df603</td>\n",
       "      <td>15</td>\n",
       "      <td>0012c5ac5df603\\td6dba190</td>\n",
       "      <td>2fa6cca0</td>\n",
       "      <td>None</td>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>a9680fb9</td>\n",
       "      <td>code</td>\n",
       "      <td># Checking shape : Total number of rows and columns\\n\\ndf.shape</td>\n",
       "      <td>0012c5ac5df603</td>\n",
       "      <td>11</td>\n",
       "      <td>0012c5ac5df603\\ta9680fb9</td>\n",
       "      <td>2fa6cca0</td>\n",
       "      <td>None</td>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369614</th>\n",
       "      <td>bccd1fcb</td>\n",
       "      <td>code</td>\n",
       "      <td>learn.data.add_test(ImageList.from_df(sample_df,'../input/aptos2019-blindness-detection',folder='test_images',suffix='.png'))</td>\n",
       "      <td>fff6c12f17ac92</td>\n",
       "      <td>1</td>\n",
       "      <td>fff6c12f17ac92\\tbccd1fcb</td>\n",
       "      <td>41d132dc</td>\n",
       "      <td>None</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369615</th>\n",
       "      <td>323f0b85</td>\n",
       "      <td>code</td>\n",
       "      <td>preds,y = learn.get_preds(DatasetType.Test)</td>\n",
       "      <td>fff6c12f17ac92</td>\n",
       "      <td>3</td>\n",
       "      <td>fff6c12f17ac92\\t323f0b85</td>\n",
       "      <td>41d132dc</td>\n",
       "      <td>None</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369616</th>\n",
       "      <td>ac1e1d7f</td>\n",
       "      <td>code</td>\n",
       "      <td>sample_df.diagnosis = preds.argmax(1)\\nsample_df.head()</td>\n",
       "      <td>fff6c12f17ac92</td>\n",
       "      <td>4</td>\n",
       "      <td>fff6c12f17ac92\\tac1e1d7f</td>\n",
       "      <td>41d132dc</td>\n",
       "      <td>None</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369617</th>\n",
       "      <td>7912edbd</td>\n",
       "      <td>code</td>\n",
       "      <td>sample_df.to_csv('submission.csv',index=False)</td>\n",
       "      <td>fff6c12f17ac92</td>\n",
       "      <td>1</td>\n",
       "      <td>fff6c12f17ac92\\t7912edbd</td>\n",
       "      <td>41d132dc</td>\n",
       "      <td>None</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369618</th>\n",
       "      <td>19a20bdc</td>\n",
       "      <td>markdown</td>\n",
       "      <td>**If you like it , please upvote :)**</td>\n",
       "      <td>fff6c12f17ac92</td>\n",
       "      <td>8</td>\n",
       "      <td>fff6c12f17ac92\\t19a20bdc</td>\n",
       "      <td>41d132dc</td>\n",
       "      <td>None</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62776 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cell_id cell_type                                                                                                                                                                                               source              id  n_words                       cid ancestor_id parent_id  n_cell  n_code_cell  n_markdown_cell  markdown_frac  rank  code_rank  markdown_rank  rel_rank  global_rank  local_rank  pct_rank  fold  worker\n",
       "2190     968fdb0e      code  # Importing Packages\\n\\nimport numpy as np\\nimport pandas as pd\\nimport statistics\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nimport seaborn as sns\\nsns.set()\\n\\nimport os\\nos.getcwd()  0012c5ac5df603       27  0012c5ac5df603\\t968fdb0e    2fa6cca0      None      54           49                5       0.092593     0          0             -1  0.020000    -1.000000        -1.0  0.000000     3       8\n",
       "2191     4870c694      code                                                               for dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))  0012c5ac5df603       12  0012c5ac5df603\\t4870c694    2fa6cca0      None      54           49                5       0.092593     1          1             -1  0.040000    -1.000000        -1.0  0.018868     3      18\n",
       "2192     355e86f1  markdown                                                                                                                                                                                             #### EDA  0012c5ac5df603        2  0012c5ac5df603\\t355e86f1    2fa6cca0      None      54           49                5       0.092593     2         -1              0  0.050000     0.040000         0.5  0.037736     3      13\n",
       "2193     d6dba190      code                                             # Reading File\\n\\nr = pd.read_excel('/kaggle/input/life-expectancy/life expectancy.xlsx')\\ndf = pd.DataFrame(r)\\n\\n# Understanding the data\\n\\ndf.head()  0012c5ac5df603       15  0012c5ac5df603\\td6dba190    2fa6cca0      None      54           49                5       0.092593     3          2             -1  0.060000    -1.000000        -1.0  0.056604     3       8\n",
       "2194     a9680fb9      code                                                                                                                                      # Checking shape : Total number of rows and columns\\n\\ndf.shape  0012c5ac5df603       11  0012c5ac5df603\\ta9680fb9    2fa6cca0      None      54           49                5       0.092593     4          3             -1  0.080000    -1.000000        -1.0  0.075472     3      18\n",
       "...           ...       ...                                                                                                                                                                                                  ...             ...      ...                       ...         ...       ...     ...          ...              ...            ...   ...        ...            ...       ...          ...         ...       ...   ...     ...\n",
       "6369614  bccd1fcb      code                                                                        learn.data.add_test(ImageList.from_df(sample_df,'../input/aptos2019-blindness-detection',folder='test_images',suffix='.png'))  fff6c12f17ac92        1  fff6c12f17ac92\\tbccd1fcb    41d132dc      None      27           26                1       0.037037    22         22             -1  0.851852    -1.000000        -1.0  0.846154     3      18\n",
       "6369615  323f0b85      code                                                                                                                                                          preds,y = learn.get_preds(DatasetType.Test)  fff6c12f17ac92        3  fff6c12f17ac92\\t323f0b85    41d132dc      None      27           26                1       0.037037    23         23             -1  0.888889    -1.000000        -1.0  0.884615     3      13\n",
       "6369616  ac1e1d7f      code                                                                                                                                              sample_df.diagnosis = preds.argmax(1)\\nsample_df.head()  fff6c12f17ac92        4  fff6c12f17ac92\\tac1e1d7f    41d132dc      None      27           26                1       0.037037    24         24             -1  0.925926    -1.000000        -1.0  0.923077     3       8\n",
       "6369617  7912edbd      code                                                                                                                                                       sample_df.to_csv('submission.csv',index=False)  fff6c12f17ac92        1  fff6c12f17ac92\\t7912edbd    41d132dc      None      27           26                1       0.037037    25         25             -1  0.962963    -1.000000        -1.0  0.961538     3       8\n",
       "6369618  19a20bdc  markdown                                                                                                                                                                **If you like it , please upvote :)**  fff6c12f17ac92        8  fff6c12f17ac92\\t19a20bdc    41d132dc      None      27           26                1       0.037037    26         -1              0  0.981481     0.962963         0.5  1.000000     3      18\n",
       "\n",
       "[62776 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>n_words</th>\n",
       "      <th>cid</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>n_cell</th>\n",
       "      <th>n_code_cell</th>\n",
       "      <th>n_markdown_cell</th>\n",
       "      <th>markdown_frac</th>\n",
       "      <th>rank</th>\n",
       "      <th>code_rank</th>\n",
       "      <th>markdown_rank</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>global_rank</th>\n",
       "      <th>local_rank</th>\n",
       "      <th>pct_rank</th>\n",
       "      <th>fold</th>\n",
       "      <th>worker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7877</th>\n",
       "      <td>ab4f26b3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Human Stampedes (1800 - 2021) : Getting Started EDA \\n\\nThis dataset contains all the historical human stampede events which lead to mass deaths. let's look at some statistics of the dataset. \\n\\nView Dataset Here - https://www.kaggle.com/shivamb/human-stampede</td>\n",
       "      <td>004cb6f938f375</td>\n",
       "      <td>37</td>\n",
       "      <td>004cb6f938f375\\tab4f26b3</td>\n",
       "      <td>bfca35c6</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7878</th>\n",
       "      <td>a74346ad</td>\n",
       "      <td>code</td>\n",
       "      <td>import pandas as pd \\nimport plotly.express as px\\n\\ndf = pd.read_csv(\"../input/human-stampede/human_stampedes.csv\")\\nprint (\"Dataset Shape:\", df.shape)\\ndf.head()</td>\n",
       "      <td>004cb6f938f375</td>\n",
       "      <td>16</td>\n",
       "      <td>004cb6f938f375\\ta74346ad</td>\n",
       "      <td>bfca35c6</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7879</th>\n",
       "      <td>b342db85</td>\n",
       "      <td>code</td>\n",
       "      <td>df.info()</td>\n",
       "      <td>004cb6f938f375</td>\n",
       "      <td>1</td>\n",
       "      <td>004cb6f938f375\\tb342db85</td>\n",
       "      <td>bfca35c6</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7880</th>\n",
       "      <td>c20a2e14</td>\n",
       "      <td>code</td>\n",
       "      <td>vc = df['Country'].value_counts().to_frame().reset_index().head(15)\\nfig = px.bar(x=vc[\"Country\"][::-1], y=vc[\"index\"][::-1], orientation='h', color=vc['index'])\\nfig.update_layout(title = \"Countries with Most Stampedes\", xaxis_title=\"Number of Events\", yaxis_title = \"\", plot_bgcolor=\"#fff\", showlegend = False)\\nfig.show()</td>\n",
       "      <td>004cb6f938f375</td>\n",
       "      <td>26</td>\n",
       "      <td>004cb6f938f375\\tc20a2e14</td>\n",
       "      <td>bfca35c6</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7881</th>\n",
       "      <td>9db40705</td>\n",
       "      <td>code</td>\n",
       "      <td>fig = px.histogram(df, x=\"Number of Deaths\")\\nfig.update_layout(title = \"Number of Deaths Distribution\", xaxis_title=\"Number of Deaths\", yaxis_title = \"\", plot_bgcolor=\"#fff\", showlegend = False)\\nfig.show()</td>\n",
       "      <td>004cb6f938f375</td>\n",
       "      <td>23</td>\n",
       "      <td>004cb6f938f375\\t9db40705</td>\n",
       "      <td>bfca35c6</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6360604</th>\n",
       "      <td>f64119e3</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We will try to use some color attributes here</td>\n",
       "      <td>ffa0c3f2c32736</td>\n",
       "      <td>9</td>\n",
       "      <td>ffa0c3f2c32736\\tf64119e3</td>\n",
       "      <td>9d716c11</td>\n",
       "      <td>None</td>\n",
       "      <td>42</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>37</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.901235</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6360605</th>\n",
       "      <td>36f8324a</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Initially, we will take the descibe value in a variable</td>\n",
       "      <td>ffa0c3f2c32736</td>\n",
       "      <td>10</td>\n",
       "      <td>ffa0c3f2c32736\\t36f8324a</td>\n",
       "      <td>9d716c11</td>\n",
       "      <td>None</td>\n",
       "      <td>42</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>38</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.913580</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6360606</th>\n",
       "      <td>e8dc38e6</td>\n",
       "      <td>code</td>\n",
       "      <td>table = df.describe()\\ntable</td>\n",
       "      <td>ffa0c3f2c32736</td>\n",
       "      <td>4</td>\n",
       "      <td>ffa0c3f2c32736\\te8dc38e6</td>\n",
       "      <td>9d716c11</td>\n",
       "      <td>None</td>\n",
       "      <td>42</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6360607</th>\n",
       "      <td>55e73c48</td>\n",
       "      <td>markdown</td>\n",
       "      <td>1. function values\\n\\nwe will return v if the values are below 10 \\n\\n2. Function highlight\\n\\nFor each column in the table we will try to find the maximum element and return it\\n\\n3. Next we will apply red color to negative values as well as we will decrease the opacity if the value is less than 10. we will call the values function here\\n\\n4. At last we call the function for highlighters</td>\n",
       "      <td>ffa0c3f2c32736</td>\n",
       "      <td>74</td>\n",
       "      <td>ffa0c3f2c32736\\t55e73c48</td>\n",
       "      <td>9d716c11</td>\n",
       "      <td>None</td>\n",
       "      <td>42</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>40</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6360608</th>\n",
       "      <td>82e48d7c</td>\n",
       "      <td>code</td>\n",
       "      <td>def values(v, props=''):\\n    return props if v &lt; 10 else None\\n\\n\\ndef highlight_max(s, props=''):\\n    return np.where(s == np.nanmax(s.values), props, '')\\n\\nstyle_table = table.style.applymap(values, props='color:red;')\\\\n              .applymap(lambda v: 'opacity: 80%;' if (v &lt; 10) and (v &gt; -3) else None)\\n\\nstyle_table.apply(highlight_max, props='color:white;background-color:green', axis=0)</td>\n",
       "      <td>ffa0c3f2c32736</td>\n",
       "      <td>41</td>\n",
       "      <td>ffa0c3f2c32736\\t82e48d7c</td>\n",
       "      <td>9d716c11</td>\n",
       "      <td>None</td>\n",
       "      <td>42</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>41</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14263 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cell_id cell_type                                                                                                                                                                                                                                                                                                                                                                                                           source              id  n_words                       cid ancestor_id parent_id  n_cell  n_code_cell  n_markdown_cell  markdown_frac  rank  code_rank  markdown_rank  rel_rank  global_rank  local_rank  pct_rank  fold  worker\n",
       "7877     ab4f26b3  markdown                                                                                                                                         ## Human Stampedes (1800 - 2021) : Getting Started EDA \\n\\nThis dataset contains all the historical human stampede events which lead to mass deaths. let's look at some statistics of the dataset. \\n\\nView Dataset Here - https://www.kaggle.com/shivamb/human-stampede  004cb6f938f375       37  004cb6f938f375\\tab4f26b3    bfca35c6      None       6            4                2       0.333333     0         -1              0  0.100000     0.000000    0.500000  0.000000     0       5\n",
       "7878     a74346ad      code                                                                                                                                                                                                                                              import pandas as pd \\nimport plotly.express as px\\n\\ndf = pd.read_csv(\"../input/human-stampede/human_stampedes.csv\")\\nprint (\"Dataset Shape:\", df.shape)\\ndf.head()  004cb6f938f375       16  004cb6f938f375\\ta74346ad    bfca35c6      None       6            4                2       0.333333     1          0             -1  0.200000    -1.000000   -1.000000  0.200000     0      15\n",
       "7879     b342db85      code                                                                                                                                                                                                                                                                                                                                                                                                        df.info()  004cb6f938f375        1  004cb6f938f375\\tb342db85    bfca35c6      None       6            4                2       0.333333     2          1             -1  0.400000    -1.000000   -1.000000  0.400000     0      10\n",
       "7880     c20a2e14      code                                                                             vc = df['Country'].value_counts().to_frame().reset_index().head(15)\\nfig = px.bar(x=vc[\"Country\"][::-1], y=vc[\"index\"][::-1], orientation='h', color=vc['index'])\\nfig.update_layout(title = \"Countries with Most Stampedes\", xaxis_title=\"Number of Events\", yaxis_title = \"\", plot_bgcolor=\"#fff\", showlegend = False)\\nfig.show()  004cb6f938f375       26  004cb6f938f375\\tc20a2e14    bfca35c6      None       6            4                2       0.333333     3          2             -1  0.600000    -1.000000   -1.000000  0.600000     0       0\n",
       "7881     9db40705      code                                                                                                                                                                                                  fig = px.histogram(df, x=\"Number of Deaths\")\\nfig.update_layout(title = \"Number of Deaths Distribution\", xaxis_title=\"Number of Deaths\", yaxis_title = \"\", plot_bgcolor=\"#fff\", showlegend = False)\\nfig.show()  004cb6f938f375       23  004cb6f938f375\\t9db40705    bfca35c6      None       6            4                2       0.333333     4          3             -1  0.800000    -1.000000   -1.000000  0.800000     0       5\n",
       "...           ...       ...                                                                                                                                                                                                                                                                                                                                                                                                              ...             ...      ...                       ...         ...       ...     ...          ...              ...            ...   ...        ...            ...       ...          ...         ...       ...   ...     ...\n",
       "6360604  f64119e3  markdown                                                                                                                                                                                                                                                                                                                                                                   We will try to use some color attributes here   ffa0c3f2c32736        9  ffa0c3f2c32736\\tf64119e3    9d716c11      None      42           26               16       0.380952    37         -1             13  0.901235     0.888889    0.333333  0.902439     0      10\n",
       "6360605  36f8324a  markdown                                                                                                                                                                                                                                                                                                                                                          Initially, we will take the descibe value in a variable  ffa0c3f2c32736       10  ffa0c3f2c32736\\t36f8324a    9d716c11      None      42           26               16       0.380952    38         -1             14  0.913580     0.888889    0.666667  0.926829     0       0\n",
       "6360606  e8dc38e6      code                                                                                                                                                                                                                                                                                                                                                                                     table = df.describe()\\ntable  ffa0c3f2c32736        4  ffa0c3f2c32736\\te8dc38e6    9d716c11      None      42           26               16       0.380952    39         24             -1  0.925926    -1.000000   -1.000000  0.951220     0      10\n",
       "6360607  55e73c48  markdown          1. function values\\n\\nwe will return v if the values are below 10 \\n\\n2. Function highlight\\n\\nFor each column in the table we will try to find the maximum element and return it\\n\\n3. Next we will apply red color to negative values as well as we will decrease the opacity if the value is less than 10. we will call the values function here\\n\\n4. At last we call the function for highlighters  ffa0c3f2c32736       74  ffa0c3f2c32736\\t55e73c48    9d716c11      None      42           26               16       0.380952    40         -1             15  0.944444     0.925926    0.500000  0.975610     0      10\n",
       "6360608  82e48d7c      code  def values(v, props=''):\\n    return props if v < 10 else None\\n\\n\\ndef highlight_max(s, props=''):\\n    return np.where(s == np.nanmax(s.values), props, '')\\n\\nstyle_table = table.style.applymap(values, props='color:red;')\\\\n              .applymap(lambda v: 'opacity: 80%;' if (v < 10) and (v > -3) else None)\\n\\nstyle_table.apply(highlight_max, props='color:white;background-color:green', axis=0)  ffa0c3f2c32736       41  ffa0c3f2c32736\\t82e48d7c    9d716c11      None      42           26               16       0.380952    41         25             -1  0.962963    -1.000000   -1.000000  1.000000     0       5\n",
       "\n",
       "[14263 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.fold==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True).to_feather('../working/train_sample.fea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>n_words</th>\n",
       "      <th>cid</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>n_cell</th>\n",
       "      <th>n_code_cell</th>\n",
       "      <th>n_markdown_cell</th>\n",
       "      <th>markdown_frac</th>\n",
       "      <th>rank</th>\n",
       "      <th>code_rank</th>\n",
       "      <th>markdown_rank</th>\n",
       "      <th>rel_rank</th>\n",
       "      <th>global_rank</th>\n",
       "      <th>local_rank</th>\n",
       "      <th>pct_rank</th>\n",
       "      <th>fold</th>\n",
       "      <th>worker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296271</th>\n",
       "      <td>6fe77431</td>\n",
       "      <td>markdown</td>\n",
       "      <td>**Functions**\\n* text_similarity_score: Compare the query text and candidate candidate and compute the sequence-relation score. The model is Bert model, you can either use pre-trained Bert model or fine-tuning by yourself. The input for the Bert model is the concatenation of query and candidate. We add a special token \"[SEP]\" between query and candidate to facilite Bert understand the sequence better. As one of the most basic Bert model \"bert-base-uncased\" has a max-sequence length 512, we will sequence input to length 512. Therefore, you might need to find interesting queries for various questions and interesting text to using this. \\n* extract_similar_papers: Given a single query and the list of documents returned from function \"create_docs_json\" (you can either use the return value or load from saved json), this function computes the score between query with each document in the list and rank them from high to low. This function returns the top N ranked documents (you can set N with the parameter num_rets). In this function, I use the title+abstract as the candidate for each paper/document. Query will be compared with the title+abstract. You can find your own way to represent the informaiton of a paper/document.\\n* extract_similar_txtsegs: Similar as the previous function, but extract text paragraphs or segments instead of documents. Given a single query and the list of documents returned from function \"create_docs_json\" (you can either use the return value or load from saved json), this function computes the score between query with each paragraphs in all documents and rank them from high to low. This function returns the top N ranked paragraphs (you can set N with the parameter num_rets).</td>\n",
       "      <td>0bcf9d0386bdc8</td>\n",
       "      <td>272</td>\n",
       "      <td>0bcf9d0386bdc8\\t6fe77431</td>\n",
       "      <td>4f6c55eb</td>\n",
       "      <td>None</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296272</th>\n",
       "      <td>5812456b</td>\n",
       "      <td>code</td>\n",
       "      <td>def text_similarity_score (query, candidate, model, tokenizer, max_seq_length=512):\\n    #################################################################\\n    # Compute similarity score between query text and candidate text\\n    # input: \\n    # query - the query text for comparison\\n    # candidate - candidate text for comparison\\n    # model - comparison model\\n    # tokenizer - tokenizer for tokenzing the plain text\\n    #\\n    # output: \\n    # txt_sim_score - similarity score\\n    #################################################################\\n\\n    txt = query.strip() + \"[SEP]\" + candidate.strip()\\n    input_ids = torch.tensor(tokenizer.encode(txt, add_special_tokens=True, max_length=max_seq_length)).unsqueeze(0)\\n    outputs = model(input_ids)\\n    txt_sim_score = outputs[0][0][0].item()\\n    return txt_sim_score\\n\\n\\ndef extract_similar_papers (query, data_dct, model, tokenizer, num_rets=30):\\n    #################################################################\\n    # Compute Extract most related papers for the query\\n    # input: \\n    # query - the query text for extraction\\n    # data_dct - data structure contians all paper information\\n    # model - similarity comparison model\\n    # tokenizer - tokenizer for tokenzing the plain text\\n    # num_rets(optional) - default=30. the number of papers to extract.\\n    #\\n    # output: \\n    # a list of most related papers. [(score, [paper_id, txt]) ... ]\\n    #################################################################\\n\\n    print(\"Extracting similar papers ....\")\\n    print(\"Query is :\")\\n    print(query)\\n    paper_scores = []\\n    for paper in tqdm(data_dct):\\n        paper_id = paper['paper_id']\\n        candidate = paper['title'].strip()+\". \"+paper['abstract'].strip()\\n        score = text_similarity_score(query, candidate, model, tokenizer)\\n        paper_item = (score, [paper_id, candidate])\\n        paper_scores.append(paper_item)\\n    paper_scores.sort(key=lambda x: x[0], reverse=True)\\n    return paper_scores[:num_rets]\\n\\n\\ndef extract_similar_txtsegs (query, data_dct, model, tokenizer, num_rets=60):\\n    #################################################################\\n    # Compute Extract most related text segments for the query\\n    # input: \\n    # query - the query text for extraction\\n    # data_dct - data structure contians all paper information\\n    # model - similarity comparison model\\n    # tokenizer - tokenizer for tokenzing the plain text\\n    # num_rets(optional) - default=30. the number of segments to extract.\\n    #\\n    # output: \\n    # a list of most related segments. [(score, [paper_id, txt]) ... ]\\n    #################################################################\\n\\n    print(\"Extracting similar text segments ....\")\\n    print(\"Query is :\")\\n    print(query)\\n    seg_scores = []\\n    for paper in tqdm(data_dct):\\n        paper_id = paper['paper_id']\\n        candidates = paper['body_text']\\n        candidates = [item['text'].strip() for item in candidates]\\n        for candidate in candidates:\\n            score = text_similarity_score(query, candidate, model, tokenizer)\\n            seg_item = (score, [paper_id, candidate])\\n            seg_scores.append(seg_item)\\n    seg_scores.sort(key=lambda x: x[0], reverse=True)\\n    return seg_scores[:num_rets]</td>\n",
       "      <td>0bcf9d0386bdc8</td>\n",
       "      <td>324</td>\n",
       "      <td>0bcf9d0386bdc8\\t5812456b</td>\n",
       "      <td>4f6c55eb</td>\n",
       "      <td>None</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848307</th>\n",
       "      <td>09a8e986</td>\n",
       "      <td>markdown</td>\n",
       "      <td>```python\\ndef bert_encode(texts, tokenizer, max_len=512):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n            \\n        text = text[:max_len-2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        tokens += [0] * pad_len\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\\n\\ndef build_model(bert_layer, max_len=512):\\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\\n\\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\\n    clf_output = sequence_output[:, 0, :]\\n    \\n    if Dropout_num == 0:\\n        # Without Dropout\\n        out = Dense(1, activation='sigmoid')(clf_output)\\n    else:\\n        # With Dropout(Dropout_num), Dropout_num &gt; 0\\n        x = Dropout(Dropout_num)(clf_output)\\n        out = Dense(1, activation='sigmoid')(x)\\n\\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\\n    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\\n    \\n    return model\\n```</td>\n",
       "      <td>4a65048798607e</td>\n",
       "      <td>135</td>\n",
       "      <td>4a65048798607e\\t09a8e986</td>\n",
       "      <td>afb17414</td>\n",
       "      <td>None</td>\n",
       "      <td>62</td>\n",
       "      <td>22</td>\n",
       "      <td>40</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.242967</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930718</th>\n",
       "      <td>9658a38e</td>\n",
       "      <td>code</td>\n",
       "      <td>from IPython.core.display import display, HTML\\nimport string\\n\\ndef extract_ratios(text,word):\\n    extract=''\\n    if word in text:\\n        res = [i.start() for i in re.finditer(word, text)]\\n    for result in res:\\n        extracted=text[result:result+75]\\n        extracted2=text[result-200:result+75]\\n        level=''\\n        if 'sever' in extracted2:\\n            level=level+' severe'\\n        if 'fatal' in extracted2:\\n            level=level+' fatal'\\n        if 'death' in extracted2:\\n            level=level+' fatal'\\n        if 'mortality' in extracted2:\\n            level=level+' fatal'\\n        if 'hospital' in extracted2:\\n            level=level+' severe'\\n        if 'intensive' in extracted2:\\n            level=level+' severe'\\n        #print (extracted)\\n        #if '95' in extracted or 'odds ratio' in extracted or 'p&gt;' in extracted or '=' in extracted or 'p&lt;' in extracted or '])' in extracted or '(rr' in extracted:\\n        if 'odds ratio' in extracted or '])' in extracted or '(rr' in extracted or '(ar' in extracted or '(hr' in extracted or '(or' in extracted or '(aor' in extracted or '(ahr' in extracted:\\n            if '95%' in extracted:\\n                extract=extract+' '+extracted+' '+level\\n    #print (extract)\\n    return extract\\n\\ndef get_ratio(text):\\n    char1 = '('\\n    char2 = '95%'\\n    ratio=text[text.find(char1)+1 : text.find(char2)]\\n    ratio=ratio.replace('â','')\\n    return ratio\\n\\n# get the upper and lower bounds from the extracted data\\ndef get_bounds(text):\\n    raw=''\\n    char1 = 'ci'\\n    char2 = ')'\\n    data=text[text.find(char1)+1 : text.find(char2)]\\n    \\n    if '-' in data:\\n        raw=data.split('-')\\n        low=raw[0][-5:]\\n        hi=raw[1][:5]\\n    \\n    if 'to' in data and raw=='':\\n        raw=data.split('to')\\n        low=raw[0][-5:]\\n        hi=raw[1][:5]\\n        \\n    if ',' in data and raw=='':\\n        raw=data.split(',')\\n        low=raw[0][-5:]\\n        hi=raw[1][:5]\\n    \\n    if raw=='':\\n        return '-','-'\\n    low=low.replace('·','.')\\n    low = re.sub(\"[^0-9.]+\", \"\", low)\\n        \\n    return low,hi\\n\\n# get the p value fomr the extracted text\\ndef get_pvalue(text):\\n    raw=''\\n    pvalue=''\\n    char1 = 'ci'\\n    char2 = ')'\\n    data=text[text.find(char1)+1 : text.find(char2)]\\n    \\n    if 'p=' in data:\\n        raw=data.split('p=')\\n        pvalue='p='+raw[1][:7]\\n        \\n    if 'p =' in data:\\n        raw=data.split('p =')\\n        pvalue='p='+raw[1][:7]\\n    \\n    if 'p&gt;' in data and raw=='':\\n        raw=data.split('p&gt;')\\n        pvalue='p&gt;'+raw[1][:7]\\n        \\n    if 'p&lt;' in data and raw=='':\\n        raw=data.split('p&lt;')\\n        pvalue='p&lt;'+raw[1][:7]\\n    \\n    if pvalue=='':\\n        return '-'\\n    pvalue=pvalue.replace('â','')\\n    return pvalue\\n\\n# extract study design\\ndef extract_design(text):\\n    words=['retrospective','prospective cohort','retrospective cohort', 'systematic review',' meta ',' search ','case control','case series,','time series','cross-sectional','observational cohort', 'retrospective clinical','virological analysis','prevalence study','literature','two-center']\\n    study_types=['retrospective','prospective cohort','retrospective cohort','systematic review','meta-analysis','literature search','case control','case series','time series analysis','cross sectional','observational cohort study', 'retrospective clinical studies','virological analysis','prevalence study','literature search','two-center']\\n    extract=''\\n    res=''\\n    for word in words:\\n        if word in text:\\n            res = [i.start() for i in re.finditer(word, text)]\\n        for result in res:\\n            extracted=text[result-30:result+30]\\n            extract=extract+' '+extracted\\n    i=0\\n    study=''\\n    for word in words:\\n        if word in extract:\\n            study=study_types[i]\\n        #print (extract)\\n        i=i+1\\n    return study\\n\\n# BERT pretrained question answering module\\ndef answer_question(question,text,model,tokenizer):\\n    input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\\n    input_ids = tokenizer.encode(input_text)\\n    token_type_ids = [0 if i &lt;= input_ids.index(102) else 1 for i in range(len(input_ids))]\\n    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\\n    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\\n    answer=(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\\n    # show qeustion and text\\n    #tokenizer.decode(input_ids)\\n    answer=answer.replace(' ##','')\\n    #print (answer)\\n    return answer\\n\\ndef process_text(df1,focus):\\n    significant=''\\n    #df_results = pd.DataFrame(columns=['date','study','link','extracted','ratio','lower bound','upper bound','significant','p-value'])\\n    df_results = pd.DataFrame(columns=['Date', 'Study', 'Study Link', 'Journal', 'Study Type', 'Severity of Disease', 'Severity lower bound', 'Severity upper bound', 'Severity p-value', 'Severe significance', 'Severe adjusted', 'Hand-calculated Severe', 'Fatality', 'Fatality lower bound', 'Fatality upper bound', 'Fatality p-value', 'Fatality significance', 'Fatality adjusted', 'Hand-calculated Fatality', 'Multivariate adjustment', 'Sample size', 'Study population', 'Critical Only', 'Discharged vs. Death', 'Added on', 'DOI', 'CORD_UID'])\\n    for index, row in df1.iterrows():\\n        study_type=''\\n        study_type=extract_design(row['abstract'])\\n        extracted=extract_ratios(row['abstract'],focus)\\n        if extracted!='':\\n            ratio=get_ratio(extracted)\\n            lower_bound,upper_bound=get_bounds(extracted)\\n            if lower_bound!='-' and lower_bound!='':\\n                if float(lower_bound)&gt;1:\\n                    significant='yes'\\n                else:\\n                    significant='no'\\n            else:\\n                significant='-'\\n            pvalue=get_pvalue(extracted)\\n            \\n            if 'aor' in extracted or 'arr' in extracted or 'ahr' in extracted or 'arr' in extracted or 'adjusted' in extracted:\\n                adjusted='yes'\\n            else: adjusted='no'\\n            \\n            if 'fatal' in extracted and 'severe' not in extracted:\\n                severe='-'\\n                slb='-'\\n                sub='-'\\n                spv='-'\\n                ss='-'\\n                sa='-'\\n                fatal=ratio\\n                flb=lower_bound\\n                fub=upper_bound\\n                fpv=pvalue\\n                fs=significant\\n                fa=adjusted\\n            else:\\n                fatal='-'\\n                flb='-'\\n                fub='-'\\n                fpv='-'\\n                fs='-'\\n                fa='-'\\n                severe=ratio\\n                slb=lower_bound\\n                sub=upper_bound\\n                spv=pvalue\\n                ss=significant\\n                sa=adjusted\\n            \\n            ### get sample size\\n            sample_q='how many patients cases studies were included collected or enrolled'\\n            sample=row['abstract'][0:1000]\\n            sample_size=answer_question(sample_q,sample,modelqa,tokenizer)\\n            if '[SEP]' in sample_size or '[CLS]' in sample_size:\\n                sample_size='-'\\n            sample_size=sample_size.replace(' , ',',')\\n                \\n            link=row['doi']\\n            linka='https://doi.org/'+link\\n            #to_append = [row['publish_time'],row['title'],linka,extracted,ratio,lower_bound,upper_bound,significant,pvalue]\\n            to_append = [row['publish_time'], row['title'], linka, row['journal'], study_type, severe, slb, sub, spv, ss, sa, '-', fatal, flb, fub, fpv, fs, fa, '-', '-', sample_size, '-', '-', '-', '-', row['doi'], row['cord_uid']]\\n            df_length = len(df_results)\\n            df_results.loc[df_length] = to_append\\n    return df_results\\n\\nfocuses=['hypertension','diabetes','male','gender','heart disease', 'copd','smok',' age ','cerebrovascular','cardiovascular disease','cancer','kidney disease','respiratory disease','drinking','obes','liver disease']\\n\\nfor focus in focuses:\\n    df1 = df[df['abstract'].str.contains(focus)]\\n    df_results=process_text(df1,focus)\\n    df_results=df_results.sort_values(by=['Date'], ascending=False)\\n    df_table_show=HTML(df_results.to_html(escape=False,index=False))\\n    display(HTML('&lt;h1&gt; Risk Factor '+focus+'&lt;/h1&gt;'))\\n    display(df_table_show)\\n    file=focus+'.csv'\\n    df_results.to_csv(file,index=False)\\n</td>\n",
       "      <td>4dd41c30aa544e</td>\n",
       "      <td>664</td>\n",
       "      <td>4dd41c30aa544e\\t9658a38e</td>\n",
       "      <td>21aceafa</td>\n",
       "      <td>ba4480b6f8da65</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169746</th>\n",
       "      <td>44f27145</td>\n",
       "      <td>code</td>\n",
       "      <td>class SSTDataset(Dataset):\\n\\n    def __init__(self, filename, maxlen, tokenizer): \\n        #Store the contents of the file in a pandas dataframe\\n        self.df = pd.read_csv(filename, delimiter = '\\t')\\n        #Initialize the tokenizer for the desired transformer model\\n        self.tokenizer = tokenizer\\n        #Maximum length of the tokens list to keep all the sequences of fixed size\\n        self.maxlen = maxlen\\n\\n    def __len__(self):\\n        return len(self.df)\\n\\n    def __getitem__(self, index):    \\n        #Select the sentence and label at the specified index in the data frame\\n        sentence = self.df.loc[index, 'sentence']\\n        label = self.df.loc[index, 'label']\\n        #Preprocess the text to be suitable for the transformer\\n        tokens = self.tokenizer.tokenize(sentence) \\n        tokens = ['[CLS]'] + tokens + ['[SEP]'] \\n        if len(tokens) &lt; self.maxlen:\\n            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] \\n        else:\\n            tokens = tokens[:self.maxlen-1] + ['[SEP]'] \\n        #Obtain the indices of the tokens in the BERT Vocabulary\\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens) \\n        input_ids = torch.tensor(input_ids) \\n        #Obtain the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\\n        attention_mask = (input_ids != 0).long()\\n        \\n        label = torch.tensor(label, dtype=torch.long)\\n        \\n        return input_ids, attention_mask, label</td>\n",
       "      <td>57577fbaf54015</td>\n",
       "      <td>166</td>\n",
       "      <td>57577fbaf54015\\t44f27145</td>\n",
       "      <td>370d61d9</td>\n",
       "      <td>None</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169760</th>\n",
       "      <td>54868d55</td>\n",
       "      <td>code</td>\n",
       "      <td>def classify_sentiment(sentence):\\n    with torch.no_grad():\\n        tokens = tokenizer.tokenize(sentence)\\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\\n        input_ids = torch.tensor(input_ids).to(device)\\n        input_ids = input_ids.unsqueeze(0)\\n        attention_mask = (input_ids != 0).long()\\n        attention_mask = attention_mask.to(device)\\n        logit = model(input_ids=input_ids, attention_mask=attention_mask)\\n        prob = F.softmax(logit)\\n        output = torch.argmax(prob)\\n        print(logit)\\n        print(prob)\\n        print(output)\\n        prob = prob[0][output]\\n        if output == 0:\\n            print('Extreme Negative {}'.format(int(prob*100)))\\n        elif output == 1:\\n            print('Negative {}'.format(int(prob*100)))\\n        elif output == 2:\\n            print('Neutral {}'.format(int(prob*100)))\\n        elif output == 3:\\n            print('Positive {}'.format(int(prob*100)))\\n        else:\\n            print('Extreme positve {}'.format(int(prob*100)))</td>\n",
       "      <td>57577fbaf54015</td>\n",
       "      <td>76</td>\n",
       "      <td>57577fbaf54015\\t54868d55</td>\n",
       "      <td>370d61d9</td>\n",
       "      <td>None</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663426</th>\n",
       "      <td>f5161f54</td>\n",
       "      <td>markdown</td>\n",
       "      <td>The BERT tokenizer returns three values:\\n\\n- **input_ids.** The list/tensor of token ids for each sentence. The id `101` represents the class token `[CLS]`, `102` represents the separator token `[SEP]`, and `0` represents the padding token `[PAD]`.\\n- **attention_mask.** This list/tensor represents which ids to use when generating the tokens (e.g. ignores the `[PAD]` tokens).\\n- **token_type_ids.** This list/tensor represents which tokens correspond to the first and second sentence (used for next sentence prediction).</td>\n",
       "      <td>6b38233efee89a</td>\n",
       "      <td>74</td>\n",
       "      <td>6b38233efee89a\\tf5161f54</td>\n",
       "      <td>7ff7315f</td>\n",
       "      <td>None</td>\n",
       "      <td>47</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883392</th>\n",
       "      <td>54c29f4c</td>\n",
       "      <td>code</td>\n",
       "      <td>'''\\nWe are preparing out data in such a way that bert model can understand it\\n\\nWe have to give three sequences as input to the BERT\\n\\nall_tokens : It basically performs the tokenization of the input sentences\\nall_masks  : This is done to make every input of the same length. We choose the maximum length of the vector and pad other vectors accordingly. We padd them \\n             with the help of '0' which tells tells the model not to give attension to this token\\nsegment Ids: This is used when we are giving multiple sentences as the input. Since we are only giving one sentence as the input we set the value of the \\n             segment ids as 0 for all the tokens.\\n             \\nFor more detailed you can visit this terrific blog : jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time\\n'''\\n\\ndef bert_encode(texts, tokenizer, max_len=512):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n            \\n        text = text[:max_len-2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        tokens += [0] * pad_len\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\\n\\n</td>\n",
       "      <td>9b7beeeb0a0e03</td>\n",
       "      <td>198</td>\n",
       "      <td>9b7beeeb0a0e03\\t54c29f4c</td>\n",
       "      <td>a9225a93</td>\n",
       "      <td>None</td>\n",
       "      <td>110</td>\n",
       "      <td>66</td>\n",
       "      <td>44</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>86</td>\n",
       "      <td>48</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146599</th>\n",
       "      <td>e8b18f0a</td>\n",
       "      <td>code</td>\n",
       "      <td># Flag to get cv score\\nGET_CV = True\\n# Flag to check ram allocations (debug)\\nCHECK_SUB = False\\n\\ndf = pd.read_csv('../input/shopee-product-matching/test.csv')\\n# If we are comitting, replace train set for test set and dont get cv\\nif len(df) &gt; 3:\\n    GET_CV = False\\ndel df\\n\\n# Function to get our f1 score\\ndef f1_score(y_true, y_pred):\\n    y_true = y_true.apply(lambda x: set(x.split()))\\n    y_pred = y_pred.apply(lambda x: set(x.split()))\\n    intersection = np.array([len(x[0] &amp; x[1]) for x in zip(y_true, y_pred)])\\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\\n    len_y_true = y_true.apply(lambda x: len(x)).values\\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\\n    return f1\\n\\n# Function to combine predictions\\ndef combine_predictions(row):\\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\\n    return ' '.join( np.unique(x) )\\n\\n# Function to read out dataset\\ndef read_dataset():\\n    if GET_CV:\\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\\n        df['matches'] = df['label_group'].map(tmp)\\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\\n        if CHECK_SUB:\\n            df = pd.concat([df, df], axis = 0)\\n            df.reset_index(drop = True, inplace = True)\\n        df_cu = cudf.DataFrame(df)\\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\\n    else:\\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\\n        df_cu = cudf.DataFrame(df)\\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\\n        \\n    return df, df_cu, image_paths\\n\\n# Function to decode our images\\ndef decode_image(image_data):\\n    image = tf.image.decode_jpeg(image_data, channels = 3)\\n    image = tf.image.resize(image, IMAGE_SIZE)\\n    image = tf.cast(image, tf.float32) / 255.0\\n    return image\\n\\n# Function to read our test image and return image\\ndef read_image(image):\\n    image = tf.io.read_file(image)\\n    image = decode_image(image)\\n    return image\\n\\n# Function to get our dataset that read images\\ndef get_dataset(image):\\n    dataset = tf.data.Dataset.from_tensor_slices(image)\\n    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\\n    dataset = dataset.batch(BATCH_SIZE)\\n    dataset = dataset.prefetch(AUTO)\\n    return dataset\\n\\n# Arcmarginproduct class keras layer\\nclass ArcMarginProduct(tf.keras.layers.Layer):\\n    '''\\n    Implements large margin arc distance.\\n\\n    Reference:\\n        https://arxiv.org/pdf/1801.07698.pdf\\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\\n            blob/master/src/modeling/metric_learning.py\\n    '''\\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\\n                 ls_eps=0.0, **kwargs):\\n\\n        super(ArcMarginProduct, self).__init__(**kwargs)\\n\\n        self.n_classes = n_classes\\n        self.s = s\\n        self.m = m\\n        self.ls_eps = ls_eps\\n        self.easy_margin = easy_margin\\n        self.cos_m = tf.math.cos(m)\\n        self.sin_m = tf.math.sin(m)\\n        self.th = tf.math.cos(math.pi - m)\\n        self.mm = tf.math.sin(math.pi - m) * m\\n\\n    def get_config(self):\\n\\n        config = super().get_config().copy()\\n        config.update({\\n            'n_classes': self.n_classes,\\n            's': self.s,\\n            'm': self.m,\\n            'ls_eps': self.ls_eps,\\n            'easy_margin': self.easy_margin,\\n        })\\n        return config\\n\\n    def build(self, input_shape):\\n        super(ArcMarginProduct, self).build(input_shape[0])\\n\\n        self.W = self.add_weight(\\n            name='W',\\n            shape=(int(input_shape[0][-1]), self.n_classes),\\n            initializer='glorot_uniform',\\n            dtype='float32',\\n            trainable=True,\\n            regularizer=None)\\n\\n    def call(self, inputs):\\n        X, y = inputs\\n        y = tf.cast(y, dtype=tf.int32)\\n        cosine = tf.matmul(\\n            tf.math.l2_normalize(X, axis=1),\\n            tf.math.l2_normalize(self.W, axis=0)\\n        )\\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\\n        phi = cosine * self.cos_m - sine * self.sin_m\\n        if self.easy_margin:\\n            phi = tf.where(cosine &gt; 0, phi, cosine)\\n        else:\\n            phi = tf.where(cosine &gt; self.th, phi, cosine - self.mm)\\n        one_hot = tf.cast(\\n            tf.one_hot(y, depth=self.n_classes),\\n            dtype=cosine.dtype\\n        )\\n        if self.ls_eps &gt; 0:\\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\\n\\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\\n        output *= self.s\\n        return output\\n\\n\\n\\n# Function to get the embeddings of our images with the fine-tuned model\\ndef get_image_embeddings(image_paths):\\n    embeds = []\\n    #m = 0.7, \\n    margin = ArcMarginProduct(\\n            n_classes = N_CLASSES, \\n            s = 30, \\n            m = 0.5, \\n            name='head/arc_margin', \\n            dtype='float32'\\n            )\\n\\n    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\\n    label = tf.keras.layers.Input(shape = (), name = 'inp2')\\n    x = efn.EfficientNetB1(weights = None, include_top = False)(inp)\\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\\n    #x = tf.keras.layers.Dropout(0.5)(x)\\n    x = margin([x, label])\\n        \\n    output = tf.keras.layers.Softmax(dtype='float32')(x)\\n\\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\\n    model.load_weights('../input/shopee-efficientnetb3-arcmarginproduct/EfficientNetB1_512_42.h5')\\n    model = tf.keras.models.Model(inputs = model.input[0], outputs = model.layers[-4].output)\\n    chunk = 5000\\n    iterator = np.arange(np.ceil(len(df) / chunk))\\n    for j in iterator:\\n        a = int(j * chunk)\\n        b = int((j + 1) * chunk)\\n        image_dataset = get_dataset(image_paths[a:b])\\n        image_embeddings = model.predict(image_dataset)\\n        embeds.append(image_embeddings)\\n    del model\\n    image_embeddings = np.concatenate(embeds)\\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\\n    del embeds\\n    gc.collect()\\n    return image_embeddings\\n\\n# Return tokens, masks and segments from a text array or series\\ndef bert_encode(texts, tokenizer, max_len=512):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n            \\n        text = text[:max_len-2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        tokens += [0] * pad_len\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\\n\\n# Function to get our text title embeddings using a pre-trained bert model\\ndef get_text_embeddings(df, max_len = 70):\\n    embeds = []\\n    module_url = \"../input/shopee-external-models/bert_en_uncased_L-24_H-1024_A-16_1\"\\n \\n    bert_layer = hub.KerasLayer(module_url, trainable = True)\\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\\n    text = bert_encode(df['title'].values, tokenizer, max_len = max_len)\\n    \\n    margin = ArcMarginProduct(\\n            n_classes = 11014, \\n            s = 30, \\n            m = 0.5, \\n            name='head/arc_margin', \\n            dtype='float32'\\n            )\\n    \\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\\n    label = tf.keras.layers.Input(shape = (), name = 'label')\\n\\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\\n    clf_output = sequence_output[:, 0, :]\\n    x = margin([clf_output, label])\\n    output = tf.keras.layers.Softmax(dtype='float32')(x)\\n    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\\n    \\n    model.load_weights('../input/bert-baseline/Bert_123.h5')\\n    model = tf.keras.models.Model(inputs = model.input[0:3], outputs = model.layers[-4].output)\\n    chunk = 5000\\n    iterator = np.arange(np.ceil(len(df) / chunk))\\n    for j in iterator:\\n        a = int(j * chunk)\\n        b = int((j + 1) * chunk)\\n        text_chunk = ((text[0][a:b], text[1][a:b], text[2][a:b]))\\n        text_embeddings = model.predict(text_chunk, batch_size = BATCH_SIZE)\\n        embeds.append(text_embeddings)\\n    del model\\n    text_embeddings = np.concatenate(embeds)\\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\\n    del embeds\\n    gc.collect()\\n    return text_embeddings\\n    \\n# Function to get 50 nearest neighbors of each image and apply a distance threshold to maximize cv\\ndef get_neighbors(df, embeddings, KNN = 50, image = True):\\n    model = NearestNeighbors(n_neighbors = KNN)\\n    model.fit(embeddings)\\n    distances, indices = model.kneighbors(embeddings)\\n    \\n    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\\n    if GET_CV:\\n        if image:\\n            thresholds = list(np.arange(3.0, 5.0, 0.1))\\n        else:\\n            thresholds = list(np.arange(15, 35, 1))\\n        scores = []\\n        for threshold in thresholds:\\n            predictions = []\\n            for k in range(embeddings.shape[0]):\\n                idx = np.where(distances[k,] &lt; threshold)[0]\\n                ids = indices[k,idx]\\n                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\\n                predictions.append(posting_ids)\\n            df['pred_matches'] = predictions\\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\\n            score = df['f1'].mean()\\n            print(f'Our f1 score for threshold {threshold} is {score}')\\n            scores.append(score)\\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\\n        best_threshold = max_score['thresholds'].values[0]\\n        best_score = max_score['scores'].values[0]\\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\\n        \\n        # Use threshold\\n        predictions = []\\n        for k in range(embeddings.shape[0]):\\n            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\\n            if image:\\n                idx = np.where(distances[k,] &lt; 3.6)[0] #3.7,3.3\\n            else:\\n                idx = np.where(distances[k,] &lt; 20.0)[0] #20,18\\n            ids = indices[k,idx]\\n            posting_ids = df['posting_id'].iloc[ids].values\\n            predictions.append(posting_ids)\\n    \\n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\\n    else:\\n        predictions = []\\n        for k in tqdm(range(embeddings.shape[0])):\\n            if image:\\n                idx = np.where(distances[k,] &lt; 3.6)[0] #3.3\\n            else:\\n                idx = np.where(distances[k,] &lt; 20.0)[0] #20,18\\n            ids = indices[k,idx]\\n            posting_ids = df['posting_id'].iloc[ids].values\\n            predictions.append(posting_ids)\\n        \\n    del model, distances, indices\\n    gc.collect()\\n    return df, predictions\\n    \\n\\ndf, df_cu, image_paths = read_dataset()\\nimage_embeddings = get_image_embeddings(image_paths)\\n</td>\n",
       "      <td>ce8b65fdfd2fc0</td>\n",
       "      <td>1091</td>\n",
       "      <td>ce8b65fdfd2fc0\\te8b18f0a</td>\n",
       "      <td>0402a6a0</td>\n",
       "      <td>7427d81de0f830</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5591331</th>\n",
       "      <td>7b4093af</td>\n",
       "      <td>code</td>\n",
       "      <td>class FastAiBertTokenizer(BaseTokenizer):\\n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\\n        self._pretrained_tokenizer = tokenizer\\n        self.max_seq_len = max_seq_len\\n\\n    def __call__(self, *args, **kwargs):\\n        return self\\n\\n    def tokenizer(self, t:str) -&gt; List[str]:\\n        \"\"\"Limits the maximum sequence length\"\"\"\\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]</td>\n",
       "      <td>e08db13aefd60c</td>\n",
       "      <td>47</td>\n",
       "      <td>e08db13aefd60c\\t7b4093af</td>\n",
       "      <td>b9f2edaa</td>\n",
       "      <td>1dadb8c3bd33bb</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5807422</th>\n",
       "      <td>0b20d677</td>\n",
       "      <td>code</td>\n",
       "      <td>from IPython.core.display import display, HTML\\nimport functools\\n\\ndef remove_stopwords(text,stopwords):\\n    text = \"\".join(c for c in text if c not in ('!','.',',','?','(',')','-'))\\n    text_tokens = word_tokenize(text)\\n    #remove stopwords\\n    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\\n    str1=''\\n    str1=' '.join(word for word in tokens_without_sw)\\n    return str1\\n### spacy score sentence\\ndef score_sentence(search,sentence):\\n        main_doc=nlp(sentence)\\n        search_doc=nlp(search)\\n        sent_score=main_doc.similarity(search_doc)\\n        return sent_score\\n\\n# custom sentence score\\ndef score_sentence_prob(search,sentence,focus):\\n    keywords=search.split()\\n    sent_parts=sentence.split()\\n    word_match=0\\n    missing=0\\n    for word in keywords:\\n        word_count=sent_parts.count(word)\\n        word_match=word_match+word_count\\n        if word_count==0:\\n            missing=missing+1\\n    percent = 1-(missing/len(keywords))\\n    final_score=abs((word_match/len(sent_parts)) * percent)\\n    if missing==0:\\n        final_score=final_score+.05\\n    if focus in sentence:\\n        final_score=final_score+.05\\n    return final_score\\n\\n# BERT pretrained question answering module\\ndef answer_question(question,text, model,tokenizer):\\n    input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\\n    input_ids = tokenizer.encode(input_text)\\n    token_type_ids = [0 if i &lt;= input_ids.index(102) else 1 for i in range(len(input_ids))]\\n    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\\n    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\\n    answer=(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\\n    # show qeustion and text\\n    #tokenizer.decode(input_ids)\\n    return answer\\n\\ndef score_sentence_study(sentence):\\n        search='included total cases patients sample size collected gathered enrolled study'\\n        #search='how many patients cases included in study cohort meta-analysis prospective retrospective'\\n        main_doc=nlp(sentence)\\n        search_doc=nlp(search)\\n        sent_score=main_doc.similarity(search_doc)\\n        return sent_score\\n\\ndef process_question(df,search,focus):\\n    df_table = pd.DataFrame(columns = [\"Date\",\"Study\",\"Study Link\",\"Journal\",\"Days After Admission Covid-19 is Present\",\"Source / Summary\",\"Study Type\",\"Sample Size\",\"Confirmation\",\"excerpt\",\"rel_score\"])\\n    # focuses to make sure the exact phrase in text\\n    #df1 = df[df['abstract'].str.contains(focus)]\\n    # focus to make sure all words in text\\n    df1=df[functools.reduce(lambda a, b: a&amp;b, (df['abstract'].str.contains(s) for s in search))]\\n    search=remove_stopwords(search,stopwords)\\n    for index, row in df1.iterrows():\\n        sentences = row['abstract'].split('. ')\\n        pub_sentence=''\\n        hi_score=0\\n        study=''\\n        hi_study_score=0\\n        for sentence in sentences:\\n            if len(sentence)&gt;75:\\n                rel_score=score_sentence_prob(search,sentence,focus)\\n                if rel_score&gt;.05:\\n                    sentence=sentence.capitalize()\\n                    if sentence[len(sentence)-1]!='.':\\n                        sentence=sentence+'.'\\n                    pub_sentence=pub_sentence+' '+sentence\\n                    if rel_score&gt;hi_score:\\n                        hi_score=rel_score\\n                \\n        if pub_sentence!='':\\n            text=row['abstract'][0:1000]\\n            \\n            question='how many patients or cases were in the study, review or analysis?'\\n            sample=answer_question(question,text,model,tokenizer)\\n            sample=sample.replace(\"#\", \"\")\\n            sample=sample.replace(\" , \", \",\")\\n            if sample=='19' or sample=='' or '[SEP]'in sample:\\n                sample='unk'\\n            if len(sample)&gt;50:\\n                sample='unk'\\n            sample=sample.replace(\" \", \"\")\\n            \\n            question='what type or kind of review study analysis model was used?'\\n            design=answer_question(question,text,model,tokenizer)\\n            design=design.replace(\" ##\", \"\")\\n            if '[SEP]'in design or '[CLS]' in design or design=='':\\n                design='unk'\\n            \\n            shorter = pub_sentence[0:1000]\\n            ### answer incubation questions\\n            incubation_period=answer_question('how many days after onset was the shedding detected',shorter,model,tokenizer)\\n            incubation_period=incubation_period.replace(\" ##\", \"\")\\n            incubation_period=incubation_period.replace(\" · \", \"·\")\\n            incubation_period=incubation_period.replace(\" . \", \".\")\\n            \\n            incubation_range=answer_question('what is the virus shedding detection route',shorter,model,tokenizer)\\n            incubation_range=incubation_range.replace(\" ##\", \"\")\\n            incubation_range=incubation_range.replace(\" · \", \"·\")\\n            incubation_range=incubation_range.replace(\" . \", \".\")\\n            \\n            confirmation=answer_question('what kind of test was conducted to confirm the shedding',shorter,model,tokenizer)\\n            confirmation=confirmation.replace(\" ##\", \"\")\\n            confirmation=confirmation.replace(\" · \", \"·\")\\n            confirmation=confirmation.replace(\" . \", \".\")\\n            \\n            authors=row[\"authors\"].split(\" \")\\n            link=row['doi']\\n            title=row[\"title\"]\\n            score=hi_score\\n            journal=row[\"journal\"]\\n            if journal=='':\\n                journal=row['full_text_file']\\n            linka='https://doi.org/'+link\\n            linkb=title\\n            final_link='&lt;p align=\"left\"&gt;&lt;a href=\"{}\"&gt;{}&lt;/a&gt;&lt;/p&gt;'.format(linka,linkb)\\n            #author_link='&lt;p align=\"left\"&gt;&lt;a href=\"{}\"&gt;{}&lt;/a&gt;&lt;/p&gt;'.format(linka,authors[0]+' et al.')\\n            #sentence=pub_sentence+' '+author_link\\n            sentence=pub_sentence\\n            #sentence='&lt;p fontsize=tiny\" align=\"left\"&gt;'+sentence+'&lt;/p&gt;'\\n            to_append = [row['publish_time'],title,linka,journal,incubation_period,incubation_range,design,sample,confirmation,sentence,score]\\n            df_length = len(df_table)\\n            df_table.loc[df_length] = to_append\\n    df_table=df_table.sort_values(by=['Date'], ascending=False)\\n    to_append = [\"Date\",\"Study\",\"Study Link\",\"Journal\",\"Days After Admission Covid-19 is Present\",\"Source / Summary\",\"Study Type\",\"Sample Size\",\"Confirmation\",\"excerpt\",\"rel_score\"]\\n    df_length = len(df_table)\\n    df_table.loc[df_length] = to_append\\n    return df_table\\n\\ndef prepare_summary_answer(text,model):\\n    #model = pipeline(task=\"summarization\")\\n    return model(text)\\n\\n###### MAIN PROGRAM ######\\n# questions\\nsearch='naso nasal nasopharynx pharyn'\\n\\n# main focus keywords\\nfocus='shedding'\\n\\n# process with spacy model and return df\\ndf_table=process_question(df,search,focus)\\n    \\ndisplay(HTML('&lt;h2&gt;'+search+'&lt;/h2&gt;'))\\n#display(HTML('&lt;h5&gt;*** Note: this table keeps the document excerpt and score for ease of review. A clean literature review formatted CSV file is avaiable in the data seciton wihtout those fields.&lt;/h5&gt;'))\\n    \\n#convert df_table to html and display\\ndf_table=df_table.drop(columns=['excerpt', 'rel_score'])\\n#df_master=df_master.fillna('blank')\\n#df_table=df_table.fillna('blank')\\ndf_table_show = df_table.append(df_master,sort=False)\\ndf_table_show = df_table_show.drop_duplicates(subset='Study', keep=\"last\")\\n#df_table_show['Study Type'].fillna(df_table_show['StudyType'], inplace=True)\\n#del df_table_show['Study Type']\\n\\ndf_table_display=HTML(df_table_show.to_html(escape=False,index=True))\\ndisplay(df_table_display)\\n\\ndf_table_show.to_csv('What do we know about viral shedding nasopharynx_.csv', index = False)\\n\\n#df_table_show.head()</td>\n",
       "      <td>e9664b8145e1e9</td>\n",
       "      <td>548</td>\n",
       "      <td>e9664b8145e1e9\\t0b20d677</td>\n",
       "      <td>2f763a2f</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cell_id cell_type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    source              id  n_words                       cid ancestor_id       parent_id  n_cell  n_code_cell  n_markdown_cell  markdown_frac  rank  code_rank  markdown_rank  rel_rank  global_rank  local_rank  pct_rank  fold  worker\n",
       "296271   6fe77431  markdown                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                **Functions**\\n* text_similarity_score: Compare the query text and candidate candidate and compute the sequence-relation score. The model is Bert model, you can either use pre-trained Bert model or fine-tuning by yourself. The input for the Bert model is the concatenation of query and candidate. We add a special token \"[SEP]\" between query and candidate to facilite Bert understand the sequence better. As one of the most basic Bert model \"bert-base-uncased\" has a max-sequence length 512, we will sequence input to length 512. Therefore, you might need to find interesting queries for various questions and interesting text to using this. \\n* extract_similar_papers: Given a single query and the list of documents returned from function \"create_docs_json\" (you can either use the return value or load from saved json), this function computes the score between query with each document in the list and rank them from high to low. This function returns the top N ranked documents (you can set N with the parameter num_rets). In this function, I use the title+abstract as the candidate for each paper/document. Query will be compared with the title+abstract. You can find your own way to represent the informaiton of a paper/document.\\n* extract_similar_txtsegs: Similar as the previous function, but extract text paragraphs or segments instead of documents. Given a single query and the list of documents returned from function \"create_docs_json\" (you can either use the return value or load from saved json), this function computes the score between query with each paragraphs in all documents and rank them from high to low. This function returns the top N ranked paragraphs (you can set N with the parameter num_rets).  0bcf9d0386bdc8      272  0bcf9d0386bdc8\\t6fe77431    4f6c55eb            None      21            9               12       0.571429     8         -1              5  0.366667     0.300000    0.666667  0.400000     3       3\n",
       "296272   5812456b      code                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        def text_similarity_score (query, candidate, model, tokenizer, max_seq_length=512):\\n    #################################################################\\n    # Compute similarity score between query text and candidate text\\n    # input: \\n    # query - the query text for comparison\\n    # candidate - candidate text for comparison\\n    # model - comparison model\\n    # tokenizer - tokenizer for tokenzing the plain text\\n    #\\n    # output: \\n    # txt_sim_score - similarity score\\n    #################################################################\\n\\n    txt = query.strip() + \"[SEP]\" + candidate.strip()\\n    input_ids = torch.tensor(tokenizer.encode(txt, add_special_tokens=True, max_length=max_seq_length)).unsqueeze(0)\\n    outputs = model(input_ids)\\n    txt_sim_score = outputs[0][0][0].item()\\n    return txt_sim_score\\n\\n\\ndef extract_similar_papers (query, data_dct, model, tokenizer, num_rets=30):\\n    #################################################################\\n    # Compute Extract most related papers for the query\\n    # input: \\n    # query - the query text for extraction\\n    # data_dct - data structure contians all paper information\\n    # model - similarity comparison model\\n    # tokenizer - tokenizer for tokenzing the plain text\\n    # num_rets(optional) - default=30. the number of papers to extract.\\n    #\\n    # output: \\n    # a list of most related papers. [(score, [paper_id, txt]) ... ]\\n    #################################################################\\n\\n    print(\"Extracting similar papers ....\")\\n    print(\"Query is :\")\\n    print(query)\\n    paper_scores = []\\n    for paper in tqdm(data_dct):\\n        paper_id = paper['paper_id']\\n        candidate = paper['title'].strip()+\". \"+paper['abstract'].strip()\\n        score = text_similarity_score(query, candidate, model, tokenizer)\\n        paper_item = (score, [paper_id, candidate])\\n        paper_scores.append(paper_item)\\n    paper_scores.sort(key=lambda x: x[0], reverse=True)\\n    return paper_scores[:num_rets]\\n\\n\\ndef extract_similar_txtsegs (query, data_dct, model, tokenizer, num_rets=60):\\n    #################################################################\\n    # Compute Extract most related text segments for the query\\n    # input: \\n    # query - the query text for extraction\\n    # data_dct - data structure contians all paper information\\n    # model - similarity comparison model\\n    # tokenizer - tokenizer for tokenzing the plain text\\n    # num_rets(optional) - default=30. the number of segments to extract.\\n    #\\n    # output: \\n    # a list of most related segments. [(score, [paper_id, txt]) ... ]\\n    #################################################################\\n\\n    print(\"Extracting similar text segments ....\")\\n    print(\"Query is :\")\\n    print(query)\\n    seg_scores = []\\n    for paper in tqdm(data_dct):\\n        paper_id = paper['paper_id']\\n        candidates = paper['body_text']\\n        candidates = [item['text'].strip() for item in candidates]\\n        for candidate in candidates:\\n            score = text_similarity_score(query, candidate, model, tokenizer)\\n            seg_item = (score, [paper_id, candidate])\\n            seg_scores.append(seg_item)\\n    seg_scores.sort(key=lambda x: x[0], reverse=True)\\n    return seg_scores[:num_rets]  0bcf9d0386bdc8      324  0bcf9d0386bdc8\\t5812456b    4f6c55eb            None      21            9               12       0.571429     9          3             -1  0.400000    -1.000000   -1.000000  0.450000     3      13\n",
       "1848307  09a8e986  markdown                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ```python\\ndef bert_encode(texts, tokenizer, max_len=512):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n            \\n        text = text[:max_len-2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        tokens += [0] * pad_len\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\\n\\ndef build_model(bert_layer, max_len=512):\\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\\n\\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\\n    clf_output = sequence_output[:, 0, :]\\n    \\n    if Dropout_num == 0:\\n        # Without Dropout\\n        out = Dense(1, activation='sigmoid')(clf_output)\\n    else:\\n        # With Dropout(Dropout_num), Dropout_num > 0\\n        x = Dropout(Dropout_num)(clf_output)\\n        out = Dense(1, activation='sigmoid')(x)\\n\\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\\n    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\\n    \\n    return model\\n```  4a65048798607e      135  4a65048798607e\\t09a8e986    afb17414            None      62           22               40       0.645161    20         -1             15  0.242967     0.217391    0.588235  0.327869     1      11\n",
       "1930718  9658a38e      code                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     from IPython.core.display import display, HTML\\nimport string\\n\\ndef extract_ratios(text,word):\\n    extract=''\\n    if word in text:\\n        res = [i.start() for i in re.finditer(word, text)]\\n    for result in res:\\n        extracted=text[result:result+75]\\n        extracted2=text[result-200:result+75]\\n        level=''\\n        if 'sever' in extracted2:\\n            level=level+' severe'\\n        if 'fatal' in extracted2:\\n            level=level+' fatal'\\n        if 'death' in extracted2:\\n            level=level+' fatal'\\n        if 'mortality' in extracted2:\\n            level=level+' fatal'\\n        if 'hospital' in extracted2:\\n            level=level+' severe'\\n        if 'intensive' in extracted2:\\n            level=level+' severe'\\n        #print (extracted)\\n        #if '95' in extracted or 'odds ratio' in extracted or 'p>' in extracted or '=' in extracted or 'p<' in extracted or '])' in extracted or '(rr' in extracted:\\n        if 'odds ratio' in extracted or '])' in extracted or '(rr' in extracted or '(ar' in extracted or '(hr' in extracted or '(or' in extracted or '(aor' in extracted or '(ahr' in extracted:\\n            if '95%' in extracted:\\n                extract=extract+' '+extracted+' '+level\\n    #print (extract)\\n    return extract\\n\\ndef get_ratio(text):\\n    char1 = '('\\n    char2 = '95%'\\n    ratio=text[text.find(char1)+1 : text.find(char2)]\\n    ratio=ratio.replace('â','')\\n    return ratio\\n\\n# get the upper and lower bounds from the extracted data\\ndef get_bounds(text):\\n    raw=''\\n    char1 = 'ci'\\n    char2 = ')'\\n    data=text[text.find(char1)+1 : text.find(char2)]\\n    \\n    if '-' in data:\\n        raw=data.split('-')\\n        low=raw[0][-5:]\\n        hi=raw[1][:5]\\n    \\n    if 'to' in data and raw=='':\\n        raw=data.split('to')\\n        low=raw[0][-5:]\\n        hi=raw[1][:5]\\n        \\n    if ',' in data and raw=='':\\n        raw=data.split(',')\\n        low=raw[0][-5:]\\n        hi=raw[1][:5]\\n    \\n    if raw=='':\\n        return '-','-'\\n    low=low.replace('·','.')\\n    low = re.sub(\"[^0-9.]+\", \"\", low)\\n        \\n    return low,hi\\n\\n# get the p value fomr the extracted text\\ndef get_pvalue(text):\\n    raw=''\\n    pvalue=''\\n    char1 = 'ci'\\n    char2 = ')'\\n    data=text[text.find(char1)+1 : text.find(char2)]\\n    \\n    if 'p=' in data:\\n        raw=data.split('p=')\\n        pvalue='p='+raw[1][:7]\\n        \\n    if 'p =' in data:\\n        raw=data.split('p =')\\n        pvalue='p='+raw[1][:7]\\n    \\n    if 'p>' in data and raw=='':\\n        raw=data.split('p>')\\n        pvalue='p>'+raw[1][:7]\\n        \\n    if 'p<' in data and raw=='':\\n        raw=data.split('p<')\\n        pvalue='p<'+raw[1][:7]\\n    \\n    if pvalue=='':\\n        return '-'\\n    pvalue=pvalue.replace('â','')\\n    return pvalue\\n\\n# extract study design\\ndef extract_design(text):\\n    words=['retrospective','prospective cohort','retrospective cohort', 'systematic review',' meta ',' search ','case control','case series,','time series','cross-sectional','observational cohort', 'retrospective clinical','virological analysis','prevalence study','literature','two-center']\\n    study_types=['retrospective','prospective cohort','retrospective cohort','systematic review','meta-analysis','literature search','case control','case series','time series analysis','cross sectional','observational cohort study', 'retrospective clinical studies','virological analysis','prevalence study','literature search','two-center']\\n    extract=''\\n    res=''\\n    for word in words:\\n        if word in text:\\n            res = [i.start() for i in re.finditer(word, text)]\\n        for result in res:\\n            extracted=text[result-30:result+30]\\n            extract=extract+' '+extracted\\n    i=0\\n    study=''\\n    for word in words:\\n        if word in extract:\\n            study=study_types[i]\\n        #print (extract)\\n        i=i+1\\n    return study\\n\\n# BERT pretrained question answering module\\ndef answer_question(question,text,model,tokenizer):\\n    input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\\n    input_ids = tokenizer.encode(input_text)\\n    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\\n    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\\n    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\\n    answer=(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\\n    # show qeustion and text\\n    #tokenizer.decode(input_ids)\\n    answer=answer.replace(' ##','')\\n    #print (answer)\\n    return answer\\n\\ndef process_text(df1,focus):\\n    significant=''\\n    #df_results = pd.DataFrame(columns=['date','study','link','extracted','ratio','lower bound','upper bound','significant','p-value'])\\n    df_results = pd.DataFrame(columns=['Date', 'Study', 'Study Link', 'Journal', 'Study Type', 'Severity of Disease', 'Severity lower bound', 'Severity upper bound', 'Severity p-value', 'Severe significance', 'Severe adjusted', 'Hand-calculated Severe', 'Fatality', 'Fatality lower bound', 'Fatality upper bound', 'Fatality p-value', 'Fatality significance', 'Fatality adjusted', 'Hand-calculated Fatality', 'Multivariate adjustment', 'Sample size', 'Study population', 'Critical Only', 'Discharged vs. Death', 'Added on', 'DOI', 'CORD_UID'])\\n    for index, row in df1.iterrows():\\n        study_type=''\\n        study_type=extract_design(row['abstract'])\\n        extracted=extract_ratios(row['abstract'],focus)\\n        if extracted!='':\\n            ratio=get_ratio(extracted)\\n            lower_bound,upper_bound=get_bounds(extracted)\\n            if lower_bound!='-' and lower_bound!='':\\n                if float(lower_bound)>1:\\n                    significant='yes'\\n                else:\\n                    significant='no'\\n            else:\\n                significant='-'\\n            pvalue=get_pvalue(extracted)\\n            \\n            if 'aor' in extracted or 'arr' in extracted or 'ahr' in extracted or 'arr' in extracted or 'adjusted' in extracted:\\n                adjusted='yes'\\n            else: adjusted='no'\\n            \\n            if 'fatal' in extracted and 'severe' not in extracted:\\n                severe='-'\\n                slb='-'\\n                sub='-'\\n                spv='-'\\n                ss='-'\\n                sa='-'\\n                fatal=ratio\\n                flb=lower_bound\\n                fub=upper_bound\\n                fpv=pvalue\\n                fs=significant\\n                fa=adjusted\\n            else:\\n                fatal='-'\\n                flb='-'\\n                fub='-'\\n                fpv='-'\\n                fs='-'\\n                fa='-'\\n                severe=ratio\\n                slb=lower_bound\\n                sub=upper_bound\\n                spv=pvalue\\n                ss=significant\\n                sa=adjusted\\n            \\n            ### get sample size\\n            sample_q='how many patients cases studies were included collected or enrolled'\\n            sample=row['abstract'][0:1000]\\n            sample_size=answer_question(sample_q,sample,modelqa,tokenizer)\\n            if '[SEP]' in sample_size or '[CLS]' in sample_size:\\n                sample_size='-'\\n            sample_size=sample_size.replace(' , ',',')\\n                \\n            link=row['doi']\\n            linka='https://doi.org/'+link\\n            #to_append = [row['publish_time'],row['title'],linka,extracted,ratio,lower_bound,upper_bound,significant,pvalue]\\n            to_append = [row['publish_time'], row['title'], linka, row['journal'], study_type, severe, slb, sub, spv, ss, sa, '-', fatal, flb, fub, fpv, fs, fa, '-', '-', sample_size, '-', '-', '-', '-', row['doi'], row['cord_uid']]\\n            df_length = len(df_results)\\n            df_results.loc[df_length] = to_append\\n    return df_results\\n\\nfocuses=['hypertension','diabetes','male','gender','heart disease', 'copd','smok',' age ','cerebrovascular','cardiovascular disease','cancer','kidney disease','respiratory disease','drinking','obes','liver disease']\\n\\nfor focus in focuses:\\n    df1 = df[df['abstract'].str.contains(focus)]\\n    df_results=process_text(df1,focus)\\n    df_results=df_results.sort_values(by=['Date'], ascending=False)\\n    df_table_show=HTML(df_results.to_html(escape=False,index=False))\\n    display(HTML('<h1> Risk Factor '+focus+'</h1>'))\\n    display(df_table_show)\\n    file=focus+'.csv'\\n    df_results.to_csv(file,index=False)\\n  4dd41c30aa544e      664  4dd41c30aa544e\\t9658a38e    21aceafa  ba4480b6f8da65       5            3                2       0.400000     4          2             -1  0.750000    -1.000000   -1.000000  1.000000     4       4\n",
       "2169746  44f27145      code                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         class SSTDataset(Dataset):\\n\\n    def __init__(self, filename, maxlen, tokenizer): \\n        #Store the contents of the file in a pandas dataframe\\n        self.df = pd.read_csv(filename, delimiter = '\\t')\\n        #Initialize the tokenizer for the desired transformer model\\n        self.tokenizer = tokenizer\\n        #Maximum length of the tokens list to keep all the sequences of fixed size\\n        self.maxlen = maxlen\\n\\n    def __len__(self):\\n        return len(self.df)\\n\\n    def __getitem__(self, index):    \\n        #Select the sentence and label at the specified index in the data frame\\n        sentence = self.df.loc[index, 'sentence']\\n        label = self.df.loc[index, 'label']\\n        #Preprocess the text to be suitable for the transformer\\n        tokens = self.tokenizer.tokenize(sentence) \\n        tokens = ['[CLS]'] + tokens + ['[SEP]'] \\n        if len(tokens) < self.maxlen:\\n            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] \\n        else:\\n            tokens = tokens[:self.maxlen-1] + ['[SEP]'] \\n        #Obtain the indices of the tokens in the BERT Vocabulary\\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens) \\n        input_ids = torch.tensor(input_ids) \\n        #Obtain the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\\n        attention_mask = (input_ids != 0).long()\\n        \\n        label = torch.tensor(label, dtype=torch.long)\\n        \\n        return input_ids, attention_mask, label  57577fbaf54015      166  57577fbaf54015\\t44f27145    370d61d9            None      21           13                8       0.380952     5          2             -1  0.214286    -1.000000   -1.000000  0.250000     0      10\n",
       "2169760  54868d55      code                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     def classify_sentiment(sentence):\\n    with torch.no_grad():\\n        tokens = tokenizer.tokenize(sentence)\\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\\n        input_ids = torch.tensor(input_ids).to(device)\\n        input_ids = input_ids.unsqueeze(0)\\n        attention_mask = (input_ids != 0).long()\\n        attention_mask = attention_mask.to(device)\\n        logit = model(input_ids=input_ids, attention_mask=attention_mask)\\n        prob = F.softmax(logit)\\n        output = torch.argmax(prob)\\n        print(logit)\\n        print(prob)\\n        print(output)\\n        prob = prob[0][output]\\n        if output == 0:\\n            print('Extreme Negative {}'.format(int(prob*100)))\\n        elif output == 1:\\n            print('Negative {}'.format(int(prob*100)))\\n        elif output == 2:\\n            print('Neutral {}'.format(int(prob*100)))\\n        elif output == 3:\\n            print('Positive {}'.format(int(prob*100)))\\n        else:\\n            print('Extreme positve {}'.format(int(prob*100)))  57577fbaf54015       76  57577fbaf54015\\t54868d55    370d61d9            None      21           13                8       0.380952    19         11             -1  0.857143    -1.000000   -1.000000  0.950000     0      10\n",
       "2663426  f5161f54  markdown                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The BERT tokenizer returns three values:\\n\\n- **input_ids.** The list/tensor of token ids for each sentence. The id `101` represents the class token `[CLS]`, `102` represents the separator token `[SEP]`, and `0` represents the padding token `[PAD]`.\\n- **attention_mask.** This list/tensor represents which ids to use when generating the tokens (e.g. ignores the `[PAD]` tokens).\\n- **token_type_ids.** This list/tensor represents which tokens correspond to the first and second sentence (used for next sentence prediction).  6b38233efee89a       74  6b38233efee89a\\tf5161f54    7ff7315f            None      47           28               19       0.404255    18         -1             10  0.293103     0.275862    0.500000  0.391304     2       2\n",
       "3883392  54c29f4c      code                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   '''\\nWe are preparing out data in such a way that bert model can understand it\\n\\nWe have to give three sequences as input to the BERT\\n\\nall_tokens : It basically performs the tokenization of the input sentences\\nall_masks  : This is done to make every input of the same length. We choose the maximum length of the vector and pad other vectors accordingly. We padd them \\n             with the help of '0' which tells tells the model not to give attension to this token\\nsegment Ids: This is used when we are giving multiple sentences as the input. Since we are only giving one sentence as the input we set the value of the \\n             segment ids as 0 for all the tokens.\\n             \\nFor more detailed you can visit this terrific blog : jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time\\n'''\\n\\ndef bert_encode(texts, tokenizer, max_len=512):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n            \\n        text = text[:max_len-2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        tokens += [0] * pad_len\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\\n\\n  9b7beeeb0a0e03      198  9b7beeeb0a0e03\\t54c29f4c    a9225a93            None     110           66               44       0.400000    86         48             -1  0.731343    -1.000000   -1.000000  0.788991     1       6\n",
       "5146599  e8b18f0a      code  # Flag to get cv score\\nGET_CV = True\\n# Flag to check ram allocations (debug)\\nCHECK_SUB = False\\n\\ndf = pd.read_csv('../input/shopee-product-matching/test.csv')\\n# If we are comitting, replace train set for test set and dont get cv\\nif len(df) > 3:\\n    GET_CV = False\\ndel df\\n\\n# Function to get our f1 score\\ndef f1_score(y_true, y_pred):\\n    y_true = y_true.apply(lambda x: set(x.split()))\\n    y_pred = y_pred.apply(lambda x: set(x.split()))\\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\\n    len_y_true = y_true.apply(lambda x: len(x)).values\\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\\n    return f1\\n\\n# Function to combine predictions\\ndef combine_predictions(row):\\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\\n    return ' '.join( np.unique(x) )\\n\\n# Function to read out dataset\\ndef read_dataset():\\n    if GET_CV:\\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\\n        df['matches'] = df['label_group'].map(tmp)\\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\\n        if CHECK_SUB:\\n            df = pd.concat([df, df], axis = 0)\\n            df.reset_index(drop = True, inplace = True)\\n        df_cu = cudf.DataFrame(df)\\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\\n    else:\\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\\n        df_cu = cudf.DataFrame(df)\\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\\n        \\n    return df, df_cu, image_paths\\n\\n# Function to decode our images\\ndef decode_image(image_data):\\n    image = tf.image.decode_jpeg(image_data, channels = 3)\\n    image = tf.image.resize(image, IMAGE_SIZE)\\n    image = tf.cast(image, tf.float32) / 255.0\\n    return image\\n\\n# Function to read our test image and return image\\ndef read_image(image):\\n    image = tf.io.read_file(image)\\n    image = decode_image(image)\\n    return image\\n\\n# Function to get our dataset that read images\\ndef get_dataset(image):\\n    dataset = tf.data.Dataset.from_tensor_slices(image)\\n    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\\n    dataset = dataset.batch(BATCH_SIZE)\\n    dataset = dataset.prefetch(AUTO)\\n    return dataset\\n\\n# Arcmarginproduct class keras layer\\nclass ArcMarginProduct(tf.keras.layers.Layer):\\n    '''\\n    Implements large margin arc distance.\\n\\n    Reference:\\n        https://arxiv.org/pdf/1801.07698.pdf\\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\\n            blob/master/src/modeling/metric_learning.py\\n    '''\\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\\n                 ls_eps=0.0, **kwargs):\\n\\n        super(ArcMarginProduct, self).__init__(**kwargs)\\n\\n        self.n_classes = n_classes\\n        self.s = s\\n        self.m = m\\n        self.ls_eps = ls_eps\\n        self.easy_margin = easy_margin\\n        self.cos_m = tf.math.cos(m)\\n        self.sin_m = tf.math.sin(m)\\n        self.th = tf.math.cos(math.pi - m)\\n        self.mm = tf.math.sin(math.pi - m) * m\\n\\n    def get_config(self):\\n\\n        config = super().get_config().copy()\\n        config.update({\\n            'n_classes': self.n_classes,\\n            's': self.s,\\n            'm': self.m,\\n            'ls_eps': self.ls_eps,\\n            'easy_margin': self.easy_margin,\\n        })\\n        return config\\n\\n    def build(self, input_shape):\\n        super(ArcMarginProduct, self).build(input_shape[0])\\n\\n        self.W = self.add_weight(\\n            name='W',\\n            shape=(int(input_shape[0][-1]), self.n_classes),\\n            initializer='glorot_uniform',\\n            dtype='float32',\\n            trainable=True,\\n            regularizer=None)\\n\\n    def call(self, inputs):\\n        X, y = inputs\\n        y = tf.cast(y, dtype=tf.int32)\\n        cosine = tf.matmul(\\n            tf.math.l2_normalize(X, axis=1),\\n            tf.math.l2_normalize(self.W, axis=0)\\n        )\\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\\n        phi = cosine * self.cos_m - sine * self.sin_m\\n        if self.easy_margin:\\n            phi = tf.where(cosine > 0, phi, cosine)\\n        else:\\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\\n        one_hot = tf.cast(\\n            tf.one_hot(y, depth=self.n_classes),\\n            dtype=cosine.dtype\\n        )\\n        if self.ls_eps > 0:\\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\\n\\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\\n        output *= self.s\\n        return output\\n\\n\\n\\n# Function to get the embeddings of our images with the fine-tuned model\\ndef get_image_embeddings(image_paths):\\n    embeds = []\\n    #m = 0.7, \\n    margin = ArcMarginProduct(\\n            n_classes = N_CLASSES, \\n            s = 30, \\n            m = 0.5, \\n            name='head/arc_margin', \\n            dtype='float32'\\n            )\\n\\n    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\\n    label = tf.keras.layers.Input(shape = (), name = 'inp2')\\n    x = efn.EfficientNetB1(weights = None, include_top = False)(inp)\\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\\n    #x = tf.keras.layers.Dropout(0.5)(x)\\n    x = margin([x, label])\\n        \\n    output = tf.keras.layers.Softmax(dtype='float32')(x)\\n\\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\\n    model.load_weights('../input/shopee-efficientnetb3-arcmarginproduct/EfficientNetB1_512_42.h5')\\n    model = tf.keras.models.Model(inputs = model.input[0], outputs = model.layers[-4].output)\\n    chunk = 5000\\n    iterator = np.arange(np.ceil(len(df) / chunk))\\n    for j in iterator:\\n        a = int(j * chunk)\\n        b = int((j + 1) * chunk)\\n        image_dataset = get_dataset(image_paths[a:b])\\n        image_embeddings = model.predict(image_dataset)\\n        embeds.append(image_embeddings)\\n    del model\\n    image_embeddings = np.concatenate(embeds)\\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\\n    del embeds\\n    gc.collect()\\n    return image_embeddings\\n\\n# Return tokens, masks and segments from a text array or series\\ndef bert_encode(texts, tokenizer, max_len=512):\\n    all_tokens = []\\n    all_masks = []\\n    all_segments = []\\n    \\n    for text in texts:\\n        text = tokenizer.tokenize(text)\\n            \\n        text = text[:max_len-2]\\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\\n        pad_len = max_len - len(input_sequence)\\n        \\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\\n        tokens += [0] * pad_len\\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\\n        segment_ids = [0] * max_len\\n        \\n        all_tokens.append(tokens)\\n        all_masks.append(pad_masks)\\n        all_segments.append(segment_ids)\\n    \\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\\n\\n# Function to get our text title embeddings using a pre-trained bert model\\ndef get_text_embeddings(df, max_len = 70):\\n    embeds = []\\n    module_url = \"../input/shopee-external-models/bert_en_uncased_L-24_H-1024_A-16_1\"\\n \\n    bert_layer = hub.KerasLayer(module_url, trainable = True)\\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\\n    text = bert_encode(df['title'].values, tokenizer, max_len = max_len)\\n    \\n    margin = ArcMarginProduct(\\n            n_classes = 11014, \\n            s = 30, \\n            m = 0.5, \\n            name='head/arc_margin', \\n            dtype='float32'\\n            )\\n    \\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\\n    label = tf.keras.layers.Input(shape = (), name = 'label')\\n\\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\\n    clf_output = sequence_output[:, 0, :]\\n    x = margin([clf_output, label])\\n    output = tf.keras.layers.Softmax(dtype='float32')(x)\\n    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\\n    \\n    model.load_weights('../input/bert-baseline/Bert_123.h5')\\n    model = tf.keras.models.Model(inputs = model.input[0:3], outputs = model.layers[-4].output)\\n    chunk = 5000\\n    iterator = np.arange(np.ceil(len(df) / chunk))\\n    for j in iterator:\\n        a = int(j * chunk)\\n        b = int((j + 1) * chunk)\\n        text_chunk = ((text[0][a:b], text[1][a:b], text[2][a:b]))\\n        text_embeddings = model.predict(text_chunk, batch_size = BATCH_SIZE)\\n        embeds.append(text_embeddings)\\n    del model\\n    text_embeddings = np.concatenate(embeds)\\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\\n    del embeds\\n    gc.collect()\\n    return text_embeddings\\n    \\n# Function to get 50 nearest neighbors of each image and apply a distance threshold to maximize cv\\ndef get_neighbors(df, embeddings, KNN = 50, image = True):\\n    model = NearestNeighbors(n_neighbors = KNN)\\n    model.fit(embeddings)\\n    distances, indices = model.kneighbors(embeddings)\\n    \\n    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\\n    if GET_CV:\\n        if image:\\n            thresholds = list(np.arange(3.0, 5.0, 0.1))\\n        else:\\n            thresholds = list(np.arange(15, 35, 1))\\n        scores = []\\n        for threshold in thresholds:\\n            predictions = []\\n            for k in range(embeddings.shape[0]):\\n                idx = np.where(distances[k,] < threshold)[0]\\n                ids = indices[k,idx]\\n                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\\n                predictions.append(posting_ids)\\n            df['pred_matches'] = predictions\\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\\n            score = df['f1'].mean()\\n            print(f'Our f1 score for threshold {threshold} is {score}')\\n            scores.append(score)\\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\\n        best_threshold = max_score['thresholds'].values[0]\\n        best_score = max_score['scores'].values[0]\\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\\n        \\n        # Use threshold\\n        predictions = []\\n        for k in range(embeddings.shape[0]):\\n            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\\n            if image:\\n                idx = np.where(distances[k,] < 3.6)[0] #3.7,3.3\\n            else:\\n                idx = np.where(distances[k,] < 20.0)[0] #20,18\\n            ids = indices[k,idx]\\n            posting_ids = df['posting_id'].iloc[ids].values\\n            predictions.append(posting_ids)\\n    \\n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\\n    else:\\n        predictions = []\\n        for k in tqdm(range(embeddings.shape[0])):\\n            if image:\\n                idx = np.where(distances[k,] < 3.6)[0] #3.3\\n            else:\\n                idx = np.where(distances[k,] < 20.0)[0] #20,18\\n            ids = indices[k,idx]\\n            posting_ids = df['posting_id'].iloc[ids].values\\n            predictions.append(posting_ids)\\n        \\n    del model, distances, indices\\n    gc.collect()\\n    return df, predictions\\n    \\n\\ndf, df_cu, image_paths = read_dataset()\\nimage_embeddings = get_image_embeddings(image_paths)\\n  ce8b65fdfd2fc0     1091  ce8b65fdfd2fc0\\te8b18f0a    0402a6a0  7427d81de0f830      10            9                1       0.100000     5          4             -1  0.500000    -1.000000   -1.000000  0.555556     2      12\n",
       "5591331  7b4093af      code                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       class FastAiBertTokenizer(BaseTokenizer):\\n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\\n        self._pretrained_tokenizer = tokenizer\\n        self.max_seq_len = max_seq_len\\n\\n    def __call__(self, *args, **kwargs):\\n        return self\\n\\n    def tokenizer(self, t:str) -> List[str]:\\n        \"\"\"Limits the maximum sequence length\"\"\"\\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]  e08db13aefd60c       47  e08db13aefd60c\\t7b4093af    b9f2edaa  1dadb8c3bd33bb      31           30                1       0.032258     3          3             -1  0.129032    -1.000000   -1.000000  0.100000     2       2\n",
       "5807422  0b20d677      code                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          from IPython.core.display import display, HTML\\nimport functools\\n\\ndef remove_stopwords(text,stopwords):\\n    text = \"\".join(c for c in text if c not in ('!','.',',','?','(',')','-'))\\n    text_tokens = word_tokenize(text)\\n    #remove stopwords\\n    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\\n    str1=''\\n    str1=' '.join(word for word in tokens_without_sw)\\n    return str1\\n### spacy score sentence\\ndef score_sentence(search,sentence):\\n        main_doc=nlp(sentence)\\n        search_doc=nlp(search)\\n        sent_score=main_doc.similarity(search_doc)\\n        return sent_score\\n\\n# custom sentence score\\ndef score_sentence_prob(search,sentence,focus):\\n    keywords=search.split()\\n    sent_parts=sentence.split()\\n    word_match=0\\n    missing=0\\n    for word in keywords:\\n        word_count=sent_parts.count(word)\\n        word_match=word_match+word_count\\n        if word_count==0:\\n            missing=missing+1\\n    percent = 1-(missing/len(keywords))\\n    final_score=abs((word_match/len(sent_parts)) * percent)\\n    if missing==0:\\n        final_score=final_score+.05\\n    if focus in sentence:\\n        final_score=final_score+.05\\n    return final_score\\n\\n# BERT pretrained question answering module\\ndef answer_question(question,text, model,tokenizer):\\n    input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\\n    input_ids = tokenizer.encode(input_text)\\n    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\\n    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\\n    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\\n    answer=(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\\n    # show qeustion and text\\n    #tokenizer.decode(input_ids)\\n    return answer\\n\\ndef score_sentence_study(sentence):\\n        search='included total cases patients sample size collected gathered enrolled study'\\n        #search='how many patients cases included in study cohort meta-analysis prospective retrospective'\\n        main_doc=nlp(sentence)\\n        search_doc=nlp(search)\\n        sent_score=main_doc.similarity(search_doc)\\n        return sent_score\\n\\ndef process_question(df,search,focus):\\n    df_table = pd.DataFrame(columns = [\"Date\",\"Study\",\"Study Link\",\"Journal\",\"Days After Admission Covid-19 is Present\",\"Source / Summary\",\"Study Type\",\"Sample Size\",\"Confirmation\",\"excerpt\",\"rel_score\"])\\n    # focuses to make sure the exact phrase in text\\n    #df1 = df[df['abstract'].str.contains(focus)]\\n    # focus to make sure all words in text\\n    df1=df[functools.reduce(lambda a, b: a&b, (df['abstract'].str.contains(s) for s in search))]\\n    search=remove_stopwords(search,stopwords)\\n    for index, row in df1.iterrows():\\n        sentences = row['abstract'].split('. ')\\n        pub_sentence=''\\n        hi_score=0\\n        study=''\\n        hi_study_score=0\\n        for sentence in sentences:\\n            if len(sentence)>75:\\n                rel_score=score_sentence_prob(search,sentence,focus)\\n                if rel_score>.05:\\n                    sentence=sentence.capitalize()\\n                    if sentence[len(sentence)-1]!='.':\\n                        sentence=sentence+'.'\\n                    pub_sentence=pub_sentence+' '+sentence\\n                    if rel_score>hi_score:\\n                        hi_score=rel_score\\n                \\n        if pub_sentence!='':\\n            text=row['abstract'][0:1000]\\n            \\n            question='how many patients or cases were in the study, review or analysis?'\\n            sample=answer_question(question,text,model,tokenizer)\\n            sample=sample.replace(\"#\", \"\")\\n            sample=sample.replace(\" , \", \",\")\\n            if sample=='19' or sample=='' or '[SEP]'in sample:\\n                sample='unk'\\n            if len(sample)>50:\\n                sample='unk'\\n            sample=sample.replace(\" \", \"\")\\n            \\n            question='what type or kind of review study analysis model was used?'\\n            design=answer_question(question,text,model,tokenizer)\\n            design=design.replace(\" ##\", \"\")\\n            if '[SEP]'in design or '[CLS]' in design or design=='':\\n                design='unk'\\n            \\n            shorter = pub_sentence[0:1000]\\n            ### answer incubation questions\\n            incubation_period=answer_question('how many days after onset was the shedding detected',shorter,model,tokenizer)\\n            incubation_period=incubation_period.replace(\" ##\", \"\")\\n            incubation_period=incubation_period.replace(\" · \", \"·\")\\n            incubation_period=incubation_period.replace(\" . \", \".\")\\n            \\n            incubation_range=answer_question('what is the virus shedding detection route',shorter,model,tokenizer)\\n            incubation_range=incubation_range.replace(\" ##\", \"\")\\n            incubation_range=incubation_range.replace(\" · \", \"·\")\\n            incubation_range=incubation_range.replace(\" . \", \".\")\\n            \\n            confirmation=answer_question('what kind of test was conducted to confirm the shedding',shorter,model,tokenizer)\\n            confirmation=confirmation.replace(\" ##\", \"\")\\n            confirmation=confirmation.replace(\" · \", \"·\")\\n            confirmation=confirmation.replace(\" . \", \".\")\\n            \\n            authors=row[\"authors\"].split(\" \")\\n            link=row['doi']\\n            title=row[\"title\"]\\n            score=hi_score\\n            journal=row[\"journal\"]\\n            if journal=='':\\n                journal=row['full_text_file']\\n            linka='https://doi.org/'+link\\n            linkb=title\\n            final_link='<p align=\"left\"><a href=\"{}\">{}</a></p>'.format(linka,linkb)\\n            #author_link='<p align=\"left\"><a href=\"{}\">{}</a></p>'.format(linka,authors[0]+' et al.')\\n            #sentence=pub_sentence+' '+author_link\\n            sentence=pub_sentence\\n            #sentence='<p fontsize=tiny\" align=\"left\">'+sentence+'</p>'\\n            to_append = [row['publish_time'],title,linka,journal,incubation_period,incubation_range,design,sample,confirmation,sentence,score]\\n            df_length = len(df_table)\\n            df_table.loc[df_length] = to_append\\n    df_table=df_table.sort_values(by=['Date'], ascending=False)\\n    to_append = [\"Date\",\"Study\",\"Study Link\",\"Journal\",\"Days After Admission Covid-19 is Present\",\"Source / Summary\",\"Study Type\",\"Sample Size\",\"Confirmation\",\"excerpt\",\"rel_score\"]\\n    df_length = len(df_table)\\n    df_table.loc[df_length] = to_append\\n    return df_table\\n\\ndef prepare_summary_answer(text,model):\\n    #model = pipeline(task=\"summarization\")\\n    return model(text)\\n\\n###### MAIN PROGRAM ######\\n# questions\\nsearch='naso nasal nasopharynx pharyn'\\n\\n# main focus keywords\\nfocus='shedding'\\n\\n# process with spacy model and return df\\ndf_table=process_question(df,search,focus)\\n    \\ndisplay(HTML('<h2>'+search+'</h2>'))\\n#display(HTML('<h5>*** Note: this table keeps the document excerpt and score for ease of review. A clean literature review formatted CSV file is avaiable in the data seciton wihtout those fields.</h5>'))\\n    \\n#convert df_table to html and display\\ndf_table=df_table.drop(columns=['excerpt', 'rel_score'])\\n#df_master=df_master.fillna('blank')\\n#df_table=df_table.fillna('blank')\\ndf_table_show = df_table.append(df_master,sort=False)\\ndf_table_show = df_table_show.drop_duplicates(subset='Study', keep=\"last\")\\n#df_table_show['Study Type'].fillna(df_table_show['StudyType'], inplace=True)\\n#del df_table_show['Study Type']\\n\\ndf_table_display=HTML(df_table_show.to_html(escape=False,index=True))\\ndisplay(df_table_display)\\n\\ndf_table_show.to_csv('What do we know about viral shedding nasopharynx_.csv', index = False)\\n\\n#df_table_show.head()  e9664b8145e1e9      548  e9664b8145e1e9\\t0b20d677    2f763a2f            None       4            3                1       0.250000     3          2             -1  0.750000    -1.000000   -1.000000  1.000000     4      19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.source.str.contains('\\[SEP\\]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1925003cfa3979ae366740114cfe890bf8d7ad5b88e4afe0ec571fe261ed45e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
